{
  "2010-BATI": "1. Introduction Assessment of business-IT alignment is a subject of continuous interest in research and industrial communities. For practitioners, validation of business-IT alignment is an important part of the organisation government; for researchers, approaches to accurate alignment measurement pave the way to new theories in the field [1]. Many approaches to business-IT alignment assessment are addressed in the literature. These approaches can be divided into three groups: questionnaire-based approaches [2], [3], [4], framework-based approaches [5], [6] and approaches based on alignment measurements [7], [8]. For many organisations the metrics-based alignment assessment is beneficial: it provides quantitative results that allow managers to measure the business value of the existing IT, and to increase this value. Our work is concentrated on the last type of business-IT alignment assessment approaches– the metrics -based ones. Validation of business-IT alignment approaches based on metrics is addressed in the research literature [9], [10], [11]. We agree with the author in [12] that in order to be valid these approaches should be grounded on a solid theory: “it is questionable whether it is worth showing that a measure is measuring a particular attribute if that attribute is not part of a theory”. Briand [12] argues that most of the proposed metrics and the way to measure them have not undergone an empirical validation. Schneidewind [10] advocates an empirical validation process in which a metric is associated with a measure of interest. This process is specified for the software metrics but remains valid for metrics in any other discipline - particularly in Enterprise Architecture. Our experience shows that practical validation criteria of a metrics-based approach can be quite different from the theoretical ones. We follow the author of [16] who argues that “a measure can be correct from a measurement theory perspective but be of no practical relevance to the problem at hand. On the other hand, a measure can be not entirely satisfactory from a theoretical perspective but can be a good enough approximation and work fine in practice”. In this paper, we discuss the theoretical and empirical (or practical) validity of metrics-based approaches to alignment assessment. We propose an empirical fitness1 validation of the measurement approach for business-IT alignment developed in [17]. First, we identify a set of practical validity criteria for this approach and illustrate these criteria on the example. Then we generalise our criteria and propose the guidelines for operationalisation of approaches based on alignment measurements. Research protocol: In the literature, five classes of empirical research are identified [18]: controlled experiments, case studies, survey research, ethnographies and action research. In our work, we have selected “case studies” as a research method type. This method offers a deep understanding of a given phenomenon and explains how and why this phenomenon occurs. In this work, we use the ABC-Supermarket case for our study.We justify this case as a critical case to test the fitness measurement approach [17]. We proceed with the case study as follows: first, we identify the criteria of theoretical validity for the fitness measurement approach [17]. To justify our criteria, we make an analysis of related works and show that these criteria are considered important in many approaches [7], [9], [10], [12], [13], [16]. Then we implement this approach in the industrial project of Information Systems (IS) evolution in ABC-Supermarket. While implementing, we (i) observe whether the theoretical criteria of validity are met; (ii) check that the theoretical criteriaare recognised as “important” by practitioners (iii) identify other validity criteria, which are important for practitioners but are omitted in the identified theoretical criteria list (we call these criteria “empirical vriteria of validity”). Our study shows that some of the theoretical criteria are refuted and other factors related to fitness metrics validity are elicited. 1 The fitness relationship definition used in [17] is ‘‘the degree to which the needs, demands, goals, objectives and/or structure of one component are consistent with the needs, demands, goals, objectives and/or structure of another component’’. This paper is organised as follows. In Section 2 we introduce the Fitness Measurement Approach and define the theoretical and the empirical criteria of validity for this approach. In section 3, we present the case study, by introducing the industrial project and the scope of our research; we describe how the fitness measurement approach was implemented, and we report the measurements’ results. In section 4, we summarise the lessons learned and discuss the gap between our understanding of the measurements validity and the perception of our industrial partners about it. In Section 5, we present the conclusions and future work. 2. Validation of Fitness Measurement Approach In this section we present the Fitness Measurement Approach developed in [17]. We define a list of theoretical criteria that should be respected by the valid fitness metrics These criteria correspond to the perception of the Fitness Metrics Approach validity from the researchers’ point of view. 2.1. Introduction of Fitness Measurement Approach In [17], authors propose an approach to evaluate the fitness relationship between the business and the system supporting it. The fitness relationship is established between components of business and system models. The approach proposes a fitness measurement according to four points of view (called “factors”): intentional, informational, functional and dynamic. This approach also identifies the ten fitness criteria associated with these factors and defines a specific metric for each of them. For example, the goal satisfaction criterion characterises the intentional alignment factor. It describes how the business goals specified within an organisation are supported by the IT systems existing in this organisation. The metric defined for this criterion is a goal count. Goal count can be measured by calculating the ratio between the business goals explicitly represented by the corresponding states of the IT systems and the total amount of business goals (see [17] for more details). The ≤ measurement result 0 < goal count 1 can be then analysed: if goal count = 1, then all goals are taken into account. Please note that the approach does not address the cost, human and social factors. It is rather concentrated on evaluating the information which is supposed to be included in the IS. Table 1. Fitness measurement framework Fitness criterion Fitness Metric Alignment factor Intentional 1. Support ratio Activity count alignment 2. Goal satisfaction Goal count 3. Actor presence Actor count 4. Resource presence Resource count Informa- 5. Information completeness Business object/System class mapping count tional 6. Informational accuracy Business /System state mapping count alignment Functional 7. Activity completeness Business object/System class mapping count alignment 8. Activity accuracy Business/System state mapping count Dynamic 9. System reliability Law-mapping count alignment 10. Dynamic realism Path mapping count The Fitness Measurement Approach is based on a set of concepts important for the alignment assessment. Business goal [19] is a set of stable states of business objects we seek to achieve. Business object (BO) is an object that represents the entities in the business domain. Business state (or BO state) is a state of a BO at a time t, defined by the values of all attributes of this BO. Business actor is defined as someone or something that interacts with the business or IT system using an interface; it participates in a business process and triggers external events that result in a state transition of a BO. Business resource [19] is a BO, which neither initiates actions nor causes a state change. In our case, a product specification is an example of a business resource. System class (or system object) is an object that represents the entities in the IT system (by analogy with a business object). System event [19] is associated with a system state change. By analogy with business activities that are changing business objects’ states, we consider system events changing states of system objects. System goals describe purposes of the system [17]. We say that a system goal maps a business goal if the states of business objects associated with this business goal are represented by the states of the corresponding system objects. System state (or system object state) is a state of a system object (class instance) at a time t. It is defined by the values of all attributes of this object. Paths are sequences of business (or system) states. Business laws represent legal rules and principles adopted by business organisations. The Fitness Measurement Approach addresses the problem of business-IT alignment in the organisations and strongly relies on the detailed information about the organisation processes, data models, etc. In case this information is not available, one can build it up as it was discussed in [20]. 2.2. The evaluation hypothesis In this section we define evaluation hypotheses of the Fitness Measurement Approach. The hypotheses consist of a set of theoretical criteria of validity for the fitness measurements. Based on our research experience and on the related literature analysis we retain the following validity criteria: 1. The measurements should be based on verifiable observations (models, specifications, interviews, etc). 2. The measurements’ results should be non-ambiguous – they should have only one interpretation. 3. The measurements should be effective: they should correspond to the problem complexity and help practitioners to decide on the course of improvement actions. 4. The measurements’ results should be accurate: they should precisely localise the misalignment in the organisation. Many works on metrics-based approaches validity confirm our validity criteria. Verifiable observations: research works [12], [22] argue that a metric is valid if it measures what it purports to measure. To do so, we need to clarify what attribute we are measuring and how we proceed to measure it. The precision of the underlined data to be measured is thus important to have a valid measurement. Non-ambiguity: in [9], authors discuss the validity of a metric structure. In order to be valid, the metric requires the validity of the attributes it measures, the unit it uses, the instrument it underlies and the measurement protocol it defines. They argue that the non-ambiguity of these elements guarantees the metric validity. Effectiveness: Fenton [11] discusses the metric validity view based on the identification of the usefulness of a metric for a stakeholder’s purpose. In [21], [16], [7] the authors argue that metrics constitute a crucial source of information for decision-making. Indeed, they (metrics) should localise where malfunctions hold and where resources are needed and give accurate information to managers in order to help them make decisions. Accuracy: Bodhuin [7] emphasises that the purpose of metrics is to check the alignment and to detect misalignment between business processes and the information system supporting them. He defines two metrics: “technological coverage” indicating the percentage of activities supported by the system. If an activity i is supported by the system, the second metric: “technological adequacy” brings more precise information and measures how adequately is the support of a set of system components for the activity i. 3. The Practical Test of the Evaluation Hypotheses In order to be widely accepted, each research method or approach, , should prove its usefulness in practice [12]. In the previous section we defined the theoretical criteria of validity for the fitness measurement approach. In the following, we test if these criteria remain valid in practice. We argue that proving the empirical validity of a research approach guarantees its entire validity. Practical validity addresses the ability of the research approach to meet the practitioners’ needs: it tells the practitioners how they can benefit from this approach and what will be the added value. To validate the Fitness Measurement Approach, we should answer the question: « when do the results of this approach can be considered by the users as satisfactory? ». To do so, we apply the Fitness Measurement Approach [17] in a real project. The appreciation of the results by practitioners informs us about the “practical validity” of this approach. 3.1 The Case study: alignment validation in ABC-Supermarket ABC-Supermarket is a mass retail company - one of the leaders on the French market. ABC-Supermarket groups approximately 3000 independent operators, and thousands outlets in France and internationally. This company specialises in different sectors of retail business and is well known in both food and non-food retail markets. The initial specialisation of the ABC-Supermarket is food and household products. Seven years ago ABC-Supermarket integrated a new product category – textile – in its portfolio and defined a new trade name - ABC-Fashion. To provide the IT support for purchase (upstream) and retail (downstream) activities for textile products, the company decided to use the existing information system – the one which is used to support the business activities for food products. Initially, the reuse of the existing IS for ABC-Fashion seemed justified as retail business defines similar processes for food and textile products. Master data for both food and textile products also have a lot in common: all these products are characterised by their type, price, etc. However, over the years, the textile trade name turnover decreases and the survival of the trade name was threatened. The existing IS showed its limits in managing the textile business. As a solution, numerous manual fixes and workarounds have been developed over years. As a result, the existing IS got overloaded with patches and became not efficient. The company decided to make evolve its information systems. The challenge becomes the trade name survival. The main objective of this evolution is to precisely define where the existing IS fails in supporting the textile business requirements, and what improvement can be made to correct the misalignment. To answer these questions we apply the Fitness Measurement Approach. While the fitness measurements results in a set of values, the process of acquiring these values leads to a deep understanding of the gap between the existing IS and the textile business requirements. 3.2 Scope of the fitness approach application The upstream activities of ABC-Fashion include marketing, products referencing, providers referencing, outlets billing, etc. These activities are supported by the existing upstream information system (or UIS) of the company. The downstream activities of ABC-Fashion address the product management in the outlet stores, e.g. stock replenishment. These activities are supported by the existing downstream information system (or DIS). The cited UIS and DIS were affected by the evolution requirements. Among listed above, product referencing is one of the most critical tasks as it maintains the link between the upstream and the downstream information systems: the outlets use DIS to order products available in UIS (see fig.1). If a product is not referenced in UIS, it is not available for ordering. That is why the IT support for the product referencing represents the main concern for the ABC-Fashion management. Mismanaging the product referencing activity affects the whole business process: the stock management (in the upstream and at the outlet level), the ordering process, marketing campaigns... For this reason, we concentrate in this study on the textile product referencing activity and how the existing IS (the food one) supports it. Fig.1. Product referencing application: the link between UIS and DIS 3.3 Constraints of the research work Researcher: One researcher, working partial time, during 9 months in the ABC- Supermarket, applied the fitness measurement approach. Experimental objects availability: Less than 20% of business and system models are available in the organisation. Indeed, over ten metrics, two were applied without building the corresponding “input” models. The unavailable artefacts have to be built. Otherwise, fitness measurements cannot be applied. 3.4 Implementation of the Fitness Measurement Approach As cited above, Fitness Measurement Approach relies on business and IS models. As the most of these models do not exist in the organisation, we built them up based on the available information sources. As a result, we were able to implement nine fitness metrics out of ten. a) Data collection To construct the missing business models, we collected data that describe the business view of ABC-Fashion on the textile product referencing. We interviewed the following business actors: (i) the head of department of the textile trade name: in order to understand the textile business requirements, (ii) the responsible of the product referencing department: in order to apprehend the product referencing problem, (iii) IS users: in order to understand the IS functioning and how it is used. The available business process landscape and process specifications were also analysed. To construct the required IT artefacts, we collected data that describe the IT support of textile product referencing provided by the existing information systems (UIS and DIS). In order to apprehend the detail of the system architecture and functioning, interviews have been conducted with the following IT actors: (i) the referencing system administrator, (ii) the referencing system designers, (iii) the referencing system developers. The following documentations were also analysed: (i) the user manuals of the product referencing system, (ii) short descriptions of application functionalities available on the Intranet, (iii) software applications’ data dictionaries, containing the information about product master data, and (iv) screenshots. We also studied the product referencing system testing it on some “toy” examples. After conducting interviews with the mentioned specialists and the examination of the existing documentation, two problems have been highlighted: (i) compared to food products, who’s assortment can be either permanent (always on the shelf) or non- permanent (a subject of a commercial operation), textile products are subdivided into three planning categories: permanent products, collection products (e.g. summer/ winter), and short-cycle products (e.g. fashionable articles, brand promotions, etc); (ii) apart from the master data, operations on textile products also require textile-specific data (e.g. colour range and size range). Business and system artefacts reflecting these two problems should be built in order to localise precisely the problem sources and identify accordingly potential solutions. b) Consolidation of collected data We are interesting in highlighting how the existing IS has been adapted to support the business requirements and to what extent it fits them. To conduct our study, we use: the MAP formalism [23] to build the business and IS goal models describing • respectively the referencing process as it is seen by ABC-Fashion and as it is supported by the referencing application. UML modelling to build the business and the system class diagrams • specifying respectively the data model for the product referencing as it is seen (or required) by ABC-Fashion and its implementation by the existing information system. These artefacts (business goal models and business and IS data models) can highlight enough the problems cited above. For lack of space, these models are not presented in the paper. Problem detected by MAP modelling: A map is a process model in which a non-deterministic ordering of intentions and strategies is defined. Intentions are the goals to achieve. Strategies represent the ways to achieve these goals. A map diagram is a labelled directed graph with intentions as nodes and strategies as edges between intentions. A section in a map is the triplet <I , i I , S > where I is the source intention, I is the target intention and S is the j ij i j ij strategy connecting the source and target intentions. A section can be, in turn, refined by a map. The comparison between business and IS maps shows that there are some business goals (at different level of abstraction) which do not correspond any IS goal. Some business strategies are also not supported by the product referencing application. In fact, as shown in Fig. 2, the business goal “Plan commercial operations” is not supported by the Product Referencing application (shown by dashed and grey arrows in Fig. 2(b)). The system implements only one strategy for achieving the goal Reference products. It does it from scratch by entering data from the product sheet directly. This means that the system does not distinguish Collection, Permanent, and Short cycle products and thus processes all product types in the same way. This explains a partial satisfaction of the business goal “Reference products”. SSttaarrtt SSttaarrtt BByy rreeaalliizziinngg tthhee ppllaann BByy rreeaalliizziinngg tthhee BByy rreeaalliizziinngg tthhee ppllaann BByy rreeaalliizziinngg ooff ccoommmmeerrcciiaall aacc tt iioonnss bb uu ddggeett ooff ppuurrcchhaassee ooff ccoommmmeerrcciiaall aacc tt iioonnss t t hh ee bb uu dd gg ee tt ooff B y e n te r in g CC 11 CC 22 CC 22 p p uu rr cc hh aa ss ee da t a fr o m CC 11 product sheet CC PPllaann ccoommmmeerrcciiaall PPllaann ccoommmmeerrcciiaall ooppeerraattiioonnss ooppeerraattiioonnss BByy mmaakkiinngg sshhoorrtt BByy bbuuiillddiinngg BByy sseelleeccttiinngg BByy mmaakkiinngg sshhoorrtt BByy bbuuiillddiinngg BByy sseelleeccttiinngg ccyyccllee ooffffeerr ccoo llll eeccttiioonnss ppee rrmm aanneenntt pprroodduuccttss cciirrccuuiitt ooffffeerr ccooll llee ccttiioonnss pp ee rrmmaanneenntt pprroodduuccttss CC33 CC33 CC 44 CC 44 CC 55 CC 55 BByy cchhaannggiinngg CC 66 B B yy pp rrii ccee r r ee ff ee rree nn cc ee CC66 RReeffeerreennccee pprroodduuccttss B B yy pp rrii ccee RReeffeerreennccee pprroodduuccttss CC77 CC77 m m aa nn aa gg ee mm ee nntt o o ff aa nn ee xx ii ssttiinngg m m aa nn aa gg eemm eenntt pprroodduucctt P P aa rr pp rr oodduuccttss P P aa rr pp rr oodduuccttss CC99 CC99 CC88 u upp dd aa tt ee CC88 BByy BByy u upp dd aa tt ee ccoommpplleetteenneessss ccoommpplleetteenneessss SSttoopp SSttoopp (a) (b) F ig. 2. (a) Business map for product referencing process (b) UIS map for product referencing application Sections C3, C4 and C5 are refined in sub maps with sub goals and strategies (11 business goals in total). An important part of these sub goals are not supported by the product referencing application. For lack of space, the sub maps are not presented in the paper. Problem detected by the UML modeling: To reference food products, only the business object Logistic Unit is required. A logistic unit for the product of a type T specifies a container, a pack, or a box with X items of the given product inside. It represents a minimal amount of the product of type T that can be ordered, delivered, or stored at the warehouse (e.g. a box of six bottles of soda). As (speaking about food products) the products in the logistic unit are identical – i.e. the same soda bottles - there is no need to reference each product item within the logistic unit separately, it is the entire logistic unit with X items inside that is referenced. At the IS level, the logistic unit is represented by the “Product file” (or “Package” concept) which contains the package description, logistic and tariff data. The same IS (conceived for food products) is used for textile product. But applying the same simplified referencing method for textile product, the following has to be considered: textile products have much more variations within the same type as they can exist in multiple sizes and colours. Making (by analogy with soda bottles) a logistic unit contain the same product variation (e.g. a box of 100 jeans {size = S, colour = ‘Navy’}) is not practical. Therefore logistic units for textile products can contain multiple variations of the same product type: a box of jeans: {{S, ‘Navy’}- >10; {S, ‘Black’}->10; {M, ‘Navy’}->20; {M, ‘Black’}->10; {L, ‘Navy’}->30; {L,‘Black’}->20}. Textile business requires the sizes and colors referencing. As the existing system does not support this need, several workarounds and manual fixes were added, for instance, a “package content File” describing the content of the logistic unit was added to inform points of sale about the quantity of products in terms of sizes and colors contained in each logistic unit. This indicative information provided by the referencing system can not be exploited by the points of sale to order a specifc product with specific size and color. The business was constrained by the system limitations causing then the emergence of gaps with the business requirements. These gaps explain several problems mainly: Points of sales cannot order only one variation of the product to replenish their • stock – they have to order at least one complete logistic unit. This leads to unsold stock, discounting, and regular company loss as a result. The marketing department cannot make forecasting based on the logistic units, • as it is not known which product was most demanded and brought to the company the maximum profit and which was not sold and caused loss. 3.5 Application of fitness measurements on the constructed data and result analysis In this section, we present fitness measurements’ results and propose guidelines that can help ABC-Fashion to improve the fit between their business and the existing IS. We note that some problems are detected during the business and IS models building. Indeed this activity is knowledge intentsive and allowed us to have a first qualitative aligment evaluation. The fitness measurements confirm and detail the qualitative evaluation by capturing the malfunctions in more detail and in terms of models’ concepts and allow us to detect how we can act (add such concept if it is absent or extend its states if it is present but mismanaged in the system...) to improve the alignment. Table 2. Fitness measurement results Criteria: Description Measures: 1. Support ratio {Number of activities represented by system events/Number 7/32 (21%) of activities}. 2. Goal {Number of business goals represented by the system 1/11 (9%) satisfaction goals/Number of business goals} 3. Actor presence {Number of business actors represented by the system user 2/5 (40%) interfaces/Number of business actors} 4. Resource {Number of business resources represented by system 0 presence classes interfaces/Number of business resources} 5. Information {Number of business objects represented by system 7/21 (33%) completeness classes/Number of business objects} 6. Information {Number of business states represented by system 21/42 (50%) accuracy states/Number of business states} 7. Activity {Number of business objects for a given activity represented values for different completeness activities in [0,1] by system classes /Number of business objects for a given activity} 8. Activity {Number of business states for a given activity represented values for different accuracy by system states/Number of business states for a given activities in activity} [0,4;0,72] 9. System {Number of business laws (where each business state is 11/31 (35%) reliability represented by a system state and the transitions between business states are represented by the transitions between Our study revealed the significant differences between the referencing activity defined for the food and the textile products. This is the reason why referencing of textile products using the existing information system was so problematic in the past. The “misfit” between business and IT is confirmed by the measurements’ results shown in Table 2. 1&2. Support ratio and Goal satisfaction: Only 20% of business activities are supported by the system and less than 10% of business goals are satisfied by the system. In fact, the significant part of upstream activities related to the planning of commercial operations on textile products (collection and short cycle operations) is not supported by the Product Referencing application. 3. Actor presence: the product referencing activity involves five actors. Only two actors interact with the system. The others (marketing, buyers and the head of the point of sale) are involved during the planning phase, which is not supported by the system. 4. Resource presence: All business resources required for the product referencing (e.g., specifications, product sheet containing information the product…) are created using Microsoft Excel software and are not integrated in the referencing system. 5. Information completeness: only the third of information is managed by the system. Our analysis revealed the following reasons that justify the low value of information completeness: (i) the need for referencing different textile product categories: the referencing of each product category has its own referencing process. These processes are manual or semi-automatic. Indeed, they are part of the planning activity which is not supported by the system. (ii) The specific requirement for referencing related to the textile products, taking into account their colour/size: on one hand, the concept of “Product” is absent in the system. Indeed, what is present in the system is the concept of “package” containing n products and not the product itself (see section 3.4). On the other hand, the size/colour business concept is missing in the system. It is for this reason that the product can not be referenced with the corresponding size and color. This generated the problems cited above and explain why the business goals are not satisfied by the system. 6. Information accuracy: Although only third of business objects are represented by system classes, 50% of business states are mapped by system states. This is explained by the fact that system objects are not consistent with business objects (the case of “Product” and “Package” concepts), they are forced to be treated in the same manner. 7. Activity completeness: More than half of business transitions are not implemented in the system. 8. Activity accuracy: for some activities, even if a business object (BO) is represented by a certain system object, the states of the latter might not represent the states of the BO. In practice, this means that the system counterpart of the BO is not processed by the IS as expected by business – it is not accurate. This explains why the completeness of an activity is higher than its accuracy. For other activities, the accuracy is higher than the completeness. In fact, some BOs are implicitly supported by the system: i.e. there is no object in the system that represents a given BO, nevertheless the system supports the behaviour of this BO. For example, the system object “Package” does not map the Product BO in textile business (as explained in “information completeness”); however, it substitutes this BO in certain operations – we can say that the “Package” mimics the behaviour of the Product in the system. 9. System reliability: More than half of business transitions are not implemented in the system. This is explained by the fact that only a few activities are supported by the system. We note that the fitness metrics organised around the four alignment factors are inter-reliated and complement each others. Indeed, the source of a mismatch detected at the goal level (intentional alignment) is explained in detail by metrics at more operational levels (functional and informational alignment). As we can observe, there are huge differences in the gaps detected by the fitness metrics. A part of them was covered by workaounds and manual fixes (several treatments are done manually and several add-ons were made). These workarounds were not detected by fitness metrics. Indeed, they (workarounds) correspond to managing the information differently in the system and at the business levels. Whereas, as presented in table 2, the metrics are based on a correspondance between business and system concepts. This concern is beyond the scope of this paper. Metrics are used to detect the gaps, to help us localising the main differences between the business and the system domains and to propose a set of corrective actions which would improve the business/IT alignment. Indeed, it was shown that (i) the existing IS support demonstrates the serious lacks of flexibility: the stock replenishment for POS is supported only on the logistic unit level. We recommended that the IS of the ABC-Supermarket should support the textile product referencing on the product level, rather than on the logistic unit level. (ii) The colour/size management functionality is essential for textile products; we advised that it should be added to the existing IS. (iii) Many business activities are not supported by the system. Consequently, the corresponding actors do not interact with the system and the handled objects and the required business resources are not present in the system. We advocated that the organization should prioritize its business activities and revise the existing IS in order to extend its functionalities to support the critical business activities. The organisation should also verify whether the interaction of some actors with the Product Referencing application would be beneficial for product-referencing processes. If this is the case, new user interfaces should be developed and the business process may be redesigned taking into account the new actors. Organisation should also consider the IS support of identified business resources. Indeed, integration of business resources can increase the interoperability and facilitate the information exchanges between the business and the IS partners. 4 Lessons learned In section 2.2, four theoretical criteria of validity have been defined. We will now evaluate the results presented in section 3 against these criteria according to two points of view: the researchers’ point of view and the practitioners’ point of view. The first one allows us to establish if the measure values obtained for the fitness metrics are hundred percent accurate, effective, verifiable and non ambiguous. The other point of view allows us to evaluate if for the practitioners these set of criteria are relevant or not. 4.1 Measurements validity: point of view of the researchers The measurements are based on verifiable observations: the models required for • the measurements application were built up and validated by specialists within the organisation. The measurements’ results are non-ambiguous: the project stakeholders • understood the results in the same way. The measurements are effective: the results have been compared with the study • made by the project team, requested by the CEO of the textile trade name. These results were confirmed during discussions with the project stakeholders. The measurements’ results are accurate: the results helped us to localise the • misalignments and confirmed the causes of non-fits. From our point of view, the fitness measurements are valid; they fulfilled all theoretical criteria of validity. 4.2 Measurements validity: point of view of the practitioners Fitness Measurement Approach was applied by one researcher on a restricted perimeter of the project as we showed in section 3. Practitioners confirm the usefulness of the approach and the effectiveness of its corresponding results. Nevertheless, concerning the reusability of the approach for other projects, the following criteria related issues bring up: The measurements are based on verifiable observations: Building models • requires much time, new skills and further resources. Managers are aware of their data weaknesses and argue that this is not the priority of the company to build and to maintain such data. The measurement results are non-ambiguous: Managers are aware of their input • data weakness and argue that even with ambiguous results, they would be satisfied. The measurements are effective: Concerning the effectiveness of the • measurements, managers are satisfied. They confirmed that we found problems that indeed exist, and localised misalignments. Nevertheless, they deprecate the fact that the results did not indicate the severity of the identified gaps. They asserted that the prioritisation of gaps severity is very important for the decision- making. The measurements’ results are accurate: The accuracy of the results is not • important for practitioners. They are interested in getting more results precision only if it is done within a short period of time. Otherwise, it does not have much value. For managers, detailed reports take more time to be done and to be understood. Simpler results are preferred, at least in a first step. Sometimes, intuition is enough. From the practical standpoint, fitness measurements are useful only if models required for their application are available in the organisation. For the majority of organisations, this is not the priority. The measurement validity perception of our industrial partners revealed that (i) some of our criteria are not such important for them, and (ii) some criteria appeared to be very important but we did not consider them in our work. Overall, practitioners do not aim a perfect alignment, especially when it requires too much time and resources. Most of the time, they are interested in some aspects of the problem, not all. What is important for practitioners is to do things –approximately– right and fast. Besides the effectiveness, for them, the efficiency criterion is crucial. Table 3 summarises the importance of measurement validity criteria viewed from the theory and the practice standpoints. Table 3. Measurement validity perceptions Theory Practice Measurement based on verifiable ++ -- observations Non-ambiguity ++ + Effectiveness ++ ++ Accuracy ++ -- Efficiency -- +++ 4.3 Discussion of the obtained results Several factors may limit the generalisation of our results: The applicability of the Fitness Measurement Approach depends on the • enterprise models availability in the organisation: if models required for metrics application exist in the organisation, the approach application is just a technical task. Otherwise, required models need to be built [20] and the application of the approach may become a complex and resource demanding task. The interpretation of the validity criteria by practitioners depends on the • organisation data maturity level. Indeed, the more the data maturity level is high the more practitioners adhere to our validity criteria and confirm our evaluation hypothesis. Fitness metrics results depend on the quality (and validity) of the built models. In • the big companies, knowledge is spread among many individuals and the understanding of the same part of business by different individuals may vary and may even be contradictory. Extensive cross-checking is thus mandatory. 5 Conclusions and future work Our study revealed that researchers and practitioners do not have the same understanding of the validity of metrics-based approaches. In fact, some of our hypotheses have been refuted during the case study and new validity criteria emerged. The main requirement of practitioners is that alignment measurements give effective results – even approximately – with regard to the time and the budget constraints of the project. Our experience allowed us to identify practical guidelines to help the successful application of the metrics-based approaches and, more precisely, the enhancement of their applicability in industrial projects. The definitions of these guidelines are based on (i) the observation of the industrial context and (ii) the practitioners’ requirements introduced during fitness measurements. We organise them as practical guidelines in three directions: Guidance in building models: The maturity of organisations (SEI Capability Maturity Model [24]) - has an impact on the metrics applicability. Indeed, metrics rely on models and verified data, which constitute the inputs of the metrics-driven approaches. The availability of such data depends on the maturity level of the organisation. For many organisations required models are often not available, and to build them is necessary to assess the business-IT alignment. Building such models is not a trivial task; the project scope should be well defined to allow the collection of the relevant data. Guidance is thus needed to assist engineers in building the business and IT models required for performing measurements. In [20], we proposed a \"build- up process\" consisting in four phases: (i) identification of the input data required by the fitness measurement approach and which should be constructed; (ii) initial data collection; (iii) data consolidation; (iv) validation of the consolidated data (which will be used as the input data of the fitness measurement approach). Customisation of the approach - Time To Market requirement: Business-IT alignment assessment is a sort of internal audit performed by an organisation in order to undertake the corrective actions and to enhance its performance. In an evolving environment, it is very important to react rapidly to the change. If it takes long time to produce and to communicate results, the measurements results become meaningless. Constructing the required artefacts for applying metrics-based approaches takes a long time (data collection and consolidation, and models validation). In order to address this issue, we observed that it is important to find a way to get, interpret and present results in a shorter time. For this reason, we argue that the measurement approaches require more agility, i.e. the results should not be delivered at once and intermediate results to lead the ways to measure are needed. Intermediate results are discussed and a deeper analysis can be undertaken if needed. The measurement cycle can thus be shortened. Customisation of the approach - Time Boxing/Design To Cost: The main constraints of a project are the time, the cost and the quality of the resulting product. The time boxing (or design to cost) is a strategy used in practice to indicate the quantity of information, which can be delivered, under the constraint of a limited time (x months) and a fixed budget (y K euros). We argue that the approach can gain in usability if it is composed of fine-grained method chunks which can be applied according to the convenience of the resources involved in the project, i.e. time, budget and actors. In our future work, we will explore the three first directions in order to improve the “usefulness” of our approach.",
  "2010-BPM": "1 Introduction The Case Management or Adaptive Case management is an emerging topic that has been extensively discussed during the last months by the BPM community. In a legislative system or health care, the notion of case has been known for many years: here by a case we understand the set of circumstances or facts related to a criminal act or a patient condition that requires a decision making and a treatment with respect to some norms or regulations. Whereas the norms are well defined, the case-related conditions can vary widely and evolve with time, preventing the agent responsible for the case treatment from applying a standard predefined template or model. The similar notion of the case has been recognized in business process management. The Case Management Process Modeling (CMPM) RFP released by OMG on September 2009 expresses the particular demand of practitioners in the case management solutions [1]. OMG defines case management as “A coordinative and goal-oriented discipline, to handle cases from opening to closure, interactively between persons involved with the subject of the case and a case manager or case team.” Systematic improvement of this process based on the user experience is one of the main objectives of adaptive case management approaches. This objective can be achieved by following the social production principle defined by the social software [26]:”Social production is the creation of artifacts, by combining the input from independent contributors without predetermining the way to do this.” Case management is an example of knowledge-intensive process that cannot be fully determined at design-time. Mechanisms of the process evolution and actors contributing in this evolution cannot be specified in advance either: for example, a manager is not any longer a passive process user, it is by aggregation, analysis, and mining [2] of case stories and associated user/designer/manager/customer experience the case management process can be improved. Therefore a mechanism to integrate the “innovative contributions not identified or planned in advance” should be provided. In order to benefit from the social software principles in business process management and case management in particular, the possibility to communicate, negotiate, and change a process definition based on the aggregated knowledge should be provided at all levels of the process lifecycle starting from the design. Thus, an ! appropriate formalism for business process modeling has to be selected. For the traditional formalisms accepted in the industry, such as BPMN or EPC, this represents a real challenge. Being almost systematically imperative and activity-driven, these formalisms encourage the early specification of the explicit order in which the activities of the process will be executed. On the other hand, these formalisms are often implicit in specifying data that is circulating throughout the process. Therefore, while presenting powerful techniques for modeling and control for the prescriptive business processes, these formalisms fail in specifying knowledge-intensive processes and case management processes in particular. Does that mean that to address the Adaptive Case Management a brand-new formalism is needed? We argue that the research methodologies and techniques developed during the past decade can be successfully tuned and then applied for the case management modeling. In this paper we discuss the use of Declarative specifications [3], Variability modeling [4], and FOL-based Formal semantics for modeling descriptive processes [5] and, in particular, case management processes. We assemble these theoretical concepts in the form of a modeling approach that we call DeCo – for Declarative Configurable process specifications. DeCo process specifications extend the BPMN notation (a de-facto standard for process modeling) providing a mechanism for descriptive process modeling, formal analysis, and step-wise evolution. The reminder of this paper is organized as follows. In Section 2 we provide the business process models taxonomy, position the case management in this taxonomy and discuss the existing business process modeling formalisms. In Section 3 we consider a mortgage approval process and attempt to model this process in BPMN-BizAgi (www.bizagi.com). Based on the encountered challenges, we formulate 5 issues that have to be addressed by a case management process modeling technique. In Section 4 we introduce DeCo process specification and illustrate how the aforementioned issues are handled in DeCo. Section 5 presents our conclusions. 2 From Business Process to Case Management 2. 1 Process model taxonomy Business processes models can be roughly divided into two categories: prescriptive and descriptive. Whereas prescriptive process models specify how things must/should be done, a descriptive process model aims at recording and providing a trace of what happens during the business process [6][25]. In practice, prescriptive models are used to specify processes with predictable sequences of simple tasks and well defined coordination rules (e.g. repetitive, highly automated production processes). Such process can be fully specified at design-time. Prescriptive modeling style, though, is inappropriate for knowledge-intensive processes based on the actor collaboration and information exchange. These processes are characterized by a weak predictability of task sequences and partially defined coordination rules; they can be only “sketched” at design-time by descriptive models. Fig.1 shows the examples of processes that can be specified using one or another (or both) modeling styles. In the global economy the growing interest of organizations to explore new markets can be observed. However, even a simple business process has to be adapted to a new execution context, depending on, among other aspects, cultural and legal considerations [7]. Thus, we distinguish another two categories of business processes models: context- specific and configurable. Highly specialized processes, defined for a given execution environment (e.g. a research experiment in chemistry or physics) can be captured by a context-specific model. Such processes are hardly reusable and for each new environment (or context) a new process (and its corresponding model) has toes be defined. Today, organizations are interested to consolidate their processes while keeping them customizable in order to reflect the context-specific parameters. Processes requiring a customization (such as role/task assignment, task ordering, and rule selection) upon their deployment can be specified using configurable process models. Case management. The glossary of RFP for CMPM defines case as “A situation, set of circumstances or initiative that requires a set of actions to achieve an acceptable outcome or objective. …” In [1], the case management process is addressed as a knowledge-driven process, where activities do not occur in a predefined order. In addition, case management processes supposed to not only anticipate the change of a business context at deployment, but also has to react in the consistent and organized manner on all the emerging case-related knowledge at run-time. Thus, we claim that case management processes shell be specified using descriptive, configurable models (top-right quadrant of our diagram in Fig.1). Specification can be configured at Negotiation at deployment and evitpircseD Specification the Oriental at run-time Criminal is fixed at marketplace Investigation design-time and is not Medical treatment supposed to Specification International evolve can be commercial evitpircserP Automated customized at transaction Manufacturing deployment Research Experiment in Internet Sales Physics Context-Specific Configurable Fig.1. Business process taxonomy. Case management can be considered as a descriptive, configurable process. 2.2 Modeling formalisms Literature provides various process modeling formalisms that we classify into four categories: activity oriented, product oriented, decision oriented and conversation oriented models [8]. The Business Process modeling formalisms defined by Unified Modeling Language, Event-Driven Process Chain (EPC), and Business Process Modeling Notation (BPMN) gain the wide recognition among practitioners today. All these formalisms are based on the activity-oriented and/or product-oriented paradigm for business process modeling. The most recent type of process models [9], [10], [11], [12] is based on the decision- oriented paradigm according to which the successive transformations of the product are looked upon as consequences of decisions. Conversation models are based on the speech act theory and on the principle that each sentence expressed by someone represents an intention, a commitment. BPMN 2.0 beta specification [13] published on August 2009 by OMG supports the increasing demand to the modeling collaborations, communication, and human actors involvement. Though, it incorporates the elements of the decision-oriented and conversational paradigms. A possibility to customize a business process taking into account an environment where this business process is instantiated is a part of the more general problem of flexibility. This problem was identified in [14] and [15] in general and in the context of WfMS respectively: Knoll and Jarvenpaa [14] introduce the term of flexibility as a form of alignment between organizations and their IT systems in turbulent environments. The authors recognize three types of flexibility in the context of IT: flexibility in functionality, in use and in modification. Heinl et al. [15] illustrate the necessity of flexibility in workflow management applications and identify two classes of flexibilities: by selection and by adaption. Another steam of research e.g. [16][17] favors what we refer to as declarative business process modeling. In [16] the representation of a business process as a trajectory in a state space is introduced. The authors attempt to declaratively describe the dynamics of a business process by defining a notion of a valid state and planning rules that make a state valid. Van der Aalst in [17] presents a case handling paradigm to cope with business process flexibility. In contrast to workflow management, case handling aims to describe what can be done to achieve a business goal but not what should be done and how. 3 Example: The Mortgage Approval Process Mortgage approval process is a typical example of a case management process. In this section, we provide a generic mortgage approval process description as defined by different financial institutions in the USA. The information provided below results from our study of multiple information sources (e.g. http://www.homebuyinginstitute.com/, http://www.mortgage-resource-center.com/, http://homebuyereducation.bankofamerica.com/, http://www.homeloancenter.com/ etc.) It represents a compilation of guidelines, recommendations, and descriptions of mortgage approval process, provided by different loan consulting firms, financial advisors, and banks and available on the web. 3.1 The Process Description A mortgage is a loan for buying a house. The terms and length of the mortgages are negotiable and can be adapted for the applicant’s situation. The mortgage approval process can be divided into the following steps: Pre-qualification; Formal application; ! Document review; Pre-approval; Property appraisal; Final approval; Closing. The goal of the pre-qualification step is to determine the potential mortgage amount that the applicant is eligible for. The purpose of the formal application is to provide the lender with documents characterizing in details the current financial situation of the applicant as well as his/her employment and credit history. Document review follows the formal application and may include the pre-approval step. The pre-approval letter issued as a result of this step indicates that the applicant is pre-approved by a lender for a specific loan amount. When the property is selected by the applicant, the mortgage lender initiates the property appraisal. The appraisal step defines the amount of the mortgage and a corresponding down payment. The lender makes “approve”, “not approve”, or “approve with conditions” final decisions based on the document review and the appraisal results. If the loan is approved, a commitment letter is issued for the applicant, and a closing date is set up. Closing (also called settlement) is a final step. During the mortgage closing, the mortgage lenders will need to purchase the house and hold the title as the applicant makes payments to them. 3.2 Specification of the Mortgage Case Management Process Using BPMN Considering the complexity of the complete process, in this paper we will focus on the Formal Application process step. The text below describes this step in details. Mortgage Approval: Formal Application 0 The applicant can request the application package by e-mail or by post. Alternatively, all the forms can be accessed on the Web. 1 Mortgage application can be submitted electronically or during a personal meeting with the mortgage lender. 2 The exact set of documents may vary depending of the financial institution and the particular situation of an applicant. These documents may include: The Social Security card; Record for past two years for residence address; Employer name, address; W-2 tax forms; federal income tax returns; Most recent pay-stubs, etc. 3-5 During the application, the lender provides the applicant with a Good Faith Estimate (GFE) of costs of loan closing; the applicant can be asked to make a final decision on the type of mortgage loan; also an interest rate for the loan can be locked in this phase. 6 Some lenders will give to the applicant an access to their website where the applicant can check on the approval status of his/her package. 7 Usually an application fee and the appraisal fee will have to be paid by the applicant during the mortgage application submission. The main purpose of this description is to illustrate the diversity of activities, actors, and information involved and to stress the unpredictable nature and variability of this process – the characteristics that make it’s modeling a challenging task. Figure 2 presents the model of the Mortgage approval process specified using BizAgi modeling tool. tnacilppA 5. Need to specify impact of 4. Need to specify data to different tasks and multiple flow possibilities the task flow 2. Need to specify role hierarchy, alternatives (possible replacements), and synonyms redneL egagtroM L A N M O R TI O A F C LI P P A 1. Need to specify Inputs/outputs while 3. Need to specify distinguishing obligatory and optional data, optional, obligatory and alternatives (possible replacements), and alternative tasks synonyms (identical artifacts called differently) Fig. 2. BMPN specification of the Formal Application in the Mortgage Approval process. 3.3 Discussion Modeling the mortgage approval process in BPMN, we have encountered the following challenges: Optional tasks. Modeling optional tasks represents one of the major challenges. For example, <Send the forms> task is not needed if an applicant has already downloaded forms from the Web or if he/she fills in the application on-line. Task Ordering. Many tasks, being obligatory for the process, cannot be preordered at design time. Based on the norms and policies of the concrete place/institution, their order may be specified at deployment (e.g. <Get an access to the approval status info>, <Get application fee>). However, some actions (e.g. <Lock in an interest rate>, <Get appraisal fee>) can be ordered only at run-time as they will be executed based on the data availability. Within the traditional BMPN formalism, task ordering can be expressed using a gateway mechanism; however, considering the large number of such ordering options, this mechanism is not efficient. An explicit specification of conditions under which one or another ordering should be applied is also challenging. In our BPMN diagram, the appropriate solution found was to leave the tasks that could not be explicitly ordered at design time outside of the process flow. Optional/alternative data objects and synonyms. The mortgage approval process consumes and produces a vast amount of data artifacts. Some of these artifacts may be obligatory, whereas the others can emerge (or be requested) in specific situations only. For example, the mortgage loan applicant can be asked to provide the mortgage lender with one or multiple different tax forms. The form W-2 (Wage and Tax Statement) is typically requested from all the applicants in USA (obligatory); the form 2555 (entitled Foreign Earned Income) should be provided by taxpayers who have earned income from sources outside the United States. In order to adapt to different applicant’s situations, the alternative data artifacts should also be specified. For example, the form 1040 is a commonly accepted tax return form, whereas the form 1040EZ is a simplified version of this form: for single and joint filers with no dependents the form 1040EZ can replace the form 1040 [source: wikipedia]. Some artifacts also can be called differently from one organization (or country) to another: the tax forms (or IRS forms) defined for USA corresponds to another tax forms in UK. For a mortgage lender working in both countries, having these synonyms explicit can be beneficial. Explicit modeling of data impact on decision making/task ordering. Data artifacts impact the task ordering and decision making at run-time. For example, if the purchase contract is provided during the application, the mortgage lender may want to immediately initiate the appraisal process. We have used BPMN artifacts to specify the relations between data and tasks where these data is required. The way the data is used in general can be indicated using annotations. However, neither one nor another mechanism is formalized in BPMN. Implicit impact of data on the decision making stays out of the process modeling scope within the existing formalism. Role assignment. Whereas abstract process participants – Mortgage lender and Applicant – are easily recognizable, concrete role assignment may depend on the financial institution: small banks have only several (2-4) roles associated with the process, whereas in the big agencies the application processing and decision making is more likely to be assigned to a number of different roles with different responsibilities. These roles can be assigned only upon deployment. As our study shows, the same task can also be performed by several roles – this postpones the role assignment until the run- time. In this section, we have listed only several challenges related to the case management process modeling. The modeling of negotiation and communication, internal decision making, modeling data/actor/task ranges are the problems that will be addressed in the future. To conclude our discussion, we formulate five requirements to a descriptive process modeling approach that the traditional formalisms can hardly meet and that we are going to address in further sections: 1. Need to specify inputs/outputs while distinguishing obligatory and optional data, alternatives (possible replacements), and synonyms (identical artifacts called differently). 2. Need to specify role hierarchy, alternative roles, and synonyms. 3. Need to specify optional, obligatory and alternative task. 4. Need to specify multiple flow possibilities. 5. Need to specify impact of data to different tasks and the task flow. 4 Declarative Configurable Process Modeling Notation (DeCo) In this work, we present the process modeling approach based on Declarative Configurable (DeCo) process specifications that extend the BPMN notation (a widely adopted standard for process modeling with more than 60 current implementations). 4.1 Theoretical Foundations The theoretical foundations of DeCo are grounded on Declarative modeling [3], Variability modeling [4], Refinement theory, and formal methods - the paradigms, proven in the research community. This work is largely based on the authors’ research reported in [5][18][19][12] and is inspired by the results presented in [20][21]. The declarative specifications for modeling business processes have been presented in [19]: this approach is based on the systematic modeling of process-related data. This allows us to introduce the notion of state. Each process task then can be associated with a set of pre-states - the states where this task can (but not necessary will!) be executed - and a set of post-states – the states resulting from the task execution respectively. Consequently, the process specification represents a set of activities (tasks to be done) with no predefined execution scenario. The declarative modeling principles allow one to postpone the decision making about the process control flow until its deployment or even execution. As soon as declarative process specification evolves - the mechanism to validate and to control this evolution is required. In the work presented in [19] the evolution from the declarative, nondeterministic process specification at design time towards precise (imperative) process specification at deployment is represented as a set of refinement steps. The notion of refinement for graphical specifications, adopted from software engineering [3], is presented in [5]. In this work, the formal semantics for graphical specifications is defined based on first order logic and set theory. These semantics allows us to reduce the problem of refinement verification to the validation of the first-order logic formula and provides the means for automated process analysis and control using the tools for automated analysis of program specifications defined in software engineering. The technique presented in [19] allows one to demonstrate that different control flow configurations of the process are valid with respect to a high-level declarative design specification. However, to deal with descriptive processes (and the case management in particular) the process configurability should not be limited by a control flow. In the literature, several major perspectives of the process models are specified [23]: the control flow perspective that captures the temporal ordering of process tasks, events, and decision points; the data perspective that captures the lifecycle of data objects (creation, usage, modification, deletion) within the process; the resource perspective that describes how the process is carried out within the organization and deals with roles and resource assignments; the operational perspective that addresses the technical aspects of process execution and specifies the elementary process tasks and their assignment to concrete applications or application components of the organizations; the context perspective that describes the attributes related to the process execution context; the performance perspective, addressing the process cost effectiveness. In [5][20][21][22] the concept of configurable process has been presented and the modeling formalism to deal with process configurability at multiple perspectives is defined. Namely the authors present the Configurable Integrated EPC (C-iEPC) modeling notation that extends the well known Event Process Chain (EPC) notation and addresses the process configurability along the control-flow, data, and resource perspectives. According to this approach, “Given a configurable process model, analysts are able to define a configuration of this model by assigning values to its variation points based on a set of requirements. Once a configuration is defined, the model can be individualized automatically..” Individualization process can be considered as automated synchronization of the process model perspectives in respond to each configuration decision. This guarantees the correctness of individualized process models by construction. To introduce the notion of correctness and to reason about individualized and configurable models, authors define formal semantics for C-iEPC based on FOL [20]. 4.2 Multi-perspective configurability Inspired by the approach presented in [20]-[22], we propose to address the 3 out of 5 requirements defined in the previous section using the multi-perspective configurability as follows: 1. Need to specify inputs/outputs while distinguishing obligatory and optional data, alternatives (possible replacements), and synonyms (identical artifacts called differently). - This requirement can be addressed by providing configurability along the data perspective. 2. Need to specify role hierarchy, alternatives (possible replacements), and synonyms. - This requirement can be addressed by providing configurability along the resource perspective. 3.Need to specify optional, obligatory and alternative tasks. This requirement can be addressed by providing configurability along the operational perspective. Due to space limitations, we consider in more details only the DeCo implementation of the data configurability. Figure 3 illustrates the data object hierarchy models for Tax forms and Tax return forms required for formal mortgage application in USA. Vast amount of data artifacts consumed and produced by a process represents a challenge for modeling. The hierarchy model of data objects describes: generalization-specialization relations between data objects; • alternatives – data objects that can replace the data object originally required by the • task; For example, the Form 1040EZ can be considered as an alternative of the obligatory 1040 form. The rule 3.2 specifies the condition where this alternative is applicable. Such rule can be described as a text or formalized in FOL. • synonyms – different terms referred to the same data object. For example, in USA, both tax forms and tax return forms can be called Internal Revenue Service (IRS) forms ! [source: wikipedia] ! Fig. 3 The data object hierarchy model for the Formal Application 4.3 Declarative specification of tasks The last two requirements: 4. Need to specify multiple flow possibilities 5. Need to specify impact of data to different tasks and the task flow can be addressed applying the declarative modeling principles as explained in [19][5]. Formal semantics permits to express the action contracts in terms of pre-condition, post- condition and invariants. We define a formal semantics for DeCo specifications based on first-order logic (FOL). Similarly to results presented in [5], it can be used for the mapping of a DeCo specification to the Alloy specification language [24] for further validation. At run-time, at any moment a process enactment is characterized by its state (a case X ( p , p ,.., p ) = state). We define a case state in DeCo as a vector . The 1 2 n p , p ,.., p components are values of data objects related to this case at a given 1 2 n Σ moment of time. A state space is a set of all possible states of a case. A For every process task we define a precondition and a postcondition. Postcondition A A is a condition that a case meets after the task termination. Precondition post pre specifies a condition that must hold upon the task invocation: If A is started in a state A post A satisfying , it is guaranteed to terminate in a state satisfying . pre Σ Precondition and postcondition are modeled as predicates over state space : A : {true, false}, Σ → pre A : {true, false} Σ × Σ → post In addition to pre- and postcondition, invariants for process tasks can be specified. A Invariant is a condition that holds before and after the task execution. inv X X ' A Task defines a transition of the case from state to state (pre- and post-states respectively). We define a task in DeCo as a binary FOL-formula A : {true, false} Σ × Σ → . We specify the task using logical implication between precondition and postcondition: def A( X , X ') A ( X ) A ( X , X ') = → pre post X If at a given state the precondition Apre of the task A holds, then the case will be transited to a A X ' state , for which the postcondition of A - - holds. post Preconditions, postconditions and invariants explicitly relate tasks with data objects within a case. Example: To specify the contract for the <Send the Forms> task from Fig.2, we define 2 supplementory predicates: ∃ ∧ isReceived(d: DataType, a:Applicant) { m:ReceivedMessage | m.content = d m.src = a } ∃ ∧ isSent(d: DataType, a:Applicant) { m:SentMessage | m.content = d m.dst = a } The former predicate evaluates to True if in the process there exists a message m received from an applicant a that contains a data object d. Along those lines, the second predicate evaluates to True when the message m with a content d is sent to an applicant a. We define the precondition and the postcondition for the <Send the Forms> task as Pre: isReceived(FormRequest, applicant); Post: isSent(AppForms, applicant) follows: There are no invariants specified for this task. The task itself can be denoted as follows: ∀ apl:Applicant | isReceived(FormRequest, apl) -> isSent(AppForms, apl) : Whenever the process receives a form request from an applicant apl, then the execution of this tasks will move the process in the state where the application forms are sent to this applicant. 5 Conclusion and Future Work OMG RFP demonstrates the increasing interest and the particular needs of the practitioners in the methodologies and tool support for case management process [1]. According to OMG RFP, the objectives of the Case Management methodologies or tools are (i) to accommodate and structure the knowledge of participants about the current case, (ii) to provide the information of the previous similar cases and help the practitioners to learn from best practices, and (iii) to progressively improve case management processes to make them “evolve toward prescribed processes.” DeCo defines the concepts for systematic gathering, and structuring the information about the data (consumed or produced), the roles, and the tasks of a process. Namely, based on the configurability modeling along multiple process perspectives, we specify generalization/specialization, synonym, and alternative relations. Compared to traditional business process modeling formalisms, DeCo provides the means to support the prescriptive nature of the case management process: the declarative modeling principles allow the designer to specify the set of activities that could (but not necessary will) be executed during a process invocation together with the conditions under which this execution can be possible: {pre, post, inv}. Though extending it, DeCo is based on the BPMN graphical notation. Our main objective with DeCo is not to provide yet another set of concepts and shapes representing them, but to reuse as much as possible the notation that (a) has already been known and used by practitioners for years and (b) has a significant developers’ support. This can help us to get a useful feedback from the BPM practitioners’ community while conceptualizing DeCo. Social software is software that supports the interaction of human beings. Thus, being largely dependent on such interactions, case management process can benefit from supporting tools based on the social software principles. The following three important missions can be successfully fulfilled by social software: (i) collection of process requirements for their further incorporation into the process design model; (ii) collection of the information related to the process context for the further process model customization at deployment; (iii) collection of the content (case subject) – related information at the process run-time for the further process model analysis and evolution (e.g. implementing process mining [2]).",
  "2010-GRN": "Innovative Enterprise Architecture (EA) approaches developed by researchers and focused on the evaluation and the analysis of organizations usually require an extensive set of “measurable” input parameters. In practice, organizations often cannot provide the required data. This makes the implementation of these approaches difficult (if at all possible). In this work we specify the build-up process that represents a systematic identification, gathering, and consolidation of data required by the target EA approach. We illustrate the build- up process on the example: We present a case study where we implement the research approach to business-IT fitness measurement in the context of a real industrial project in a mass retail company: Initially, the poor quality of the input data presented a serious threat for business-IT alignment assessment using the fitness measurements. After the input parameters have been built up, the applicability of the fitness measurements was significantly improved (from 10% to 90%) and alignment assessment was accomplished successfully. We believe that using the build-up process it is possible to increase the applicability of any research approach in EA and, in spite of the initial absence of input measurable parameters, to deliver meaningful results to practitioners. design science INTRODUCTION Nowadays, numerous approaches aiming at validation, measurement, and improvement of business-IT alignment are developed by the research community (Bleistein 2005; Bodhuin, 2004; Kearns & Ledere, 2003; Campbell & Avison, 2004; Luftman, 2000; Rychkova, 2008; Simonin, 2007; Wegman, 2005; Wieringa, 2003). However, many of these approaches have difficulties in being adopted by the industrial organizations whose needs they are addressing. To identify and eliminate a malfunction in a car, a mechanic needs to know the vehicle’s technical characteristics (from the car registration book), its current state, and accident history (from the car maintenance book), and its condition of exploitation. This information represents an important input for the diagnostics and repair. Along similar lines, approaches for diagnostics and assessment of business-IT alignment strongly rely on their input data - models, specifications, and other documents that describe an organization “As-Is”. The quality of this input data is one of the main factors affecting the applicability of EA approaches. Research approaches in EA are particularly sensitive to input data quality. In practice, Problem: organizations cannot meet the high requirements that researchers define for the input data (e.g. data models, business process models, IS documentation, etc.). Often such data is incomplete or does not exist at all. This prevents the innovative research approaches from being adopted by practitioners (Fig.1). In this work we do not encourage the organizations to improve their process documentation in order to benefit from innovative research. In contrast, we claim that researchers themselves can significantly improve the applicability of their approaches. In this article we discuss the guided implementation that allows a researcher (or a group of researchers) to build up necessary input data based on the documentation and other information sources available in the organization. We believe that using the build-up process, it is possible to increase the Hypothesis: applicability of the research approach and, in spite of the initial absence of input measurable parameters, to deliver meaningful results to practitioners. The approach to measure the fitness relationship between business and IT in organizations presented in (Etien & Rolland, 2005) is an example of an EA approach developed in academia. This approach addresses the problem of business-IT alignment: it defines metrics to quantify the fit between the business of an organization and the IT systems that support it. Metrics application (the process called here and below the fitness measurement) allows organizations (i) to identify precisely the aspects of business (business goals, activities, and so on) that are not supported or ill-supported by the existing IT and (ii) to specify the strategies to improve business-IT alignment. In this work, we present a case study that validates our hypothesis on a practical Method: example. The subject of our study is the fitness measurement approach proposed in (Etien & Rolland, 2005). We apply this approach to a project of business-IT alignment assessment in a mass retailer company - the ABC-Supermarket (by the agreement with the company, its real name will not be disclosed). To increase the applicability, we define a process that builds up required measurable parameters based on the available documentation and other information sources. Our experience shows that small companies (10-50 people, up to 20 Case Evaluation: processes and applications) typically have no difficulties providing the information required for business-IT alignment assessment. Among the reasons, we can highlight the following: process and information system landscapes of such companies are not too complex and kept under control; the roles of data and process owners are well defined and can serve the trustworthy information sources. This is not always the case for the big organizations with hundreds applications and processes on place. In such organizations the problem of inconsistent or missing data becomes a serious threat for business-IT alignment assessment. Thus, the bigger an organization, the worse applicability of the fitness measurement approach can be expected. Figure 1. [The traditional implementation of a research approach for Business-IT alignment assessment] ABC-Supermarket is a mass retail company - one of the leaders on the French market. Today ABC-Supermarket counts approximately 3000 independent operators and thousands of outlets in France and internationally. This company encounters ~130 000 employees (including 700 specialists working in IT); it runs about 120 business processes supported by 220 applications. Thus, for our study, ABC-Supermarket represents a critical case: we claim that if our hypothesis is valid for ABC-Supermarket case, it is likely to be valid in many other cases. In this work we define a build-up process that consists of three main phases: (i) identification of the parameters to build up (ii) data collection, (iii) data consolidation. Each of these phases is followed by a validation. We also specify techniques that can be used in each phase. We implement the build-up process for the fitness measurement approach and illustrate it on the ABC-Supermarket case. The results of our study confirm our hypothesis: they show that the build-up process increases the applicability of the fitness measurement approach from 10% to 90%. Solely based on the data available in the ABC-Supermarket only one metric out of ten specified in (Etien & Rolland, 2005) can be applied; implementing the build-up process, we retrieve the input data for eight other metrics. This article has the following structure: In Section 2 we present the state of the art; in Section 3 we define the generic build-up process that can be customized for an arbitrary research approach in EA. In Section 4 we introduce the fitness measurement approach and identify the input parameters required by this approach. In the second part of this section we introduce a case study: the application of the fitness measurement approach to ABC-Supermarket. A gap between required input parameters and available data prevents the complete application of fitness metrics. In Section 5 we show how the build-up process is used to fill this gap and to improve the applicability. In Section 6 we present the summary of the ABC-Supermarket fitness measurement project. In Section 7 we discuss the lessons learned and present our conclusions. STATE OF THE ART The validation and the assessment of business‐IT alignment is a subject of continuous interest in research and industrial communities. For practitioners, validation of business‐IT alignment is an important part of organization government; for EA researchers, approaches to accurate alignment measurement pave the way to new theories in the field (E Chan & Reich, 2007). These approaches strongly rely on quality of their input data that represents domain­specific knowledge of an organization. Collection of domain‐specific knowledge is not a trivial task: it depends on the organization, the problem scope, and the complexity of the EA approach itself. In this section, we discuss several approaches for the validation and the assessment of business‐IT alignment and we investigate (i) the awareness of these approaches about the domain‐specific knowledge collection and (ii) the instruments of domain‐specific knowledge collection used by these approaches. Many approaches to business‐IT alignment validation and assessment are addressed in the literature. In this section, we discuss three different groups: the questionnaire‐based approaches, the framework‐based approaches and the approaches based on metrics. Questionnaire­based approaches. The approaches defined by Renner (2003), Kefi (2003) and Beeson (as cited in Etien, 2005) specify questionnaires as instruments for domain‐specific knowledge collection. They are implemented as follows: the specialist in charge of business‐IT alignment validation conducts interviews with business and IT actors in the organization. The issues addressed by the questionnaires include: “successes and failures in communication between business and IT actors”; “successes and failures in understanding of the IT domain by the business actors”; “successes and failures in understanding of the Business domain by the IT actors”; etc. Kefi (2003), for example, addresses the problem of alignment by asking the interviewees to answer the following question: “On a scale of 1 – 5, how do you rate IT alignment in your organization?” The goal of these interviews is to reveal the mutual understanding of the business and IT strategies of the organization by its actors. The approaches discussed above result in a single indicator characterizing the alignment. The approaches listed below provide more detailed analysis of the alignment in organizations: In (Bergeron, 2004), authors define an approach to assess the alignment between IT structure and IT strategy. They specify a questionnaire that addresses four issues: IT environment perception; IT strategic use; IT planning and control; IT acquisition and implementation. As a result four alignment indicators are produced. Kearns and Ledere in (Kearns & Ledere, 2003) propose another questionnaire‐based approach that evaluates 12 different aspects of business‐IT alignment in organizations: 6 aspects are related to the alignment between an organization business plan and its IT plan; and 6 other aspects are related to the alignment between the IT plan and the business plan. The common characteristic of questionnaire‐based approaches is that they are solely based on the analysis of personal experience and understanding of the situation by the actors in the organization. To make the alignment assessment results more objective, various techniques can be used. One of them is proposed in (Burn 1993, 1996): in this publication, the author defines the Organizational Culture Audit (OCA). The OCA can be considered as an instrument for alignment assessment. It addresses the following issues: the external strategy vs. the IT strategy; the internal infrastructure models for business and IT, etc. The author claims that (i) the business‐IT alignment is an ongoing process and (ii) in order to provide accurate and objective assessment results, the OCA should be applied regularly (e.g. once per year) by different managers. Framework­based approaches. Capability Maturity Model Integration (CMMI) is a framework for assessment of processes in organizations (http://www.sei.cmu.edu/cmmi/). It originates from CMM – Capability Maturity Model for software development processes. Mature processes can be characterized as transparent, reliable, flexible and agile, i.e. they are able to efficiently support any change of the organization strategy. CMMI appraises the current maturity level of processes in an organization and offers guidelines to improve these processes and to bring them to the next maturity level. SCAMPI ‐ Standard CMMI Appraisal Method for Process Improvement – is the appraisal process defined for CMMI (http://www.sei.cmu.edu/cmmi/). SCAMPI specifies three levels of formality for appraisals: Class A, B, and C. SCAMPI appraisal processes are conducted by SEI‐authorized Lead Appraisers and are based on the well‐defined process specification (SCAMPI Upgrade Team, 2006). SCAMPI can be considered as an instrument for the domain‐specific knowledge collection for CMMI. In (Luftman, 2000), the Strategic Alignment Maturity Assessment framework is defined. This framework is inspired by CMM and is used for assessing the alignment between business and IT strategies. This framework defines five maturity levels of processes based on industrial best practices: 1. Initial/Ad hoc; 2. Committed; 3. Established/focused; 4. Improved/managed; 5.Optimized. Similar to CMMI, at each maturity level, this framework offers guidelines to improve the processes and bring them to the next maturity level. An appraisal process for this framework is specified in (Luftman & Brier, 1999). This process plays a similar role as SCAMPI for CMMI – it defines and consolidates the domain‐specific knowledge and helps to prepare the organization for the actual assessment. Approaches based on metrics. These approaches address multiple aspects of alignment in organizations using various metrics. An approach presented in (Bodhuin, 2004) defines two metrics for alignment assessment: the technical coverage, which stands for a percentage of process activities adequately supported by a software system; and the technical adequacy, which refers to the quality of the IT support. Simonin (2007) proposes an approach based on Model Driven Engineering (MDE) to measure static and dynamic alignment. To our knowledge, no instrument or technique for domain‐specific knowledge collection was defined by these approaches. Except for questionnaire‐based approaches, other approaches for alignment validation and assessment often rely on domain‐specific knowledge presented in a concrete form – the form of models (data models, business process models etc.) and specifications (requirements specifications, IS specifications etc.). These models and specifications represent more objective and accurate sources of information. However, development of such models is an expensive and time‐consuming task that many organizations are not willing to do. Reich and Benbasat study the alignment in organizations based on written business plans, IT plans, self‐reports etc. (Reich & Benbasat, 1996). Authors report on the low applicability of their approach: two out of four alignment metrics were applicable. Among the reasons, they identify the following: IT and business plans often do not exist; self‐ reports are not accurate. Beats reports on the measurement of business‐IT alignment in the banking industry (Beats, 1996). The author states that IT alignment is hindered by a lack of domain‐specific knowledge among banking managers. According to (Reich & Benbasat, 2000; Baets, 1992), corporate strategy is often unknown, or unclear. Campbell (2005) argues that formal business strategies are often too ambiguous for business managers to understand. As our experience shows, research approaches in EA are particularly sensitive to the lack of mature data and process documentation in the industrial organizations: These approaches often require a complete set of specifications (business and IT). On the other hand, they are based on the assumption that all these specifications are available. Such research approaches obviously have serious applicability problems as soon as they are exposed to realistic environments. (E Chan & Reich, 2007) argues that the alignment research is mechanistic and fails to capture real life. Ciborra (1997) claims that scientific literature addressing alignment is too theoretical. He recommends a Mintzberg‐like approach, where researchers should go to the field and get insights from practitioners (Mintzberg, 1973). THE BUILD­UP PROCESS SPECIFICATION The traditional process of implementation of a research approach in EA is illustrated in Fig. 1: it includes (1) the application of the approach by a researcher using the input data provided by the organization and (2) the interpretation/validation of the results by the organization. In reality, only a part of the data required by the approach is available in the organization. The rest of input parameters are either incomplete or missing. This makes the implementation of the approach difficult (if at all possible). As practice shows, the majority of researchers are not addressing domain-specific knowledge collection in their works. We argue that explicit specification of such an activity for a given research approach can significantly improve the applicability of this approach. In this section we define the build-up process as a process of constructing measurable parameters required by a given (research) approach. Any research approach in EA that requires measurable parameters to be provided as input can potentially benefit from this build-up process. What is the Build­Up Process? The build-up process takes its roots in the design science. The design-science in IS focuses on the problems characterized by “unstable requirements and constraints based upon ill-defined environmental contexts, … by a critical dependence upon human cognitive ... [and] social abilities” etc. (Hevner et al., 2004). In this work, we investigate the problem that exhibits the similar characteristics and we propose the build-up process as a solution. “Design science addresses research through the building and evaluation of artifacts designed to meet the identified business need.” (March and Smith, 1995, Hevner et al., 2004). We take as a reference SCAMPI (SCAMPI Upgrade Team, 2006) and follow the principles of agile development methodology (http://agilemethodology.org/) in order to define the building and the evaluation activities for our build-up process. By artifacts in our case, we understand models that represent a problem domain of the organization. SCAMPI (Standard CMMI Appraisal Method for Process Improvement) defines the appraisal process for Capability Maturity Model Integration (CMMI). From SCAMPI we adopt the following phases of the build-up process: 1. Identification of the parameters for build up 2. Data collection 3. Data consolidation By definition, the build-up process represents a systematic identification, gathering, interpretation, and consolidation of the data required by a target EA approach. “Agile development methodology attempts to provide many opportunities to assess the direction of a project throughout the development lifecycle.” (http://agilemethodology.org/). Following the agile methodology principles, the extensive communication between a researcher and the domain experts in the organization – is the core of the build-up. Therefore, each phase specified above and performed by the researcher is complemented by a specific validation (or inspection) phase, where the main role is played by the domain experts: 1.a Inspection of the required data 2.a Validation of the collected data 3.a Interpretation and validation of the consolidated (built up) data Figure 2 illustrates the phases of the build-up process. Below, each phase is discussed in detail. Figure 2. [Guided implementation of a research approach for Business-IT alignment assessment: Identification of the parameters to build up, data collection, data consolidation and corresponding validation phases are introduced]. The requirements for an effective design-science research are formulated in the form of seven guidelines: design as an artifact; problem relevance; design evaluation; research contributions; research rigor; design as a search process; communication of research (Hevner et al., 2004). The vlidation of the build-up process against these guidelines is outside the scope of this paper and will be addressed in our future work. The Main Phases of the Build­Up Process Phase 1: identification of the parameters for build­up The goal of this phase is to identify what input data has to be built up. During this phase, the researcher has: To study the objectives and constraints of the target approach in order to derive the − required input parameters; To identify the available data relevant to the problem scope; − To evaluate the available data against the required input parameters; − Phase 1.a: inspection of the required parameters This phase teams up the researcher and the domain experts in the organization. The goal of this phase is: To familiarize the domain experts with the target approach terminology; − To familiarize the researcher with the vocabulary used and/or accepted in the − organization; To express the approach requirements in domain‐specific terms; − To identify the parameters that cannot be built up (should be ignored); − We focus on the last point in detail: To identify parameters that have to be ignored, the team should inspect each required parameter x answering the following question: “How the parameter x can be validated?” or “Who can approve or disapprove x in the organization?”. If there is no means to validate x – it should be ignored. The application scenario of the target approach should be modified (e.g. reduced) accordingly. As a result of phases 1 ‐1a, the researcher should be able to identify which input parameters are available in the organization, which can be built up, and which should be ignored. Phase 2: data collection This phase is based on observations and gathering of trustworthy evidence about the organization “As‐Is”. During this phase, the researcher has: To identify the relevant data sources (documents, knowledgeable individuals); − To use systematically the techniques of data collection; − By document sources we mean all available documentation (i.e. business documentation, technical documentation, intranet, and any other resources related to the problem area). Knowledgeable individuals are people in the organization (project leaders, managers, end users) whose knowledge and understanding of a problem context can contribute to the alignment assessment. Data collection techniques may include but are not limited to: interviews, questionnaires, surveys, reverse engineering, documentation studying, role playing, workshops, mind mapping, etc. Phase 2.a: validation of the collected data To guarantee that the result of the build‐up process is accurate, the collected data has to be validated by the researcher and by the domain experts in the organization. The collected data is valid if it is accurate, corroborated, and consistent. The validation process specified in SCAMPI provides the definitions for the accuracy and corroboration. We adopt these definitions for the build‐up process. The collected data is accurate if (i) it can be explicitly associated with one or several required input parameters of the target approach, (ii) it was derived from trustworthy evidence seen or heard during data collection sessions; (iii) it is clearly worded and expressed in terms used at the organizational unit. The collected data is corroborated if (i) it is derived from at least two different sources (individuals or document sources); (ii) it is collected from at least two different data gathering sessions (interviews, or meetings); (iii) it is cross‐checked with other observation results; The collected data must be consistent with other validated observations and collected data. The researcher can assure the accuracy, the corroboration, and the consistency of the collected data by setting up the collection process and by using the data collection techniques according to the indications above. Phase 3: data consolidation This phase aims at processing, restructuring, and formalizing the collected data. As a result of this phase, the researcher develops a set of models and specifications that can be directly used by the target approach for the alignment assessment. Data consolidation techniques may include but are not limited to: - Model engineering (i.e. business process modeling, goal modeling, data modeling, other modeling techniques) - Requirements engineering (requirements identification and formalization) - Process mining - Data mining - Extreme programming or rapid prototyping (when the better understanding of the processes and/or applications is required). The examples of consolidated data are: business process models, UML diagrams, business rules, requirements models, code, etc. Phase 3.a: interpretation and validation of the consolidated (built up) data The domain experts play the main role in this phase. The goal of this phase is: To familiarize the domain experts with the models and specifications obtained during − consolidation; To confirm the correctness and unambiguity of the consolidation results; − To validate the consolidation results against the collected data and to prove that no − contradictory conclusions are made. To guarantee the correctness of the built up parameters, the consolidated data must be consistent with other valid observations. In aggregate, all collected and consolidated artifacts should constitute a set of truths about the organization. CASE STUDY: MEASURING A FITNESS RELATIONSHIP IN ABC­SUPERMARKET Fitness Measurement Approach In (Etien & Rolland 2005), the authors specify ten criteria and the corresponding metrics to evaluate the fitness relationship between the business and the system supporting it. Authors use the Wand and Weber (WW) ontology (Wand & Weber, 1992) to represent the components of the system (IT) level and Soffer and Wand (SW) ontology (Soffer & Wand, 2004) - to represent business concepts. These two ontologies are adaptations of Bunge’s ontology (as cited in Etien & Rolland, 2005), which is largely recognized for its theoretical foundations. Fitness relationship is expressed in terms of relations between WW and SW ontologies. Table 1 shows the list of fitness criteria, their corresponding metrics, and the way to measure them. For example, the fitness criterion called Goal satisfaction describes how the strategic or business goals, specified within an organization, are supported by the IT systems existing in this organization. The metric defined for this criterion is a goal count. Goal count can be measured by calculating the ratio between the business goals explicitly represented by the corresponding states of the IT systems and the total amount of business goals. The measured value 0 < goal count ≤ 1 can be then analyzed: if goal count = 1, then all goals are taken into account. Definitions: Business goal (Soffer & Wand, 2004) is a set of stable states of business objects we seek to achieve. Business object is an object that represents the entities in the business domain. Business state (or business object state) is a state of a business object at a time t. It is defined by the values of all attributes of this business object. Business actor can be defined as someone or something that interacts with the business or IT system using an interface; it participates in a business process and triggers external events that result in a state transition of a business object. Business resource (Soffer & Wand, 2004) can be considered as a business object that is passive – i.e. it neither initiates actions nor causes a state change. In our case a product specification is an example of a business resource. System class (or system object) is an object that represents the entities in the IT system (by analogy with a business object). System objects are often identified with classes in the UML class diagram of the system (the reason why both terms class and object are used). System event (Soffer & Wand, 2004) is associated with a system state change. By analogy with business activities that are changing business objects’ states, we consider system events changing states of system objects (or classes). A system event e can be described by a triple of state <g, s, s’> where s is an initial state of the system, s’ is a resulting state of the system, and g(e,s)=s’ is a state transition caused by the event e. System goals describe purposes of the system (Etien, 2005). We say that a system goal maps some business goal if the states of business objects associated with this business goal are represented by the states of the corresponding system objects (or classes) in the system. System state (or system object state) is a state of a system object (or class instance) at a time t. It is defined by the values of all attributes of this object. Paths are sequences of business (or system) states. Business laws represent legal rules and principles adopted by business organizations. Business processes of an organization are implemented according to these laws. Therefore, with respect to the business laws some paths of states can be considered as legal and others – illegal. Table 1: Fitness measurement framework (Etien & Rolland, 2005) Fitness criteria Fitness metrics Measurement details Number of activities represented by system 1. Support ratio Activity count events/Number of activities 2. Goal Number of business goals represented by the system Goal count satisfaction goals/Number of business goals Number of business actors represented by the system 3. Actor presence Actor count user interfaces/Number of business actors 4. Resource Number of business resources represented by the Resource count presence system classes/Number of business resources Business Number of business objects represented by system 5. Information object/System class classes/Number of business objects completeness mapping count 6. Informational Business /System Number of business states represented by system accuracy state mapping count states/Number of business states Business The same as information completeness but for one 7. Activity object/System class given activity completeness mapping count Business/System The same as information accuracy but for one given 8. Activity state activity accuracy mapping count Number of business laws (where each business state 9. System Law-mapping count is represented by a system state and the transitions reliability between business states are represented by the transitions between the corresponding system states) /Number of business laws Number of paths (where each sequence of business 10. Dynamic states connecting initial and final state is represented Path mapping count realism by a corresponding sequence of system states)/Number of possible paths of business states. Case: Product Referencing in the ABC­Supermarket ABC-Supermarket is a mass retail company - one of the leaders on the French market. It counts approximately 3000 independent operators and thousands of outlets. This company specialises in different sectors of retail business and is well known in both food and non-food retail markets. The initial specialization of the ABC-Supermarket is food and household products. Seven years ago ABC-Supermarket integrated a new product category – textile – in its portfolio and defined a new trade name - ABC-Fashion. To provide the IT support for purchase (upstream) and retail (downstream) activities for textile products, the company decided to use an already existing information system that supports the food products. The upstream activities of ABC-Fashion include marketing, product referencing, provider referencing, outlet billing, etc. These activities are supported by the existing upstream information system (UIS) of the company. The downstream activities of ABC-Fashion address product management in the outlet stores, e.g. stock replenishment. These activities are supported by the existing downstream information system (DIS). Among the tasks listed above, the most critical one is the product referencing task performed by the Product Referencing application: this task maintains the link between the upstream and the downstream information systems. The outlets are using DIS to order products available in UIS. If a product is not referenced in UIS, it is not available for ordering. Initially, the reuse of the existing Product Referencing application for ABC-Fashion seemed justified: retail business defines similar processes and master data for food and textile products. Master data also have a lot in common: both food and textile products are characterized by their type, price, etc. However, some differences still had to be addressed. We focus on two of them: (i) from the planning perspective, food products can be divided in two categories: permanent (always on the shelf) and non-permanent. For the textile products three planning categories are specified: permanent products, collection products (e.g. summer / winter), and short-cycle products (e.g. fashionable articles, brand promotions, etc.). Each product category requires its own referencing process; (ii) food products are referenced by logistic units. A logistic unit for the product P specifies a container, a pack, or a box that contains x identical items (or x kilograms) of P (e.g. 10kg box of tomatoes). It also represents a minimum amount of P that can be ordered, delivered, or stored in a warehouse. As the items in the box are the same - there is no need to reference each product item separately. For the textile products this does not hold as each logistic unit may have multiple product variations (e.g. jeans of different sizes and colors). Thus, each item of the textile products should be referenced separately, based on its size and color. Figure 3 illustrates the difference between food and textile logistic unit contents: Figure 3. [Logistic unit for food products (on the left) and its application to the textile product (on the right)] Neither additional planning categories nor specific data attributes for textile products are supported by the existing UIS and DIS. As a solution, numerous manual fixes and workarounds have been developed after several years. As a result, the existing information system got overloaded with patches and became not efficient. The company decided to make a revision of its information systems (UIS and DIS). The main objective of this revision is to understand, whether the existing information system (considered as a whole) can sustain the textile product category in general and product referencing in particular? And if not – what improvement can be made? To answer these questions we apply the Fitness Measurement Approach. APPLICATION OF THE BUILD­UP PROCESS TO THE CASE STUDY In our case study, we are using the Fitness Measurement Approach to evaluate how well the existing Product Referencing application of the ABC-Supermarket handles the following issues: Referencing the three textile product categories: permanent, collection, short cycle. ‐ Referencing the textile products taking into account the information about product size and ‐ color. Identification of the Parameters for Build­Up Table 2 illustrates the situation encountered by the researcher in ABC-Supermarket in the beginning of the project: Column 1 presents all the parameters required for fitness metrics implementation (derived from Table 1). Part A lists all the parameters that are related to business and can specify the viewpoint of the ABC-Fashion on the product referencing activities. Part B lists the IT related parameters that characterize the existing information system support for the Product Referencing. Column 2 indicates the availability of these parameters in the organization. For available parameters the source of information is also specified. Table 2. Input parameters required by Fitness measurement approach and their availability in the beginning of the project. 1. Required measurable parameters : 2. Recognized/ Available in the organization (y/n) A. BUSINESS ARTIFACTS RELATED TO PRODUCT REFERENCING 1 List of business activities Y (from business process models) 2 List of business goals N 3 List of business actors N 4 List of business resources Y (from business process model) 5 List of business objects N 6 List of business states N 7 List of business state transitions N 8 List of business laws Ignored B. IT ARTIFACTS RELATED TO PRODUCT REFERENCING 1 List of system events Y (from user manual of the Product Referencing application ) 2 List of system goals N 3 List of system user interfaces or system classes N representing actors 4 List of system classes (representing resources) N 5 List of system classes (representing business N objects) 6 List of system states N 7 List of transitions between system states N 8 List of allowed system state sequences (paths) Ignored The fitness measurement approach requires 16 input parameters. The researcher and a group of domain experts have inspected these parameters and received the following results: Only 3 parameters out of 16 are available in the ABC-Supermarket. This allows the researcher to apply one fitness metric out of ten presented in Table 1 (the Support Ratio metric). To improve the applicability of the fitness metrics, we implement the build-up process to construct as many required parameters as possible. 13 parameters out of 16 are identified as candidates for the build-up process. Due to the lack of related data sources, 2 parameters, namely, List of business laws and List of allowed system state sequences have been excluded from the consideration. Therefore, 11 parameters will be built up. Challenges The main challenge we met during this phase was related to identification of the problem scope: The problem was not well formulated; even with the help of the domain experts it was difficult to identify relevant (and exclude irrelevant) data that will contribute to the further alignment assessment. We proceed with the build-up process as follows: 1. Data collection 1.1. We collect the three parameters, available in the organization: List of business activities (A.1 in Table 2), List of business resources (A.4), and List of system events (B.4); 1.2. We gather business and IT artifacts for the 11 parameters A.2-3, A.5-7; B.2-7 from Table 2 by studying the document data sources and conducting the interviews with business and IT actors in the organization; 1.3. We validate the collected data conducting the interviews with domain specialists in the organization; 2. Data consolidation 2.1. We consolidate the collected data (e.g. by developing necessary models and specifications) and build up the missing parameters; 2.2. We validate built up parameters conducting the interviews with domain specialists in the organization. Below we illustrate the collection and the consolidation phases on several examples. The complete summary of the build-up process implementation is presented in Table 3. Data Collection Collecting the business artifacts for the build­up process List of business goals (A.2) Sources: Business process models; ‐ Business actors (the head of marketing department of ABC-Fashion, the head of referencing ‐ department, IS users). Techniques: We studied the process specifications identifying the keywords. Based on these keywords we formulated the business goals. We conducted interviews with business actors in order (i) to complete the process specifications by adding missing activities, (ii) to develop non- existing business process specifications and (ii) to validate our definitions of the business goals. Result: 9 business goals was obtained Collected data examples: Create permanent products; Create collection products; Plan commercial operations; Manage product prices; etc. List of business objects (A.5) Sources: Business process models; ‐ Business actors (the head of the referencing department, IS users); ‐ Techniques: the initial information is collected from the business process documentation. To complete our study, we conducted interviews with business actors. Result: 15 business objects are collected. Collected data examples: Commercial operation; Collection product; Short cycle product; Permanent product; Logistic unit; etc. List of business objects states (or business states) (A.6) Sources: Business process models; ‐ Business actors (the head of the referencing department, IS users); ‐ Techniques: the initial information is collected from the business processes documentation. We have validated the derived list of states by conducting the interviews with the head of the referencing department and IS users. Result: 46 business object states are collected. Collected data examples: States of Collection product: {referenced, selected, selection validated, selection rejected, price updated, reference changed, renewed in the new season, discounted} Collecting the IT artifacts for the build­up process List of system goals (B.2) Sources: Application documentation (short description of application functionality) available in the Intranet Techniques: no special technique. Result: one system goal relevant to our study: the Reference products goal. List of System objects (classes) representing business objects (B.5) Sources: Application documentation: data dictionaries, screen shots, user manuals; ‐ IT actors (the application administrator, application designers, developers); ‐ Techniques: Testing of IS and analysis of the available documentation. To complete the collection and validate the initial findings, we also conducted interviews with the IT actors. Result: 21 system objects are identified. Collected data examples: Subsidiary; Management category; Purchase code; Season; Product file; Package content file; etc. List of system object states (or system states) (B.6) Sources: User manual for the Product Referencing application; ‐ IT actors (the application administrator); ‐ Techniques: Testing of IS and analysis of the available documentation. We have validated the derived list of states by conducting the interviews with the referencing application administrator. Result: 54 system states are collected. Collected data examples: States of the logistic unit object: {created, extended, rejected, deleted, duplicated, updated, waiting for technical validation, waiting for effective validation} Challenges During the data collection we faced the following problems: (i) The identification of data sources (documentation and knowledgeable individuals) was extremely difficult: data and process owner roles were not explicitly defined in the organization and the information was collected from various data sources; specialists involved in the data gathering process often provided contradictory information; available documentation was often outdated or incomplete. (ii) It was challenging to validate the collected data: Due to the absence of the single “truth”, it was difficult to cross-check the data and to show that it is corroborated. Data Consolidation In this phase we analyze and consolidate the row data collected in the previous phase, and create the missing measurable parameters from Table 2. In our case study, we use two consolidation techniques: the intentional modelling called map (Rolland, 1999) and UML modelling. A map is a process model that specifies a non-deterministic ordering of intentions and strategies. Here the intentions mean goals that should be achieved. The strategies represent ways to achieve these goals (Rolland, 1999). The map diagram represents a labeled directed graph with intentions as nodes and strategies as edges between intentions. Two special intentions called “Start” and “Stop” indicate respectively the beginning and the end of navigation in the map. A section in a map is the triplet <I , I , S > i j ij where I is the source intention, I is the target intention and S is the strategy connecting the i j ij source and target intentions. A section can be refined by a map. In our project, we produce two types of map diagrams for the product referencing process: Business map describing the product referencing as it is seen by ABC-Fashion 1. management (Figure 4(a) and 4(b)) System map describing the product referencing process as it is implemented by the 2. existing information systems of ABC-Supermarket; (Figure 6(a) and 6(b)) We developed a UML business class diagram that specifies the data model for the product referencing as it is defined by ABC-Fashion. Using a reverse engineering we developed a UML class diagram that specifies the data model for the Product Referencing application. Business artifacts consolidation List of business goals (A.2) Techniques: Consolidation process using business map (Fig. 4) permitted us to: Identify new business goals (that were not captured during the data collection) and add ‐ them to the list; Identify relations (subgoal-supergoal) between goals and place these goals at different ‐ levels of abstraction within map diagrams; Identify redundant goals and remove them from the list. ‐ Figure 4. [(a) The Business map for Product referencing; This map contains two goals: Plan commercial operations and Reference products. Goals are connected by arrows representing business strategies. To achieve the Reference products business goal three strategies are identified: by making short cycle offer (C3), by building collections (C4), or by selecting permanent products (C5); (b)The Business map for Collection products referencing. This map presents the refinement of the section C4 of the Product referencing - <Plan commercial operations, Reference products, by building collections>. In a similar way sections C3 and C5 can be refined]. Result: 11 business goals at 2 different hierarchical levels are defined; Examples: plan commercial operations, reference products, (for collection product) make the collection plan, select collection products, create collection products, etc. Figure 5. [A fragment of the UML business class diagram for the product referencing. The ABC- Fashion product is represented by the corresponding class that has 3 specializations: Collection_Product, Permanent_Product, Short_Cycle_Product. Each product is associated with the corresponding specialization of a Product_Management. Each product can exist in several sizes and colors: this is reflected by the Color/Size class. Different products can be aggregated into logistic units represented on the diagram by the class Logistic_Unit]. List of business objects (A.5) Techniques: the UML class diagram where business object are represented by classes is illustrated in Figure 4. This diagram describes the ABC-Fashion business in terms of required data. UML modeling allowed us to identify relations between business objects; also 6 new objects have been identified. Result: 21 new business objects are consolidated. IT Artifacts Consolidation List of system goals (B.2) Techniques: IS map Result: 2 system goals at 2 hierarchical levels: Reference products, and Create products (Fig.6) Figure 6 [(a) The IS map for Product Referencing application; This map contains one goal – Reference products. This goal can be achieved by entering data from a product sheet (where the product sheet is a file with the product information) (b) The IS map for Product Creation. This map presents the refinement of the section C1 of the Product referencing map. Create products is a sub goal of the Reference products goal. Two strategies for the product creation are identified. The arc labelled XOR specifies that these strategies are exclusive, i.e. either one or another (but not both) can be used]. We can read the map in Fig.6(b) as follows: In order to Reference products, the products must be created [in the IS] either by attaching product to commercial operation management or by attaching product to permanent product management. List of system objects (classes in UML diagram) (B.4,5 in Table 2) Techniques: To identify the system objects for the Product Referencing application we develop a UML class diagram based on the available application documentation and data dictionaries. This class diagram specifies the product data as defined by ABC-Supermarket. A fragment of this diagram is shown in Figure 7 - left side. A Product file (Figure 7 - middle) – another system object -is generated from the product database and used by the upstream information system (UIS). It contains the administrative, logistic and pricing information about the product in a package. We have also identified two system add-ons developed specifically to support referencing of the textile products in ABC-Fashion: A Package Content File is associated to the Product File. It describes the content of each • package (quantity of product items per size and color). Short Cycle Envelope File defines the list of products that will be referenced as “short cycle” • products. It is used by replenishment agents for preparing outlets’ short cycle orders. Result: 24 system objects are identified in total. Figure 7. [An overview of the Product Referencing application data model (a fragment)]. Challenges In order to familiarize the domain specialists with the produced models we set up several rounds of meetings and workshops. This required a lot of time. To validate the built up parameters, we conducted several rounds of interviews with different domain specialists in the organization. There are some examples: Business and IT goal models (maps) are validated by the project managers and process • administrator; The UML class diagram presenting the business data model is validated by the ABC- • Fashion manager; The UML class diagram presenting the system data model is validated by the head of the • product referencing department; All correspondences between business and system activities are validated by the Process • administrator; Validation revealed that the built up data is mutually consistent. Summary In our case study we implemented three phases of the build-up process and built up 11 input parameters for the Fitness measurement approach. Table 3 summarizes the results of the build-up process. Table 3. The Build-up process summary Parameter Result after Consolidation technique Result after collection consolidation A.1 List of business 32 - - activities A.2 List of business goals 9 Business map 11 A.3 List of business actors 3 UML sequence diagram 5 A.4 List of business 4 - - resources A.5 List of business objects 15 UML class diagram (for 21 Business objects) A.6 List of business states 46 UML business object state 55 diagram A.7 List of business state 30 UML business object state 31 transitions diagram A.8 ignored - - B.1 List of system events 9 - - B.2 List of system goals 1 IS map 2 B.3 List of system user 1 UML sequence diagram (for 3 interfaces or system classes referencing application) representing actors B.4-5 List of system objects 21 UML class diagram (for 24 (classes) referencing application) B.6 List of system states 54 UML system object state 40 diagram B.7 List of transitions 33 UML system object state 28 between system states diagram B.8 ignored - - PROJECT SUMMARY AND RESULTS Project summary The project was conducted by 1 researcher working 9 months part-time in the ABC-Supermarket. Approximate time distribution: 66% - build-up phase; 33% - metrics application and analysis of results. Interviews have been conducted with: the head of marketing department of the ABC-Fashion; the head of the product referencing department; UIS/DIS users; the Product Referencing application administrator, developers, IT actors (the application administrator, application designers, developers). The following documents have been at the disposal of the researcher: Business process models; Application documentation (short description of application functionality) available in the Intranet; Application data dictionaries; screen-shots; user manuals for Product Referencing application. The following documents have been produced as a result of the build-up process: Business maps describing the product referencing process as it is seen by ABC-Fashion ‐ management; System maps describing the product referencing process implemented by the existing Product ‐ Referencing application; A UML business class diagram that specifies the data model for the product referencing as it ‐ is seen by ABC-Fashion; A UML class diagram that specifies the data model for the Product Referencing application; ‐ A UML entity-relationship diagram that specifies the actors and their involvement into ‐ product referencing process; Multiple object-state diagrams have been created to identify a behavior and lifecycles of ‐ business and system objects. Project Results Application of the fitness metrics from Table 1 reveals a significant “misfit” between the Product Referencing Process defined by the ABC-Fashion and the information system of the ABC- Supermarket that supports this process. The results are summarized in Table 4. Table 4: Fitness measurement results Criterion: Measurement result: 1. Support ratio 0.21 2. Goal satisfaction 0.09 3. Actor presence 0.4 4. Resource presence 0 5. Information completeness 0.33 6. Information accuracy 0.5 7. Activity completeness Various values for different activities in range (calculated for “Create the [0..1] reference in the system”). 8. Activity accuracy Various values for different activities in range [0.4 .. 0.72] 9. System reliability 0.35 10. Dynamic realism - The build-up process implemented for the Product Reference application in ABC-Supermarket permitted us to apply 9 fitness metrics out of 10 and to assess fitness relationship. Measurement results presented in Table 4 pinpoint the sources of the misfit in two problematic areas: Referencing of three textile product categories: permanent, collection, and short cycle. ‐ Referencing textile products taking into account the information about product size and color. ‐ Based on these results, the detailed guidelines on alignment improvement have been formulated. As this work is focused on the inputs of the fitness measurement approach and the way to deliver these inputs for further measurement, we will leave the detailed discussion about the results from Table 4 (that present the outputs of the fitness measurement approach) to future work. The domain specialists in the ABC-Supermarket have verified the fitness measurement results: another study, conducted by the internal analysts in ABC-Supermarket completely confirmed the results obtained using fitness measurement approach. This demonstrates that the build-up process resulted in the meaningful and correct input data. CONCLUSION AND LESSONS LEARNED As observed, many research approaches in EA have difficulties in being adopted by industrial organizations. One of the reasons for that are their high expectations on the input data availability and completeness that organizations cannot meet. In this article we discussed how researchers themselves can act in the real world environments in order to increase the applicability of their approaches: we make a hypothesis that using the build-up process, it is possible to increase the applicability of the research approach and, despite the initial absence of the input measurable parameters, to deliver meaningful results to practitioners. The build-up process introduced in this article can be considered as an instrument for domain- specific knowledge collection: it guides researchers in creating the input parameters necessary for a target (research) approach implementation. The build-up process takes its roots in the design science (Hevner et al., 2004). It is specified based on the agile methodology principles (http://agilemanifesto.org/) and represents a set of iterations where each activity performed by a researcher followed by a validation performed by the domain specialists in the organization. We presented a case study where the fitness measurement approach defined in (Etien & Rolland, 2005) has been implemented as a part of the industrial project launched by ABC- Supermarket. The applicability of the fitness measurements solely based on the data available in the ABC-Supermarket was 10% (one metric out of ten could be applied). Implementing the build-up process, we were able to construct 11 input parameters and to apply nine fitness metrics in total. This demonstrates a significant improvement in the applicability of the fitness measurement approach (from 10% to 90%) and confirms our hypothesis. The correctness of the built up data has been also verified: the measurement results based on this data has been confirmed by another study conducted in the ABC-Supermarket independently from our study. Lessons Learned Whereas fitness measurement results have received a positive feedback from the organization (this confirmed the build-up process effectiveness), the project performance was evaluated by the ABC-Supermarket as “unsatisfactory”. The reason: the build-up process has taken too much time (approximately six months). From the practical standpoint, it is a serious drawback: all the industrial projects always run in conditions where time and resources are limited. Therefore, even if finally we were able to assess the alignment in the organization, it was done too late and could not affect a strategic decision considering the existing IS. We identify the following problems that made our build-up process so time consuming: (i) Definition of the problem scope and identification of the relevant information sources. In the big companies like ABC-Supermarket collecting of the problem-specific data is not a trivial task. The researcher has to reduce the problem scope and to capture the relevant data without drowning in many irrelevant details. Thus, to make the build-up more efficient, the researcher herself has to be a knowledgeable specialist in the domain or has to team up and to closely collaborate with such specialists in the organization. (ii) Data validity. In the big companies, problem-related knowledge is often spread among individuals; understanding of the same part of business by different individuals may vary and may be even contradictory. To verify that the collected/consolidated data is mutually consistent, the researcher has to do an extensive cross-checking: this involves setting up the environment (scheduling interviews, organizing workshops) and discussing the results with many different specialists in the organization. Thus, to increase the build-up process efficiency, the project has to gain a visibility in the organization and to receive a support from the different team leaders and senior managers. We also identify the following questions that should be answered by EA researchers developing industry-oriented approaches: (i) How much time do we need to deliver the results? How can we estimate this time? Estimated time to market is a very important characteristic of an EA project. Business-IT alignment assessment results, for example, can have a great impact on the business and/or IT strategy of the entire organization. Thus, it is important to give a realistic estimation of the project time based on the available resources. Taking into account that the build-up process strongly affects the overall project time, we consider the time to market estimation for the build- up process as our main priority in the future research. (ii) What kind of intermediate results can be delivered? When? Organizations try to protect themselves from any possible risk related to project delays, insufficient results, or changed requirements: intermediate reporting (when the decision to launch the next project stage is based on the results of the previous stage) improves the approach agility and also has a high value for a manager. For the research approaches it means specification of the independent phases and identification of the intermediate results, which could be communicated to the organization during the project. For the build-up process, it means construction of the required input parameters one by one instead of collecting and consolidating all the data in one batch. Unfortunately, this implementation of the build-up process is not always possible: as fitness metrics approach shows, required input parameters are often based on the same models or specifications and highly related. Therefore, a lot of efforts are usually required in the beginning of the build-up process, when the researcher is obliged to collect and consolidate a huge amount of data in order to build up the first parameter. As the same models are reused, the build up time for each subsequent parameter drops. For the moment of this work, we found no evidence that these and similar problems affecting the applicability and efficiency of the innovative EA research are addressed in the literature. We find this subject very important and plan to address problems mentioned above in our future work.",
  "2011-RDN": "Abstract— Business process modeling is a valuable technique\nhelping organizations to specify their processes, to analyze their\nstructure and to improve their performance. Conventional\nprocess modeling techniques are proven to be inefficient while\ndealing with non-repetitive, knowledge-intensive processes such\nas Case Management processes. In this work we use the MAP\nnotation to model a Mortgage Approval Process as defined in\nBanking. To increase the navigability and practical value of map\nmodels, we extend the MAP notation with the concepts of Roles,\nRelations between roles, and Role Configuration Rules.\nKeywords: Role ; Process model MAP\nI. INTRODUCTION\nIn the modern economy, companies are more and more\ninterested in expanding their businesses and exploring new\nmarkets. Proliferation of Information and Communication\nTechnologies (ICT) fosters collaboration and makes people\nacross the globe work together as they are sitting in one office.\nWorking cultures, local policies and regulations, however,\nremain specific to a country or a region forcing the companies\nto customize their processes for each particular case and to take\ninto account where and under which conditions the process is\nexecuted) [1].\nBusiness process modeling is a valuable technique helping\norganizations to specify their processes, to analyze their\nstructure and to improve their performance. Conventional\ntechniques for business process modeling are imperative - they\nencourage the early specification of the control flow [2][3][4].\nUsing these techniques, specification of numerous options and\nvariations in a process model related to its context of execution\nbecomes tedious if at all possible [5][6][7][8].\nSimilar problem statements have been identified and\nstudied in the area of process engineering.\nFor more than ten years process engineering remains a key\nissue for the information system (IS) communities [9]. For\ninstance, [10] [11] [12] [13] show that an improved\ndevelopment process leads to improved productivity of the\nsoftware systems industry and improves systems quality.\nSimilarly to a business process, IS development process\nwas traditionally organized into sequences of activities (e.g.,\n“analysis,” “design,” “coding,” “testing”) [14] [15], with a\nrigid control flow. Later on, these activity sequences were\nreplaced by values and practices [16] [17]. However, [18]\nhighlights that, in practice, IS engineers are faced with\ncountless methodical choices: they can choose to execute one\nactivity completely or partially, to combine aspects of two or\nmore different activities, or to ignore an activity. Thus,\ndecisions made through the IS development process should\nreflect the situation at hand and help an IS engineer to specify\nand enact a unique process adapted to this situation.\nIn response to this problem, several methodologies\nsupporting intentions have been developed [19] [20] [21].\nIntention-oriented process modeling focuses on what the\nprocess is intended to achieve (i.e. what is the reason behind\nthe process execution), instead of how it is going to achieve.\n[22]. Nevertheless, many of these methodologies do not\nemploy the concept of intention/goal as an integral part of their\nprocess model. These process models focus on how the process\nis performed and externalize what the process is intended to\naccomplish – the intention. In MAP process models, [21], in\ncontrast, the intentional level is used to guide engineers\nthrough IS processes by dynamic choices of the executable\nservice sequences. Each time when an intention is realized, the\nmodel suggests an executable service (or several alternative\nservices) that can be invoked on the next step. The resulting IS\ndevelopment process is adaptive and flexible as it is\nconstructed dynamically following the situation.\nIn this paper we use the MAP methodology to model a\nMortgage Approval Process as defined in Banking. Providing a\nflexible mechanism for flexible and customizable process\nspecification, original MAP notation, nevertheless, remains\nrestrictive: it does not allow us to specify roles for this process.\nThe concept of Role is a key concept in process modeling.\nRoles are present in nearly all process models - they help the\nmodeler to structure the process model, by grouping the\nresponsibilities and answering the question “who does what?”.\nIS designer, developer, stakeholder, decision-maker, client,\nclerk – are all examples of roles. According to [27] the role is\nthe definition of an organizational intention shared by a\ncollection of users, all of whom having the same privileges and\nobligations to a set of work processes in an organization. The\nrole is also the main concept for the representation of\ncooperative processes. In other words, the role is the basic\nblock of the organization perspective [47], which reflects\ninformation about the organizational structure and actors to\nwhich the business process is intended. Roles can be\nrepresented and used differently in process models depending\non the notation.\nThe original MAP meta-model presented in [21] does not\ninclude the concept of Role. However, roles implicitly\nparticipate in decision-making in all the levels of an\norganizational process expressed by a map. For example, it can\nbe a specific role responsible for making a choice between\nseveral alternatives in the map. The notion of role will then be\nuseful to explicitly define who can take the navigation\ndecisions. Roles can also be present in the description of an\nexecutable service (e.g. some activities may be performed by\nspecific roles and only them). Therefore, in order to improve\nguidance on maps, it is important to include the concept of role\ninto the MAP metamodel.\nIn this work we extend the MAP notation with the concepts\nof Roles, Relations between roles, and Role Configuration\nRules. These modeling concepts were introduced as a part of\nDeCo (Declarative Configurable Process Specification)\napproach in [1]. This mechanism allows us to anticipate\nabundant variations of context as, even within the same bank,\ndifferent agencies may have different stuffing and thus,\ndifferent distribution of responsibilities between employees. As\na result, different actors may be required/available to execute\nthe Mortgage Approval Process. We will demonstrate how\nthese variations can be incorporated within the same map.\nThe reminder of this paper has the following structure. In\nSection 2 we discuss the related works and identify the\nadvantages of MAP applied to modeling case management\nprocesses. We also explain the limitations of the existing MAP\nnotation and provide the rationale for its extension. In Section 3\nwe present an example of the Mortgage Approval Process and\nexplain the MAP meta-model on this example. In Section 4 we\nextend the MAP notation with the mechanism to explicit Role\nvariability modeling. In Section 5 we present our conclusions\nand discuss our future work.\nII. RELATED WORK\nA. Process flexibility\nBusiness process modeling is a valuable technique helping\norganizations to specify their business processes, analyze their\nstructure and improve their performance. Conventional\ntechniques for business process modeling are imperative - they\nencourage the early specification of the control flow. However,\nfor certain forms of business processes the control flow cannot\nbe identified at design time.\nOne of the examples is a Case Management Process [25].\nThe case management process can be characterized as follows:\nit (i) has a descriptive nature, (ii) varies largely depending on\nits execution context, (iii) involves a lot of, often implicit,\ndecision making resulted from communication/ negotiation\namong human actors, (iv) is based on the available/emerging\ncase-related knowledge [26]. For such processes, early\nspecification of the control flow is not only challenging but\nalso unnecessary and even harmful.\nAs advocated in [27] an unstructured (i.e. without\npredefined control flow) process cannot be represented in\nterms of flow of tasks or activities or operations... The model\nproposed in [27] allows to represent it as a ‘black box’\nassociated to a set of resources that it uses and produces and a\nset of participating roles. The key concept of such kind of\nunstructured processes is the information and knowledge\nsharing in the work group (i.e. among actors playing roles).\nAnother paradigm to deal with non-prescriptive or non\nimperative processes is provided by conversation models. The\nlatter are based on the speech act theory and on the principle\nthat each sentence expressed by someone represents an\nintention, a commitment [28]. For instance, the Action model\n[29] defines a structure to represent the conversation\nrelationship between two participants, customer and performer.\nThe type of process models addressed in [30], [31], [32] is\nbased on the decision-oriented paradigm according to which\nthe successive transformations of the product are considered as\nconsequences of decisions. The underlying paradigm is that a\nprocess model does not have only to specify the linking of\nactivities or product states but also the intention behind the\nexecution of activities and their ordering at the run time.\nDecision-oriented models guide the decision making process\nthat shapes the business and thus help reasoning about the\nrationale of decisions [33]. This paradigm seems to be the\nparticularly appropriate for representing processes requiring\nflexibility.\nThis paradigm does not require modeling the process\ncontrol flow in advance and allows alternative strategies to be\nconsidered.\nB. Variability\nIn [34][35] the concept of configurable process has been\npresented and the modeling formalism to deal with process\nconfigurability at multiple perspectives is defined. Namely the\nauthors present the Configurable Integrated EPC (C-iEPC)\nmodeling notation that extends the well-known Event Process\nChain (EPC) notation and addresses the process configurability\nalong the control-flow, data, and resource perspectives.\nAccording to this approach, “Given a configurable process\nmodel, analysts are able to define a configuration of this model\nby assigning values to its variation points based on a set of\nrequirements. Once a configuration is defined, the model can\nbe individualized automatically.”\nIn [26] the method for Declarative Configurable process\nspecifications (DeCo) is presented. This method defines an\nextension to BPMN [36] graphical notation and identifies\nthree types of relations between roles: synonym, alternative\nand specialization.\nC. Roles and Actors\nThe concept of Role is closely related to the concept of\nActor in modeling methodologies. Various definitions of\nActors and Roles can be found in literature. The most common\ndefinition states that an Actor specifies a Role played by a user\nor any other system that interacts with the subject [37]. In I*\n[19], actors are considered as “active entities that carry out\nactions to achieve goals by exercising their know-how and\ncould be composed of Agents, Roles and Positions, each of\nwhich is an actor in a more specialized sense”. OPF [38] gives\nanother definition of an actor (a producer) which is “a core\nabstract method component that models someone or something\nthat performs work units” (produces work products or provides\nservices).\nSeveral attempts have been made to introduce the concept\nof Role in the MAP notation [39][23]. In [39], the MAP metamodel has been extended with the concept of Indicator.\nIndicators are used to help decision-making in the navigation\nthrough a process map. A typology of indicators was defined\nbased on well-known project characteristics (for instance the\ncost, the duration, the formality degree, and so on). This\nindicator typology includes a particular indicator, which\nrepresents the Expert role. This indicator is used to define\nwhich roles are linked to each executable services of a map.\nHowever, this information, even if it helps the guidance\nthrough a map, doesn’t give the user enough information about\nthe variability inherent to the role concept. For instance, it\ncannot be used to know if a specific expert role is related to\nother expert roles in the project.\nIn [23] the concept of Role has been added to the MAP\nmeta-model in order to support the “operationalization” of\nMAP process models. Namely, this concept provides an\nexplicit relation between an intentional process perspective\ncaptured by MAP process model and its organizational\nperspectives described by a business process. Enhancing the\nMAP notation using the role concept also allowed us to\nexplicitly link navigation guidelines associated to a map with\nthe roles executing them. Namely, for the intention selection\nand the strategy selection guidelines, the role taking the\nnavigation decisions can be specified. For the intention\nachievement guidelines, the role executing the corresponding\nsection in the map can be specified. The third capability added\nto the MAP notation thanks to the role was the possibility to\ndeal with non-structured processes as suggested previously in\n[27]. While improving the traceability between different\nprocess perspectives, the extended MAP process model\npresented in [23] provides little guidance for process users and\ndoes not enable process customizations.\nThe purpose of this work is to enhance the concept of Role\nin MAP and to provide the mechanisms for (i) expressing role\nvariability and (ii) role configuration. These mechanisms will\nimprove the guidance on maps taking into account all the role\nrelationships present in the project.\nIII. APPLYING MAP TO CASE MANAGEMENT\nPROCESS MODELING\nA. Example: The Mortgage Approval Process\nIn this paper we consider an example of processing a\nmortgage claim as defined by different financial institutions in\nthe USA. The information provided below results from our\nstudy of multiple information sources (e.g. http://www.\nhomebuyinginstitute.com/, http://www.mortgage-resourcecenter.com/, http://homebuyereducation.bankofamerica.com/,\nhttp://www.homeloancenter.com/ etc.). It represents a\ncompilation of guidelines, recommendations, and descriptions\nof the Mortgage Approval Process, provided by different loan\nconsulting firms, financial advisors, and banks, available on the\nweb.\nA mortgage is a loan for buying a house. On the USA\nmarket, there exist various mortgages that will be appropriate\nfor special situations such as bad credit loans, first time\nhomebuyer, etc. The terms and length of the mortgages are\nnegotiable and can be adapted for the applicant’s situation.\nThe mortgage approval process can be divided into the\nfollowing steps:\n1. Pre-qualification\n2. Formal application\n3. Document review\n4. Pre-approval\n5. Property appraisal\n6. Final Approval\n7. Closing\nThe goal of the pre-qualification step is to determine the\npotential mortgage amount that the applicant is eligible for.\nThe purpose of the formal application is to provide the lender\nwith a package of documents characterizing in details the\ncurrent financial situation (assets) of the applicant as well as\nhis/her employment and credit history. Document review\nfollows the formal application and may include the preapproval step. The pre-approval letter issued as a result of a\npre-approval indicates that a thorough analysis of the\napplicant’s credit, income, and assets has been completed and\nthat the applicant is pre-approved by a lender for a specific\nloan amount. Being optional, this letter, however, may\nrepresent a valuable negotiation instrument for an applicant\nwhile searching for a property. When the applicant selects the\nproperty, the potential mortgage lender initiates the Property\nappraisal. The result of the appraisal defines the amount of the\nmortgage and a corresponding down payment that has to be\npaid by the applicant. The lender makes “approve”, “not\napprove”, or “approve with conditions” final decisions based\non the document review and the appraisal results. If the loan is\napproved, a commitment letter is issued for the applicant, and\na closing date is set up. Closing (also called settlement) is a\nfinal step. During the mortgage closing, the mortgage lenders\nwill purchase the house and hold the title, as the applicant\nmakes payments to them.\nPre-qualification and Pre-approval are compulsory steps;\nPre-approval and Property appraisal can be considered as parts\nof a Document review and serve as a baasis for the Final\napproval.\nBelow, we illustrate how this processs can be modeled\nusing the MAP meta-model.\nB. The MAP meta-model\nGoal modeling is an effective wayy for identifying\nrequirements of software systems by focusing on\nunderstanding the intentions of the involvedd stakeholders [40]\n[41] and the process model MAP is an exammple of goal model\nthat has been conceived to meet this challenge. A map\nexpression provides a synthetic view of thhe variability of a\nprocess in a relatively easy to understand wway. Variations are\nrevealed in two ways, by the gradual movement down the\ndifferent levels of a top map, and byy the alternative\nstrategies/paths available at a given mapp level [42]. The\nintentional MAP meta-model has been intrroduced in the IS\nengineering domain [21] and validated iin several fields:\nrequirement engineering [43], method eengineering [44],\nenterprise knowledge development [45] andd process modeling\n[21].\nThe map corresponding to the mortgage aappraisal process is\npresented in Fig. 1.\nFigure 1. High level map of the Mortgage Approval Prrocess and Refinement\nmap of the section A of this map\nA map is a diagram composed of nodes and edges. Nodes\nare intentions to realize and edges are strateggies to reach these\nintentions. An edge enters a noode if its associated strategy can\nbe used to achieve the target inntention (the given node). Since\nthere can be multiple edges enntering a node, a map is able to\nrepresent multiple ways to achieve an intention.\nFig.1 (a) represents a Higgh-level map of the mortgage\napproval process defined in thee previous section. There are two\nintensions specified on this leveel: Define Mortgage Application\nand Approve Mortgage Appliccation. The former intention is\nrealized when the Formal application is completed and\nvalidated (steps 1 and/or 2 of thhe example); the latter is realized\nwhen Final Approval decision is made (step 6 of the example).\nThe final intention (Stop) corresponds to the process\ntermination and can be realizedd either by Closing the mortgage\n(in case of a positive approval ddecision) (step 7) or by rejection\nof the mortgage claim otherwise.\nFig.2 shows the MAP model using the UML formalism.\nThe key element of a map is aa section. A section represents a\ntriplet: a source intention, a ttarget intention, and a strategy\nlinking these source and target intentions. The section\ncorresponds to a process of achhieving the target intention from\na specific situation following aa particular strategy. In Fig.1 (a)\nthe triplet <Define Mortgage appplication, By complete revision\nwith pre-approval, Approve moortgage application> represents\na section that can be interrpreted as follows: when the\napplication has been completedd and validated, the process can\ncontinue to its final approval ffollowing the complete revision\nwith a pre-approval intermediatte step.\nlinked to\ntarget source\npath thread\n1..* 1..* 1..*\nbundle\nrefers follows\nFigure 2. MAAP meta-model\nThree kinds of relationships are defined for sections:\nthread, path and bundle.\nA thread relationship represents the possibility to achieve a\ntarget intention from the source intention following several\ndifferent paths (defined by a different strategy). Each of these\npaths can be seen as a section in the map. Multiple strategies\nleading to an intension describe multiple ways to realize this\nintension. On the High-level map shown in Fig.1 (a), the\nApprove Mortgage Application intention can be realized either\nby complete document revision, which include Document\nreview (step 3) and Property Appraisal (step 5) or by complete\nrevision with Pre-approval (steps 3-4-5).\nA path relationship represents the precedence (or the order)\nbetween sections. We say that one section succeeds (precedes)\nanother if and only if the source (target) intention of the former\nis the target (source) intention of the latter. There is absolutely\nno path predefined as the engineer constructs his own path\nfollowing the situation at hand. Given the thread and the path\nrelationships, an intention can be achieved by several\ncombinations of sections. In our example, the mortgage lender\ncan navigate through the map presented in Fig.1 (a) in order to\napprove a mortgage. Several paths are possible, e.g. the\napplication can be submitted directly or following the prequalification procedure first; approval can be granted without\nthe pre-approval step or with the latter, etc.\nA bundle relationship shows the possibility for several\nsections having the same source and target intentions to be\nmutually exclusive. (Mutual exclusion is depicted by dashed\nlines in a map). In Fig. 1 (a) the strategy “By closing” linking\nthe Approve Mortgage Application intention to the terminal\nintention Stop represents a bundle relationship. It combines two\nsections that are mutually exclusive: either the closing is done\nin escrow or face-to-face.\nA section in a map may be refined with another map using\nthe refinement relationship. Refinement is an abstraction\nmechanism by which a complex assembly of sections at level\ni+1 is viewed as a unique section at level i. This relationship\nintroduces levels in the process representation as each map\nmay be represented as a hierarchy of maps. Fig.1 (b) represents\nthe refinement of a section of the high level map illustrated in\nFig.1 (a) and provides more details on Final Approval process\nof a mortgage application.\nWhen a section cannot be refined any further, an\nExecutable Service that fulfills the intention may operationalize\nit. It implies the transformation of the product under\ndevelopment with the execution of a service (which can be a\nguideline, a workflow, an algorithm, a business process model,\netc.).\nA map specifies one ‘Intention Selection Guideline’ (ISG)\nper each intention I i , except for ‘Stop’ intention. Given an\nintention I i , an ISG identifies the set of intentions {I j } that can\nbe achieved in the next step and proposes arguments to select\none of them. There is also one ‘Strategy Selection Guideline’\n(SSG) per intentions pair <I i ,I j >. Given two Intentions I i , I j and\na set of possible strategies S ij1 , S ij2 , ..S ijn applicable to I j , the\nassociated SSG guides the selection of a S ijk . We can say that\nISGs and SSGs describe the know-how of a domain in socalled decision process chunks [23].\nIV. EXTENDING MAP FOR ROLE AND ROLE\nVARIABILITY MODELING\nEnabling multiple strategic choices, the MAP specification\ndoes not provide enough information for realization of these\nchoices. One of the recurrent problems is the role assignment.\nAs specified in DeCo [1], synonyms, alternatives,\noptional/obligatory process elements and hierarchies of\nelements defined by generalization/specialization relations\nprovide a mechanism for modeling process variability. The\nconfiguration rules, on the other hand, support the\nconfiguration of variants based on the situation.\nIn this work we apply these mechanisms to model\nvariability of roles in maps. In our example, the roles\nassociated with the Mortgage Approval Process can vary not\nonly from country to country and from one bank to another\nfollowing theirs internal policies, but also from one agency to\nanother within one bank (e.g. based on the size of this agency).\nWhereas some sections of an abstract mortgage process\nillustrated in Fig 1(a) can be assigned to abstract actors (e.g.\nMortgage lender), concrete role assignments for the refined\nMortgage Approval Process (Fig. 1(b)) may depend on the\nfinancial institution: small banks have only several (2-4) roles\nassociated with the process, whereas in big agencies the\napplication processing and decision making is more likely to be\nassigned to a number of different roles with different\nresponsibilities. As our study shows, the same responsibility\ncan also be performed by several roles or delegated from one\nrole to another according to some internal rules.\nAs a result, different actors may be required/available to\nexecute the Mortgage Approval Process. Thus, to provide the\nefficient guidance for both administration and personnel of a\nbank, an explicit specification of roles, relations between them\n(e.g. hierarchy, alternatives or possible replacements, and\nsynonyms) is required.\nA. Enhanced MAP concepts\nRole variability is important in the real life process\ndescriptions: in the organizations the same responsibilities are\noften associated to multiple roles, at the same time, some role\nnames can have double meaning depending on the project or\nprocess. Thus, the process gets overloaded with exceptions and\nbecomes extremely complex providing little support and\nguidance. To get more practical value, the explicit specification\nof roles, their relations and rules of role configuration/\nassignment is required.\nFigure 3 represents an extended meta-model of MAP and\nincludes the following concepts: Role, Relations between roles,\nand Role Configuration Rule. These concepts enable a\nmechanism for modeling role variability in MAP process\nmodels. Variability of roles in business process models is a\nmechanism to address the organizational complexity. It was\ndiscussed in [35].\nStart\nuses\nlinked to uses\ntarget source\n*\n1..* Map\n* * refine 0..1\nbundle\nrefers follows\nExecutable\nService made by\n*\nis executed by\nrelation\napply\n*\nFigure 3. Enhanced MAP meta-model\nA role is the definition of an organizational intention shared\nby a number of actors [46]. Role can be identified with a set of\nresponsibilities and can be fulfilled by one or several actors in\nthe organization. Loan consultant, Loan officer, and Appraisal\nagent – are examples of roles.\nIn the enhanced MAP meta-model, the concept of role is\nspecialized into individual role and group role. For example,\nLoan Consultant is an individual role whereas a Loan\nProcessor is a group role. A group role contains several\nindividual roles.\nAs in [26] [1], we define three types of relations between\nroles: Synonym, Alternative and Configuration.\nRole configuration expresses possible instantiation of a\ngeneric role into configured roles. For instance, the Mortgage\nLender may be configured into several other roles, as the Loan\nConsultant, the Loan processor, the Closing Agent and so on.\nWithin a configuration, different roles can be related via\nsynonym or alternative relations.\nSynonym relation expresses the fact that a specific role can\nbe found in different organizations/processes under different\nnames while still encapsulating the same set of responsibilities\n(i.e. this role has “synonyms”). For example, in the Mortgage\nApproval Process, the Loan Consultant role is a synonym to\nthe Loan Broker role. Thus, during the process instantiation,\nboth roles can be assigned to the same actor.\nAlternative relation expresses the fact that, under specific\nconditions, the responsibilities associated with one role can be\nfulfilled by another role (or delegated to it). For example, if the\nlocal agency does not have its own Appraisal agent, an\nExternal Appraisal agent can be recruited to perform the\nproperty appraisal task. Along those lines, the role of the Loan\nProcessor, that performs an application revision and decision\nmaking, can be fulfilled by a Loan Broker (for the smaller\nagencies).\nWe associate the executable service supporting a map\nsection to one or several roles. This captures the knowledge\nabout which tasks can be taken by which role and offers a\nbetter guidance of the map execution. The executable service\n(i.e. a business process chunk [23], a method chunk [42]...)\nencapsulates a process knowledge specified in the\norganizational layer (by answering what, who, when, where\nand how questions).\nB. Modeling the Role hierarchy for Mortgage\nApproval Process\nFigure 4. Role hierarchy and relations between roles in Mortgage Approval\nProcess\nFigure 4 illustrates the hierarchy of Roles defined in\nMortgage Approval Process. The roles are modeled as\nrectangles (following the UML notation) with the name in the\nupper compartment.\nA rectangle with thick outline and the \"C\" in the right\ncorner is a “compact” notation that refers to a role that can be\nfurther configured in the process model based on the situation\nor context. We envisage integrating the Role element into the\nMAP notation using this compact representation. For\nconvenience, the configuration details can be shown on a\nseparate diagram as represented in Fig.4.\nIn our example, the abstract role Mortgage lender combines\na set of complementary and possibly overlapping\nresponsibilitie. These responsibilities can be fulfilled by a\nsingle actor or shared by a group of actors, giving a rise to\nmore specific roles: Loan Consultant, Loan Processor,\nUnderwriter, etc.\nIn the diagram representing the details of role\nconfiguration, we define another two types of relations\nbetween roles: Synonym relation, depicted by a dashed line\nwith a tag “syn” and Alternative relation, ddepicted by a solid\nline with a tag “alt”. These relations refleect the policies of\norganization and are used to explicitly deescribe “who does\nwhat” based on the concrete context.\nNamely, the synonym relation refers to tthe fact that in the\ncontext of a given process, two roles have abbsolutely the same\nresponsibilities. Thus, they can be defined innterchangeably (for\ndifferent organizations for example). In ouur example, Loan\nBroker and Loan Officer roles are synonnyms of the Loan\nConsultant role as within the mortgage apprroval process they\nfulfill the same responsibilities.\nThe alternative relation refers to the sittuations when the\nresponsibilities of one role can be delegateed to another role,\notherwise not having those responsibilities. In our example in\nFig.4, Loan Processor role has an alternative - the Loan officer\nrole. This alternative is bounded by a condiition R1 (depicted\nwith a shaded circle) meaning that whenevver the number of\nagents in the local agency is less thenn 6 persons, the\nresponsibilities in the Mortgage approval process normally\nfulfilled by a Loan Processor can (will) be ffulfilled by a Loan\nOfficer.\nAlong those lines, External Appraisal Agent and State\nAttorney are alternative roles for the bank AAppraisal agent and\nAttorney respectively. These alternatives can be activated\nwhenever their corresponding roles insidde the bank are\nunavailable.\nWe emphasize here that all the considereed relations can be\ndefined for a given process only, e.g. whereas for one process\ntwo roles can be considered as synonyms, ffoor another process\nthis will not necessarily be the case as the sets of\nresponsibilities fulfilled by the same rolee within different\nprocesses might be different.\nWe specify configuration rules for the rolle diagram. These\nrules define the conditions when the synonnym or alternative\nrelation between roles can be activated; theyy can be expressed\nin a form of a predicate evaluating to Truue if condition is\nsatisfied at a given context of False otherwisse. In our example\ndepicted in Fig. 4 the Rule R1 describess a condition for\nactivating alternative relation: it states thaat the number of\nagency employees should be less than 6 in orrder to activate the\nalternative relation between the Loan Proocessor and Loan\nOfficer roles. Otherwise, in a bigger agenccy, the documents\nrevision should not be delegated to a Loan Offficer.\nFig.5 illustrates other examples of rules tthat can be used to\nconfigure the roles: the availability of a givven role in the real\nenvironment, geographical location of an aggency – these and\nmany other factors can affect the configurattion of the process\nmodel, in particular imposing the role and acttors configuration.\nR1 # agents < 66\nFigure 5. Role configuration rule exammples\nConfiguration rules can bbe defined on the level of a\ncompany, its country divisioon, and/or its local branch or\nagency.\nThe extended map with thee role specifications is illustrated\nin Fig. 6.\nFigure 6. Mortgage approoval process enhanced maps\nThe upper part of the figuree depicts the high level view on\nthe Mortgage Approval Processs. The section A represents the\nprocess of our interest – the Moortgage approval as executed by\na Bank. This section is asssigned to the abstract role of\nMortgage Lender.\nThe bottom part of Fig. 6 rrepresents the refinement of this\nsection and can be interpreted aas follows:\nThe mortgage application ccan be made either by following\na pre-qualification process or by directly making an\nappointment in a bank (we caall it “a personal contact”). The\nLoan Consultant is a role ressponsible for guiding the client\nthrough both of these processess. After the formal application is\ncompleted, the document reviiew process is initiated by the\nmortgage lender; this process cconsists of several steps and can\nbe accomplished following diffferent paths. The pre-approval of\nthe mortgage is assigned to a ssingle role (the Loan Processor).\nThe final approval should be caarried out by the Loan Processor\nand should be accompanied byy the property appraisal process\ncarried out by the Appraisall agent. There are three roles\nassigned for the closing processs. Grateful\nV. CONCLUSION AND FUTURE WORK\nThis paper proposes a mechanism to model roles and role\nvariability in the intentional meta-model MAP. Namely, we\nextended the MAP notation with the concepts of Roles,\nRelations between roles, and introduced the Role Configuration\nRules (formalization of these concepts will be addressed in our\nfuture publications).\nThe proposed mechanism enables a better guidance and\nnavigation on maps as each process step can be defined for\nspecific roles only. The number of proposed alternatives to\nnavigate through the map is then reduced to the steps\ncompatibles with the user role.\nWe illustrated our modeling process on the example of the\nMortgage Approval process as defined in the USA banks (the\nsimplified specification has been considered). The Mortgage\napproval process is an example of Case Management process.\nAs shown in [26], modeling this process using traditional\nimperative modeling style can be difficult if at all possible. In\nthis paper we demonstrated the advantages of intentional\nmodeling using MAP enriched with role modeling mechanism.\nThe first advantage is that the main user (the mortgage lender\nin our example) has a perfect view of the stakeholders involved\ninto processes. The second advantage is that the resulting\nmodel allows us to anticipate variations of context (e.g.\ndifferent bank agencies having different distribution of\nresponsibilities between employees) while managing the model\ncomplexity.\nProposed notation for modeling role variability is not\nlimited for MAP; as proposed in DeCo [1], it can be used to\nextend another process modeling notations such as BPMN\n[36].\nIn future, we plan to extend the notation proposed in this\nwork by adding cardinality constraints.",
  "2011-RN": "1. Introduction\nRecent publications illustrate the increasing interest\nof BPM practitioners in both methodology and tools\nfor unstructured, knowledge-intensive processes such\nas case management processes (CMP) [1][2]. In its\nreport from November 2009 [1], Gartner Inc. has\npublished 5 Business Process Management predictions\nfor 2010 and beyond that reflect the challenges and\nemerging BPM trends. Namely, they acknowledge the\nneed for support of unstructured, dynamic, knowledgeintensive processes. According to Gartner, “By 2013,\ndynamic BPM will be an imperative for companies\nseeking process efficiencies in increasingly chaotic\nenvironments.”\nAnother prediction highlights the importance of\ndeveloping a business process modeling culture in the\norganizations: ”By 2014, 40% of business managers\nand knowledge workers in Global 2000 enterprises\nwill use comprehensive business process models to\nsupport their daily work, up from six percent in 2009.”\nJanelle Hills, a vice president in Gartner Research,\ncomments on this prediction in [3]: “Comprehensive,\ngraphical (rather than textual) and explicit process\nmodels that capture and represent organizational\nknowledge will create a lingua franca for business and\nIT roles.”\nThe Case Management Process Modeling (CMPM)\nRequest For Proposal released by OMG on September\n2009[2] expresses the particular demand of\npractitioners in the case management solutions. OMG\ndefines case management as “A coordinative and goaloriented discipline, to handle cases from opening to\nclosure, interactively between persons involved with\nthe subject of the case and a case manager or case\nteam.” Case management processes have multiple\napplications, including “licensing and permitting in\ngovernment, insurance application and claim\nprocessing in insurance, patient care and medical\ndiagnosis in healthcare, mortgage processing in\nbanking...” [2]. The main resource of a CMP is\nknowledge obtained as a result of communication\nbetween multiple actors/users and stored in a “case\nfile”. There are two types of knowledge CMP deals\nwith: explicit knowledge about the case subject (e.g. a\npatient’s body temperature) and tacit knowledge\nrequired to manage the case (e.g. experience of a\nmedical team). Whereas explicit knowledge can be\nstored, managed, and shared easily between the actors,\ntacit knowledge is hard to represent and to transfer – it\nexists in the form of “experience” or “intuition”.\nBeing largely based on tacit knowledge, CMPs can\nbe hardly analyzed, improved, and replicated by the\norganizations. This problematic is extensively explored\nby researchers in knowledge management discipline\n[4][5][6]. Nonaka and Takeuchi, in [6], describe\nconversion of different types of knowledge within\norganizations as an iterative and spiral process that\nincludes four phases: socialization, externalization,\ncombination and internalization (SECI). This process\nexplains how individuals’ tacit knowledge is getting\ntransformed into, first, tacit, and then explicit\nknowledge of an organization. According to [6], SECI\nstimulates the new knowledge creation and application\nresulting in the improved processes. Along those lines,\nexperience is one of the main objectives of adaptive\ncase management approaches.\nThe dominant business process and workflow\nsupporting formalisms are almost systematically\nactivity oriented: the main advantage of these\nformalisms is a possibility to generate executable\nprocess specifications and also to simulate and validate\nthese specifications prior to the process deployment.\nThis technique guarantees better control over processes\nand helps to avoid costly errors. However, providing a\nhigh degree of control, aforementioned formalisms\nsuffer from the lack of adaptability: once the process is\ndesigned, it becomes difficult (if at all possible) to\nadjust it with respect to a changing execution context\nor emerging knowledge. Thus, being well suited for\nprescriptive, context-specific business processes, these\nmodeling formalisms are not appropriate for the\nknowledge-intensive processes and the case\nmanagement process modeling (CMPM) in particular.\nInspired by the OMG request for proposal, we\nformulate the following question: Can we provide a\nformalism that would support the process run-time\nadaptability while providing control and ensuring\nprocess correctness?\nDuring the last decade, process adaptability and\nevolution support remains the central area of interest\nfor many researchers: among others, numerous\ncontributions of the groups at the University of Ulm\n(Dadam, Reichert et al) and the Eindhoven University\nof Technology (van der Aalst et al) can be emphasized.\nIn this work we introduce the modeling approach for\nCMP based on Declarative Configurable (DeCo)\nspecifications. The originality of this approach is in its\nattempt to distill different ideas explored in different\nperiods of time by different research groups in BPM\nand software engineering: declarative modeling, model\nverification, variability modeling.\nCompared to traditional modeling approaches, DeCo\nexplicitly defines three modeling levels that capture\nknowledge-intensive processes (i) at design and result\nin a configurable, non-deterministic design\nspecification that focuses on the process goals; (ii) at\ndeployment and result in a customized design\nspecification that reflects the process deployment\nenvironment; (iii) at execution and result in a fully\ndeterministic process trace that reflects post-factum a\nsingle process enactment.\nTo enable the process control, we formalize the\nrelationships between the process specifications at\nthese three levels as refinement relationships [7]. Using\nformal methods of refinement validation, we can\nensure that the process customizations made at the\ndeployment level and the process execution are aligned\nwith the process goals specified at design.\nsystematic improvement of CMP based on the user\nIn this paper, we focus on the DeCo modeling levels\ndefinition. The formalization of refinement relations\nbetween specifications at these levels and refinement\nvalidation will be addressed in our future publications.\nWe illustrate DeCo specifications on the example of a\nmortgage approval process.\nThe reminder of this paper is organized as follows:\nIn Section 2 we discuss the business process taxonomy\nand position CMPs within this taxonomy. We also\ndiscuss the existing trends in process modeling,\nhighlighting challenges and perspectives in modeling\nknowledge-intensive processes and CMPs. Section 3\nintroduces a mortgage approval process – a CMP\nexample. In Section 4 we present DeCo process\nspecifications: we discuss the theoretical foundations,\ndefine the three modeling levels for our specifications,\nand introduce the DeCo process lifecycle. In Section 5\nwe illustrate the graphical notation of DeCo on the\nexample of mortgage approval process. This notation\nextends the BPMN notation providing the concepts for\ndeclarative process modeling, formal analysis, and\nstep-wise evolution. In section 6 we present our\nconclusions.\n2. Case management process modeling: a\nnew challenge for BPM\n2.1. Process taxonomy\nBusiness Processes can be roughly divided into two\ncategories [8]:\n1. Prescriptive processes: processes with predictable\nsequences of simple tasks and well defined\ncoordination rules. Such process can be fully specified\nat design-time.\n2. Descriptive processes: knowledge intensive\nprocesses, strongly based on the actor collaboration\nand information exchange. They can be also\ncharacterized by a weak predictability of task\nsequences and only partially defined coordination\nrules. Such process can be only “sketched” at designtime; at run-time this process should reflect the\nemerging knowledge and adapt accordingly.\nIn the global economy the growing interest of\norganizations to explore new markets can be observed.\nHowever, even a simple business process has to be\nadapted to a new execution context, depending on, among\nother aspects, cultural and legal considerations [9]. We\ndistinguish another two categories of business processes:\n1. Context-specific processes: highly specialized\nprocesses, defined for a given execution context (e.g. a\nresearch experiment in chemistry or physics).\n2. Configurable processes: processes requiring a\ncustomization (e.g. role/task assignment, task ordering,\nand rule selection) upon its deployment in order to reflect\nthe context-specific parameters. For such processes the\n“deployment-time” adaptability is required.\nFigure 1 illustrates our process taxonomy and\nprovides the reader with several examples.\nCase management. The glossary of RFP for CMPM\nin [2] defines case as “A situation, set of circumstances\nor initiative that requires a set of actions to achieve an\nacceptable outcome or objective.…”. Case\nmanagement processes (CMP) can be characterized by\nthe following: it is driven by emergent knowledge\nabout the case subject or the environment; largely\nbased on tacit knowledge (e.g. human expertise);\nhighly unpredictable; difficult to replicate; hard to\nanalyze and improve as no HOWTOs available.\nAccording to the definition provided in [13], CMPs can\nbe considered as knowledge-intensive business processes.\nAccording to our classification in Fig. 1, CMP\nsupposed to not only anticipate the change of a\nbusiness context at deployment, but also has to react in\nthe consistent and organized manner on all the\nemerging case-related knowledge at run-time.\nTherefore, we identify CMP as descriptive,\nconfigurable processes and position them in the topright quadrant of our diagram in Fig.1.\nFigure 1: Business process taxonomy: CMPs\ncan be considered as descriptive configurable\nprocesses.\n2.2. Modeling formalisms\nThe Business Process modeling formalisms defined by\nUnified Modeling Language, Event-Driven Process Chain\n(EPC), and Business Process Modeling Notation (BPMN)\ngain the wide recognition among practitioners today.\nAccording to the classification provided in [8], these\nformalisms are based on the activity-oriented and/or\nproduct-oriented paradigm for business process modeling.\nBPMN 2.0 beta specification [10] extends BPMN 1.0\ndistinguishing Private (internal) business processes,\nAbstract (public) processes, and Collaboration (global)\nprocesses. It enables model orchestrations and\nchoreographies as stand-alone or integrated models and,\ntherefore, provides better support for modeling\ncollaborations, communication, and human actors’\ninvolvement.\nThe process models presented in [11]-[12] is based\non the decision-oriented paradigm according to which\nthe successive transformations of the product are\nlooked upon as consequences of decisions.\nVan der Aalst in [15] presents a case handling\nparadigm to cope with business process flexibility. In\ncontrast to workflow management, case handling aims\nto describe what can be done to achieve a business\ngoal but not what should be done and how.\nIn [16], an approach for mining change logs in\nadaptive process management systems is presented.\nSince adaptive processes provide the information about\nprocess changes – these changes can be used for\norganizational learning and process improvement.\nIn [8], the taxonomy of flexible/adaptive workflow\napproaches is presented. Approaches, providing a\nflexibility by selection (a priori) and by adaptation (a\nposteriory) are distinguished. The flexibility by adaptation\nallows for evolution (adaptation) of the process\ndefinitions (design models) as well as process instances\nduring their execution. The modeling formalisms\nsupporting the flexibility by selection offer the capacity to\ndeal with the change of the process deployment\nenvironment on the instance level but do not result in an\nevolution of process definitions (design models).\nIn [14], the authors distinguish three classes of\nbusiness processes: operational, practice-oriented and\ncase-handling. They use the term organizational\npractices to address the result of externalization of tacit\nknowledge by organizations. The question of how to\ndevelop and use these practices in order to provide\nsystematic analysis and improvement of CMP is one of\nthe main objectives of adaptive case management.\nIn this work, we present the process modeling\napproach based on Declarative Configurable (DeCo)\nprocess specifications that extend the BPMN notation\nwith the constructs for declarative and variability\nmodeling support. This approach explores flexibility\nby adaptation and assumes an eventual evolution of the\ninitial (descriptive) process definition via enactment\nanalysis or process mining [17].\n3. Example: the mortgage approval process\nMortgage approval process is a typical example of a\ncase management process. In this paper, we provide a\ngeneric mortgage approval process description as\ndefined by different financial institutions in the USA.\nThe information provided below results from our study\nof multiple information sources (e.g.\nhttp://www.homebuyinginstitute.com/,\nhttp://www.mortgage-resource-center.com/,\nhttp://homebuyereducation.bankofamerica.com/,\nhttp://www.homeloancenter.com/ etc.) It represents a\ncompilation of guidelines, recommendations, and\ndescriptions of mortgage approval process, provided by\ndifferent loan consulting firms, financial advisors, and\nbanks and available on the web.\nA mortgage is a loan for buying a house. The\nmortgage approval process can be divided into the\nfollowing steps: Pre-qualification; Formal Application;\nDocument Review; Pre-approval; Property Appraisal;\nFinal Approval; Closing. Whereas the order of these\nmain steps varies rarely, the documents required, the\nsequences of tasks, the participants and the length of\neach step can be different depending on the place, the\nfinancial institution’s policies, and the applicant’s\nsituation and requirements. In this paper we will focus\non the Formal Application process step:\nMortgage Approval: Formal Application\n0 The applicant can request the application package\nby e-mail or by post. Alternatively, all the forms\ncan be accessed on the Web.\n1 Mortgage application can be submitted\nelectronically or during a personal meeting with the\nmortgage lender.\n2 The exact set of documents may vary depending of\nthe financial institution and the particular situation\nof an applicant. These documents may include: The\nSocial Security card; Record for past two years for\nresidence address; Employer name, address; W-2\ntax forms; federal income tax returns; Most recent\npay-stubs, etc..\n3-5 During the formal application, the mortgage lender\nprovides the applicant with a Good Faith Estimate\n(GFE) of costs of loan closing; the applicant can be\nasked to make a final decision on the type of\nmortgage loan; also an interest rate for the loan can\nbe locked in this phase.\n6 Some lenders will give to the applicant an access to\ntheir website where the applicant can check on the\napproval status of his/her package.\n7 Usually an application fee and the appraisal fee will\nhave to be paid by the applicant during the\nmortgage application submission.\nThe main purpose of this description is to illustrate the\nvariability of activities, actors, and information\ninvolved and to emphasize the unpredictability and\ndiversity of the process scenarios – the characteristics\nthat make it’s modeling a challenging task.\n4. Declarative Configurable process\nspecifications: DeCo\n4.1. Theoretical foundations\nWhile designing DeCo modeling approach, the\nfollowing guiding principles have been considered:\n1. Fixed constraint often means lost\nopportunities: DeCo formalism should not limit but\nguide the modeler, encouraging him/her to think in\nterms of possibilities, alternatives, and variations\ninstead of rules and constraints.\n2. Very little is certain at design-time: DeCo\nspecifications should separate the process goals (what\nshould be achieved?) from the process means (what\nshould be done to achieve it?) and from the process\nrealizations (how?). There is a proper time to address each\nof these aspects which is not necessary a design-time.\n3. Controlled anarchy: DeCo formalism should\nsupport the mechanisms for assembling meaningful\nprocess specifications from the predefined process\nparts or variants and for controlling this\nmeaningfulness.\nIn order to follow the guidelines above, DeCo\nformalism is grounded on:\n(cid:1) The declarative modeling paradigm [18] that\nsupports nondeterminism and, thus, releases a\ndesigner from the obligation to determine the exact\nprocess scenarios upfront;\n(cid:1) The variability modeling [19] that provides the\nmechanism to specify multiple variants for different\nprocess components (tasks, data objects, actors, etc),\nwhile hiding the complexity and keeping the model\nreasonably readable;\n(cid:1) The refinement theory and formal methods [18][20] the paradigms, defined in software engineering and\nproviding the methodology and tool support for\nprogram specifications analysis and validation. In our\ncontext, we use these methodology and tools to ensure\nthe correctness of process customization and process\nrealization with respect to the design specification.\nThis work is largely based on the research reported in\n[7][21][22][12] and is inspired by the results presented\nin [23][24]. DeCo approach enhance the results\npresented in [22] providing an explicit notion of\nmodeling levels and extending the graphical notation.\n4.1.1 The declarative modeling principles allow one to\npostpone the decision making about the process control\nflow until its deployment or even execution. The\ndeclarative specifications for modeling business processes\nhave been presented in [22][21]: this approach is based on\nthe systematic modeling of process-related data. This\nallows us to introduce the notion of process state (or case\nstate) as a vector X = ( p 1 , p 2 ,.., p n ) . The\ncomponents p 1 , p 2 ,.., p n are values of data objects\nrelated to this case at a given moment of time. For every\nprocess task A we define a precondition and a\npostcondition. Postcondition A post is a condition that the\ncase meets upon the task termination. It specifies all\npossible case states after the task completion, including\nsuccess and failure. Precondition A pre specifies a\ncondition that must hold upon the task invocation. It\nspecifies all possible case states at which the task can (but\nnot necessary will!) be executed. We specify the task\nusing logical implication between precondition and\npostcondition as follows:\nA( X , X ') def = A ( X ) → A ( X , X ')\npre post\nThe expression above can be interpreted as follows: If at\na given state X the precondition A pre of the task A holds,\nthen the case will be transited to a state X ' , for which\nthe postcondition of A - A post - holds.\nThe process specification in DeCo represents a set of\ntasks with no explicit execution scenario: at run-time,\neach task can be dynamically selected from the list of\ntasks enabled at a given case state.\n4.1.2 The refinement theory and formal methods.\nAs soon as process specification evolves based on the\nuser experience or emerging knowledge, the\nmechanism to validate and to control this evolution is\nrequired. In the work presented in [22] the evolution\nfrom the declarative, non-deterministic process\nspecification at design time towards precise\n(imperative) process specification at instantiation is\nrepresented as a set of refinement steps. The notion of\nrefinement for graphical specifications, adopted from\nsoftware engineering [18], is presented in [7]. In this\nwork, the formal semantics for graphical specifications\nis defined based on first order logic and set theory.\nThese semantics allows us to reduce the problem of\nrefinement verification to the validation of the firstorder logic formula and provides the means for\nautomated process analysis and control using the tools\nfor automated analysis of program specifications\ndefined in software engineering.\n4.1.3 The variability modeling. The technique\npresented in [22] allows one to demonstrate that\ndifferent control flow configurations of the process are\nvalid with respect to a high-level declarative design\nspecification. However, to deal with descriptive\nprocesses (and the case management in particular) the\nprocess configurability should not be limited by a\ncontrol flow.\nIn the literature, several major perspectives of the\nprocess models are specified [26]: the control flow\nperspective, the data perspective, the resource\nperspective, the operational perspective, the context\nperspective, and the performance perspective. In\n[19][23][24][25] the concept of configurable process\nhas been presented and the modeling formalism to deal\nwith process configurability at multiple perspectives is\ndefined. In their work, the authors address the process\nconfigurability along the control-flow, data, and\nresource perspectives. According to this approach,\n“Given a configurable process model, analysts are able\nto define a configuration of this model by assigning\nvalues to its variation points based on a set of\nrequirements. Once a configuration is defined, the\nmodel can be individualized automatically...”.\nIndividualization process can be considered as\nautomated synchronization of the process model\nperspectives in respond to each configuration decision.\nTo introduce the notion of correctness and to reason\nabout individualized and configurable models, authors\ndefine formal semantics for C-iEPC based on FOL[23].\n4.2. Multi-level process specifications with\nDeCo\nAccording to [27], a process definition obtained at\ndesign time includes a network of activities,\nrelationships between them, information about\nindividual activities including participants, data, etc.\nWhile modeling knowledge-intensive or descriptive\nprocesses, the major challenge is related to the fact that\nmultiple process aspects cannot be identified at design-\ntime and, thus, cannot be included into the process\ndefinition. In DeCo, we identify three modeling levels\nfor descriptive processes and, accordingly, define the\nfollowing process specification types:\n1. Configurable design specification;\n2. Customized design specification;\n3. Realization specification (or process trace).\nBelow we explain the reasoning behind these three\nlevels.\n4.2.1. Design vs. Deployment. In our example, the\ndeployment environment of the mortgage approval process\ncan strongly affect the way the process constructed: being\ndefined for the US financial organizations, this process still\nshould be adjusted taking into account legislative norms\nand regulation of different States. For example, in the\nwestern part of US the mortgage closing is done “in\nescrow.” In the rest of the country the closing is done\n“face-to-face”. Thus, an organization specialized in\nmortgage business in the entire US market and interested\nto consolidate its processes (instead of keeping different\nprocess definition for different States), should anticipate\nthose variations upfront.\nOne option for this organization can be to specify all\npossible execution paths as alternative branches within\na single process model. However, this often results in\ntangled processes containing many exceptions.\nMoreover, if this organization decides to enter, for\nexample, the European market, integration of the new\nprocess scenarios will require a lot of efforts. The\nsecond possibility is to identify and support several\nmodeling (or abstraction) levels for the process while\nmaintaining the relations between those levels [22].\nIn DeCo, as discussed in [24], we make a distinction\nbetween a configurable process definition (in [24] is\ncalled a configurable reference process model) resulted\nfrom the process design, and a customized process\ndesign specification (according to [24], model\nconfiguration or individualization) reflecting the\nconcrete environment where the process enactment\nsupposed to be observed.\n4.2.2. Deployment vs. Execution. For descriptive\nprocesses, multiple process details often cannot be\nspecified during the customization either. The\nemerging case-related knowledge and the customerorganization communication patterns can vary widely\nwithin the same predefined deployment environment\naffecting the concrete process scenario. On the other\nhand, they can be identified only during the execution.\nFor example, during the mortgage approval process, a\nconcrete applicant situation may affect the list of\nrequired documents and, consequently, the required\nrevision activities: the form W-2 (Wage and Tax\nStatement) is typically requested from all the\napplicants in USA, whereas the form 2555 (Foreign\nEarned Income) should be provided by taxpayers who\nhave earned income from sources outside the United\nStates. The applicant may prefer to communicate with\nhis/her loan consultant by e-mail or by setting up\npersonal meetings – this can also affect the whole\nprocess scenario. Along those lines, a current situation\non the market, a property appraisal report, or other\nemerging data may require a special treatment and\nresult in various courses of actions and decisions.\nTherefore, in our case, the customized design\nspecification should remain configurable and allow for\nthe “fine tuning” at execution. Thus, we make a\ndistinction between a customized design specification of\nthe process and its occurred realization that reflects the\ndecision making and dynamic process configuration\nduring its single invocation. To document such process\nrealizations we define the third type of DeCo\nspecifications – the process realization (or trace). The\nmain purpose of the process traces is to construct the\nknowledge base and to contribute into further process\nimprovement (e.g. using process mining [16]).\n4.3. DeCo process life cycle\nWe define 4 process lifecycle phases in DeCo:\nDesign, Customization, Execution, and Analysis. Figure\n2 illustrates the lifecycle and its phases.\n4.3.1. Design Phase. During this phase, the process goal,\na set of activities/tasks that supposed to lead to this goal,\nthe contracts for each individual activity (pre, post, inv),\nand data required to fulfill these contracts have to be\nidentified. The following questions can characterize the\ndesign phase:\n- What can characterize the process success or failure?\n- What are the tasks that must/should/could be done to\nachieve the desired result?\n- What data artifacts are required (mandatory) for each\ntask? Are there any alternatives?\n- What are the available solution patterns?\n- Who can make a decision? Are there any alternatives?\nDesign phase is terminated with a non-deterministic\ndesign specification that defines process invariants. It\nfocuses on what can be done to accomplish the process\nand what is needed for it.\n4.3.2. Deployment/Customization Phase. At\ndeployment, the process environment is selected.\nTherefore, the initial design specification can be\ncustomized reflecting the emerging information. This\ncustomization can involve: specification of the set of\navailable roles, role/tasks assignments, partial ordering\nof tasks, definition of business rules etc. In order to\nrespect the emerging constraints, new tasks can be\nadded whereas some activities/tasks defined at design\ncan be removed or some contracts can be precised. The\nfollowing questions can characterize the customization\nphase:\n- What are the constraints/norms to comply with? Their\nnature (e.g. organizational, geo-political, physical,\ntemporal, cultural, etc.)?\n- What are the tasks that must/should/could be done to\nensure the compliance?\n- What data artifacts are required?\n- What data artifacts are available to carry out the process?\n- What are the accepted/recommended solution patterns?\nAre there any alternatives?\n- Who should make a decision? Are there any alternatives?\nCustomized design specification focuses on what the\norganization has to do to accomplish the process in a given\nenvironment and what will be available for it to do so.\n4.3.3. Execution Phase. Properly defined, a\ncustomized design specification enables configuration\nof the process enactment at run-time, supporting the\ndecision making about the process handling based on\nthe emerging data. These decisions may include:\ntriggering, canceling, re-execution, delegation of tasks,\nrestricting or relaxing decision rules, initiating a\nsupplementary investigations or information requests,\nassigning actors, etc. These decisions can be driven by\nthe business rules and explicit regulations as well as by\nthe actor experience, process history, best practices, etc\n(aspects that cannot be specified at design time).\nFigure 2. DeCo process lifecycle.\nAs explained in 4.1.1, the process specification in\nDeCo represents a set of tasks with no explicit\nexecution scenario: at run-time, a task can be\ndynamically selected from the list of tasks enabled at a\ngiven case state. As soon as the task is triggered, it\nbecomes a part of the process enactment trace.\n4.3.4. Analysis Phase. Upon each process enactment\ntermination one possible process path (trace) is\nexplored and can be stored in the history record. Such\nCase history can be studied in order to generalize the\nrepetitive scenarios and/or derive business rules or\nguidelines. These guidelines and rules can be further\nincorporated into initial or customized design\nspecifications making the process more deterministic.\n5. DeCo modeling notation\nDeCo process specifications extend the BPMN\nnotation (a de-facto standard for process modeling)\nproviding a mechanism for descriptive process\nmodeling, formal analysis, and step-wise refinement.\nThis notation is in its infancy. With DeCo, we\nintroduce the following features to the BPMN\ngraphical notation:\n- Action contracts, including preconditions,\npostconditions and invariants (denoted by ‘Pre’,\n‘Post’, ‘Inv’ respectively);\n- Distinction between cross-boundary data objects\n(denoted by ‘IN’ / ‘OUT’) and local data objects;\n- Optional vs. obligatory tasks/data/conditions\n(depicted by dashed or solid lines respectively);\n- Configurable vs. fixed tasks/data/actors (depicted by\nbold or standard lines respectively);\n- Configuration rules (if explicit);\n- Task/role/data object specialization (denoted by ‘S’).\n- Task/role/data object alternatives (denoted by ‘A’).\n5.1. Configurable design specification\nFigure 3 illustrates the configurable design\nspecification of the Formal Application phase of the\nmortgage approval process.\nThe SendForms task in Fig.3 is specified with a\nprecondition “request is received” and a postcondition\n“forms are sent”. These conditions can be formalized as\npredicates in first-order logic. This formalization is\nbeyond the scope of this paper. Each condition is depicted\nas a link between the task and a data object. The link can\nbe annotated with a condition expression in text or FOL.\nPayStubs, CCBills, Confirmation are examples of\ncross boundary data objects. TaxForms and TaxReturn\nare configurable data objects (R1 and R3 are the\ncorresponding configuration rules). BalanceSheet is an\noptional configurable data object: this form is demanded\nfrom the applicant only “if the applicant is selfemployed”. This fact is documented in the rule R2.\nGetApplicationFee is an optional task that can be\ndecided upon in each particular case. SendForms is an\noptional configurable task, i.e., if selected, it can be\nspecialized: SendFormsByPost SendFormsByFax\nSendFormsByE-mail. Similarly one can configure the\nLockInAnInterestRate task. Apart from SendForms\nand RegisterApplication, other tasks are not preordered.\nThe MortgageLender role is a configurable role\nthat can be further specialized and splited into multiple\nroles (depending on the organization).\n5.2. Customized design specification\nAt deployment, the design specification is customized\nwith respect to the process deployment environment.\nFigure 4 illustrates the customized design specification\nof the formal mortgage application that has been\nobtained from the initial design specification (Fig.3)\nfor the concrete financial institution - Bank X: The\nSendForms task is removed as BankX provides all the\nmortgage application forms on-line. The\nRegisterApplication task is configured as follows: the\nTaxForms data object from Pre 1.2 is specialized: the\nW-2 form must be provided by the applicant; the\nTaxReturn data object is specialized and replaced by the\n1040 form. The ‘A’ in the top right corner of this object\ndenotes that this object has alternatives (i.e. possible\nreplacements): at run-time the form 1040 can be replaced\nby the simplified version of this form – 1040EZ. Generic\nactor MortgageLender from the design specification is\nspecialized and represented now by two actors: Processor\nand LoanCunsultant. The tasks are distributed between\nFigure 3. Configurable design specification.\nCustomized design specification.\nthese actors. LockInAnIinterestRrate task is specialized\nto “by fax”, however this task has alternatives (e.g. it is\npossible to lock in the interest rate by phone).\nRealization specification (trace).\nThis customization of the design specification reduces\nthe nondeterminizm of the latter and provides more\nconcrete details of the process execution as defined for\nthe BankX, however, it rests configurable and nondeterministic (tasks are not preordered).\n5.3 Trace\nAt the execution, the process is configured “on the\nfly”. Figure 5 illustrates a process trace that reflects a\nsingle invocation of the mortgage approval/formal\napplication process customized in the previous section.\nThis specification depicts the data objects received and\nthe selected task ordering.\nCollection of the process traces can be used for\nfurther process analysis and improvement. In order to\nprove that the trace is aligned with the corresponding\ndesign specifications, the relations between those\nspecifications should be formalized as refinement\nrelations and refinement should be validated. These\nformalization and process are beyond the scope of this\npaper.\n6. Conclusion\nIn the academic classification, case management\nprocesses can be related to descriptive, knowledgeintensive, or unstructured processes. Until recently, those\nprocess types have not been recognized as a part of BPM.\nThis partially explains why a majority of modeling\nformalisms presented on the BPM market today fail in\nproviding an appropriate level of adaptability while\nensuring the process validation and control for\ndescriptive, knowledge-intensive processes.\nTraditional, activity-driven formalisms for business\nprocess modeling, such as BPMN, encourage the early\nspecification of process details (at design-time), whereas\nfor knowledge-intensive processes, such as case\nmanagement, this is not possible. Thus, multiple\nmodeling levels (and corresponding specifications) for\nknowledge-intensive processes are needed.\nIn this work, we presented the modeling formalism\nbased on Declarative Configurable specifications (DeCo)\nthat includes three types of specifications. Configurable\ndesign specification focuses on the process goals\nindependently from the process deployment environment.\nDesign specification is non-deterministic and can be\nconfigured for various environments upon the process\ndeployment. Customized design specification reflects the\nknowledge about the process deployment environment\n(e.g. governmental norms and regulations, customer\nbehavior patterns, etc). It can be obtained by refinement\nof the design specification. In general case, this\nspecification is non-deterministic and supports run-time\nadaptability driven by emerging knowledge. DeCo\nrealization specification (or process trace) reflects one\nactual process invocation. Aggregated history of process\ntraces is beneficial for further process analysis and\nimprovement.\nThe proposed approach allows the BPM specialists to\nseparate the process goals from the process means and,\nfinally, from the concrete process realization details\nimproving the adaptability.\nWhile exposing a process to a complex environment, it\nis important to ensure that the constraints imposed by this\nenvironment are feasible and that the process goal can\neventually be achieved. This problem will be addressed in\nour future works that will be focused on the alignment\nbetween DeCo specifications.",
  "2012-R": "1 Introduction During the last decades, business process management (BPM) became an imperative for efficient functioning and evolution of organizations and gave a rise to the third wave of research interest in business process modeling and analysis [1]. Since then, the main efforts of researchers have been focused on development of process modeling languages that would be easy to understand by both technical and business users and that would provide better control over processes. As a result, the majority of the process modeling methodologies widely used today (e.g. BPMN, UML, EPC) is characterized by a powerful graphical notation, a rich design environment, and by the imperative style of their models. Imperative modeling largely contributes to the process control [2]. Modeling approaches based, for example, on coupling a graphical language such as BPMN and an operational language such as BPEL infinitely attract BPM practitioners giving them a toolbox to design, execute, test and eventually instantiate and deploy the process models. M. La Rosa and P. Soffer (Eds.): BPM 2012 Workshops, LNBIP 132, pp. 65–76, 2012. © Springer-Verlag Berlin Heidelberg 2012 66 I. Rychkova Requiring large upfront investments into a process scenario definition, such approaches still pay off very well for deterministic, repetitive processes (such as automated production lines). Assuming that execution scenarios are changing rarely, once the process is automated, an organization will quickly benefit from the economy of scales. Latest publications show the increasing interest of BPM practitioners in unstructured, knowledge-driven processes. The term “knowledge-driven” or “knowledge-intensive” refers to the fact that a process execution scenario significantly depends on knowledge of a human expert rather then on the predefined process model. Such process can vary from one execution to another demonstrating large unpredictability [3][4][5]. One of the examples of knowledge-driven processes is case “A coordinative and goal- management (CMP). OMG defines case management as oriented discipline, to handle cases from opening to closure, interactively between persons involved with the subject of the case and a case manager or case team”. Seeking to increase their efficiency by process automation, organizations also admit that for the knowledge-driven processes such as CMP, the ability to adapt the process scenario according to a situation (we call it post-design adaptability) is the most essential. This makes automation of case management processes following an imperative approach too expensive: a number of modifications to initial process model will quickly outweigh any automation benefits [6]. In the CMPM request for proposal released in November 2009 [4], OMG formulates in detail the problem of case management process modeling and support, illustrating the need for another paradigm. Declarative Configurable specification language (DeCo) was first introduced in [7]. With DeCo, we exploit an idea that an activity of “scenario definition” for a business process is not limited to a process design time (as imperative BPM requires) but it makes an integrated part of process deployment and even execution. In this paper, we define configurable process elements (namely, configurable data objects) and context- based configuration rules for DeCo. These modeling concepts allow one to continuously adapt the process along its lifecycle and, consequently, they pave the road for the automated decision-making support of knowledge workers. We illustrate our ideas on the example of a mortgage approval process. The reminder of this article is organized as follows: in Section 2 we discuss the related works. In Section 3 we introduce the mortgage approval process (a CMP example), present our motivations in creating DeCo and relate this work with our previous research. In Section 4 we discuss the modeling principles of DeCo and introduce the configurable data object and context-based configuration rules concepts. In Section 5 we present conclusions and discuss our future work. 2 Related Works Today, the majority of de-facto modeling standards including UML, EPC, BPMN imperative exploit the modeling style. Imperative process models are suitable for simulation [9], thus they can be highly advantageous for practitioners helping them to control the process and to exclude incorrect or undesirable scenarios already at design. BPEL[10] is a standard executable language for process models documented Towards Automated Support for Case Management Processes 67 in BPMN. Operational semantics for process model execution based on Petri Nets and Pi calculus is considered in [11][12]. The research reported in [13] proposes a formal semantics of BPMN defined using Petri nets. In [14], the technique for simulation and analysis of process specified with DEMO [15] is presented. Yet providing the means for simulation and control, imperative process models are proven to be restrictive [16]: specification of numerous options and variations for the sake of process adaptability becomes difficult if at all possible in such models [17][5]. declaratively: Alternatively, a process can be specified this modeling style supports non- determinism and allows modeler to postpone the decision making about the process execution scenario until its deployment or even execution. In [18], the detailed comparison of imperative and declarative modeling styles is provided. In [19][20], SEAM (for Systemic Enterprise Architecture Methodology) is presented. In SEAM, joint actions processes in an organization can be modeled as with implicit (nondeterministic) execution scenarios. Modeler refines SEAM models selecting an appropriate level of details. Thus the detailed specification of the process flow can be postponed. In the similar way, the MAP methodology [21][22] addresses the process intentions strategies. specifications using the notions of and Each intention in MAP can be realized following one or multiple alternative strategies, leaving the process execution largely nondeterministic and, therefore, adaptable for a given context. MAP notation was used to address process variability and to model process lines in [37]. Whereas some researchers decide to develop their modeling notations from scratch, others extend the existing standards providing them with the desired properties. In [23] both the EPC (Event Process Chain) and the BPMN metamodels are extended with elements for modeling process goals and process performance measures. In [24] the BPMN notation is extended to provide the concepts for querying the business process definitions and extracting the business process patterns. In [25][26] the (imperative) EPC notation is extended with the concepts for process configurability along the control-flow, data, and resource perspectives. While supporting process adaptability, declarative models encompass a significant ambiguity and, as a consequence, are not suitable for simulation. Numerous efforts to achieve adaptability and control under the roof of the same process modeling approaches have been reported in the literature. The underlying semantics is ranged from LTL (linear temporal logic) and FOL (first-order logic) to Petri nets enabling automated modelchecking and theorem proving techniques for model validation and analysis known from the software engineering. In [27], DecSerFlow language for Declarative modeling of service flows is presented. In [28] the same authors present the DECLARE system models. for supporting declarative (loosely-structured) process The formal semantics of DECLARE and DecSerFlow is based on LTL. The formal semantics for the Configurable Integrated EPC (C-iEPC) modeling notation presented in [25] is based on FOL and serves to validate the process model correctness. 3 DeCo: Motivation and Relation to Our Previous Research In [29][30] the declarative semantics for a graphical modeling notation for enterprise modeling called SEAM is discussed. In [29] we consider the variability aspects of 68 I. Rychkova business process modeling and propose declarative modeling approach and semantics based on Alloy [31]. Alloy specification language allows one to validate the conformance between a high-level declarative design specification of a process with its low level imperative implementation specification. In [32][7], we put the research of the previous years together in order to develop an approach offering to a modeler an extensive configurability opportunities while implementing the principles of declarative modeling and supporting automated model analysis and step-wise refinement [33]. We called this approach DeCo – for Declarative Configurable process specifications. In [36] we introduce a configurable roles and add this concept to MAP notation. This work introduces another two modeling concepts of DeCo: configurable data objects and contextual configuration rules. We illustrate the use of these concepts on the example of mortgage approval process. 3.1 Motivating Example: Mortgage Approval Process Mortgage approval process is a typical example of a case management process. In this paper, we provide a generic mortgage approval process description as defined by different financial institutions in the USA. The information provided below represents a compilation of guidelines and descriptions of mortgage approval process, provided by different loan consulting firms, financial advisors, and banks and available on the web (e.g. http://www.homebuyinginstitute.com/, http://www.mortgage-resource-center.com/, http://homebuyereducation.bankofamerica.com/, etc.) A mortgage is a loan for buying a house. The mortgage approval process can be divided into the following sub processes: Pre-qualification, Formal Application, Document Review, Pre-approval, Property Appraisal, Final Approval, Closing. Whereas the order of these main sub processes varies rarely, the documents required, the sequences of tasks, the participants of each sub process can be different depending on the place, the financial institution’s policies, and the applicant’s situation and requirements. In this paper, we focus only on the Formal Application sub process: Formal Application 0 An applicant can request the application package by e-mail or by post. Alternatively, all the forms can be downloaded from the web site of a prospective lender. 1 A mortgage application can be submitted electronically or during a personal meeting with the mortgage lender. 2 The exact set of documents may vary depending on the financial institution and the particular situation of an applicant. These documents may include: the social security card, record for past two years for residence address, employer data, various Internal Revenue Service (IRS) forms, recent pay-stubs, etc. 3-5 During the formal application, in some states, the mortgage lender provides the applicant with a Good Faith Estimate (GFE) of costs of loan closing; the applicant can be also asked to make a final decision on the type of mortgage loan and to lock in an interest rate. Towards Automated Support for Case Management Processes 69 during the formal application submission an applicant has to pay the 6 Usually application fee and the appraisal fee. For some agencies, however, the appraisal will be charged later, whereas the application procedure can be free of charge. Though substantially simplified, the description above illustrates the variability of activities, actors, and information involved into the process. The process scenario can also differ substantially depending on the execution context (e.g. the country, state, agency). For a financial organization operating globally and dealing with multiple environments definition of a single process model becomes a challenging task. In our previous work reported in [32] we modeled this Formal Application sub five process using BPMN and formulated modeling challenges common for imperative modeling approaches in general: 1. Need to specify task inputs/outputs while distinguishing obligatory and optional data objects, alternatives (possible replacements), and synonyms (identical artifacts called differently). 2. Need to specify role hierarchy, alternative roles and synonyms. 3. Need to specify optional, obligatory, alternative task and synonyms. 4. Need to specify multiple control flow possibilities. 5. Need to specify an impact of data on different tasks and the task flow. 4 Improving Adaptability of Mortgage Approval Process Model with DeCo 4.1 Replacing Imperative Scenario with Declarative Specification The graphical notation of DeCo is based on BPMN [34] whose modeling concepts are widely used and recognized by practitioners. But similarities terminate here since, compared to BPMN, DeCo implements the declarative modeling principles. These principles allow one to postpone the decision making about the eventual process scenario until its deployment or even execution. Declarative approach to process modeling represents an alternative to continuous exception definition, use of “if-then-else” or “switch” constructions within a traditional, workflow-based process model [31]. Instead of modeling a flow of process activities, in DeCo, we focus on modeling individual activities (or clusters of contract activities). Each activity is associated with its that specifies (i) a conditions or a situation when this activity can be executed (ii) a situation that will result from this execution. As a result, instead of a flow of preordered tasks, DeCo specification describes: - a set of tasks (with no explicit ordering) that must, should, or could be executed during the process; - a set of rules allowing a dynamic task selection at a given process state from the list of tasks enabled at this state [7]. In a general case, the resulting process scenario is highly nondeterministic as each of the enabled task, if selected, will result in a different case development. Strictly speaking, 70 I. Rychkova the concrete scenario of how a given case has been managed can be known only upon the process termination. We call it an execution trace. 4.2 The Role of Context “.. any information that can be used to characterize Dey in [35] defines a context as the situation of an entity. An entity is a person, place or object that is considered relevant to the interaction between a user and an application, including the user and applications themselves.” As our example shows, mortgage approval process can vary strongly from country to country, agency to agency, and even from one mortgage application to another. These entities make a part of the process context and should be taken into consideration while configuring a concrete process scenario. More specifically, the context relevant to our process should encompass the characteristics of a mortgage lender and an applicant. The mortgage lender (a financial institution) can be characterized by its internal policies and adopted standards in customer services, risk management etc. For any concrete agency, also its resources available for process execution should be taken into consideration: number of employees, their roles, expertise, and responsibilities. Moreover, any financial institution should comply with some external regulations (e.g. federal law, etc.) that are defined by its country or state of residence. Thus, a geographical situation of a mortgage lender is also a context for mortgage approval process. The information about a mortgage applicant is explicitly handled by the process – it is a mortgage application file itself. At design time, usually little is known about a process context: a CMP should comply with an industry standard, other external regulations and, if known, internal policies of an organization implementing it. At deployment, the context is getting more explicit: agency type, its location, local resources and other specific facts about the process context allow designer to configure the process accordingly. The final and the most specific part of the process context (or case context) is an application file – it appears and fills in during the mortgage approval execution. It allows configuring the process scenario in all details. According to DeCo modeling approach, all the emerging context information should be constantly transformed into process configuration rules (at design, and deployment) or used to check the task compliance with such rules (at deployment and execution). Thus, the DeCo specification of mortgage approval process can be seen as a repository of tasks where each task can be instantiated for a given country, agency and even application case by answering the following questions: -what resources (data, people, etc) are required for the task execution? -what will be produced/modified upon termination of this task? -what other constraints (e.g. functional, temporary, legacy, etc) must be fulfilled for the task execution? DeCo does not provide means for modeling process context but rely strongly on the availability of context information. Coupling of DeCo with some context modeling approach will be explored in our future research. Towards Automated Support for Case Management Processes 71 4.3 Configurability at Multiple Perspectives the In the literature, several major perspectives of the process models are specified [8]: control flow perspective that captures the temporal ordering of process tasks, events, and the data perspective decision points; that captures the lifecycle of data objects (creation, the resource perspective usage, modification, deletion) within the process; that describes how the process is carried out within the organization and deals with roles and resource the operational perspective assignments; that addresses the technical aspects of process execution and specifies the elementary process tasks and their assignment to concrete the context perspective applications or application components of the organizations; that the performance describes the attributes related to the process execution context; perspective, addressing the process cost effectiveness. DeCo defines the concepts and semantics addressing process model configurability (i) control flow declarative style on the perspective by supporting and allowing non- data, operational, resource deterministic execution scenarios; (ii) on the and perspectives, configurable data objects, tasks, and roles providing the modeling notation for context contextual configuration rules. respectively; (iii) on the perspective, providing Configurable roles in DeCo was already addressed in [36]. In this paper, we introduce the notation and semantics for configurable data objects and context-based configuration rules in DeCo. The other concepts will be addressed in detail in our future publications. 4.4 Configurable Data Objects and Their Semantics A case of a foreign applicant recently arrived to the US following a new job assignment can become extremely difficult for a potential mortgage lender. The main problem with this required case is that the applicant cannot provide the documents by the standard process; other instead, she is submitting documents issued by a bank, an employer, or authority of her previous country of residence. Not matching the standards, such application can launch a long investigation process or can be simply rejected. optional DeCo defines configuration mechanisms for data objects that include and obligatory consumed, produced, data objects, specifies if a certain data artifact is or modified synonym alternative by a given task and defines and relations between data objects as illustrated in Fig.1. These mechanisms help knowledge worker to anticipate the situation that she never met before, to process the data artifacts not “previewed” by a standard process scenario (if one exists) and to use them for more efficient decision- making. Data object configuration in DeCo corresponds to configurability along the data perspective according to the taxonomy defined in [8]. Tax forms Tax Figure 1 illustrates the data object configuration diagram for and return forms required for formal mortgage application in the USA. A rectangle with \"C\" thick outline and in the right corner refers to a data object that can be further configured in the process model based on the situation or context. alternatives By we specify the data objects that can replace the data object synonyms originally required by the task; are completely identical data objects used under different names by organizations or departments of one organization. A dashed 72 I. Rychkova line with a tag “syn” depicts the synonym relation. A solid line with a tag “alt” deppicts the alternative relation. Morre formally, these relations can be specified as follows: d ,..,d Let be a taskk where are the data objects consumed, producedd, or 1 n T. modified by 11. Fig. Data object configuration in DeCo Definition 1: x synonym d d π Data object is a of for a given process iff it can replacce i i T π: in with no impact on In Fig. 1 both tax forms annd tax return forms can be called Internal Revenue Servvice (IRS) forms [source: wikipeedia]. This is expressed using a synonym relation. Definition 2: T y The fact thatt for a given process aanndd ttaasskk the data object iss an alternative d y for under a ccertain condition R implies that: If R holds then cann be i d accepted by T instead of : i and In Fig.2, the form 1040EZZ is a simplified form that can be applicable to single joint filers with no depenndents [source: wikipedia]. It can be considered as an alternative of the obligatoory 1040 form. The rule 2.2 in this diagram expliccitly specifies the condition wheree this alternative is applicable. d optionnal A data object is for a given task T if it can be omitted in the contract oof T. For example, the GFE certiificate is optional for some countries and can be omittedd in Register Application task. In DeCo, optional data objects are depicted with dasshed borders. Using the data object cconfiguration, we can separate the management of ddata objects from the process sceenario management. Let us now come back too the foreign applicant example. Considering that the bannk is willing to satisfy the foreiggn applicant’s request, the main trouble for a loan offficer would be to identify the relaations between the standard forms provided by this appliccant Towards Automated Support for Case Management Processes 73 but unknown for the current bank and the standard forms required by the mortgage approval process of this bank. Such investigation terminates with conclusions like the following: - The form X provided by the applicant is an analogy of the form Y required by the process; - The form X provided by the applicant can be accepted in place of the form Y required by the process under certain condition C; - The form X provided by the applicant is irrelevant to the process. This information can be expressed in terms of synonym, alternative relations or optional/obligatory property in DeCo: - The form X provided by the applicant is an analogy of the form Y required by X is a synonym of Y within this process;  the process - The form X provided by the applicant can be accepted in place of the form Y if condition C is met, then  required by the process under certain condition C X is an alternative to Y within this process; X is  - The form X provided by the applicant is irrelevant to the process optional within this process. The more synonym or alternative relations are determined, the more process model evolves, and the better decision-making support for the loan officer is provided. 4.5 Modeling Contextual Configuration Rules One process often needs to be customized in order to meet the requirements of its deployment environment (e.g. country, state, corporate division) and/or to anticipate the concrete case circumstances [7]. Therefore, the possibility to enable/disable synonym and alternative relations and optional/obligatory properties of an element has to be provided not only at process design but also at customization and instantiation. For this purpose, (context rules DeCo defines context configuration rules for short). In DeCo diagrams, context rules are depicted with dark circles labeled with a name of the rule. In the current version of DeCo, these rules are formulated as predicate True, expressions. If, based on the context, such predicate evaluates to then the corresponding configuration mechanism is enabled. This concept can be illustrated on the following example: According to the banking regulations in the US, in certain cases the mortgage applicant may be asked to provide the lender with one or several supplementary tax forms and tax return forms. The diagram in Fig.1 illustrates the context rules implementing these regulations. Here the form W2 (Wage and Tax Statement) is an obligatory data object for the mortgage application; the form 2555 (Foreign Earned Income) should be provided by taxpayers who have earned income from sources outside the United States (a context rule 1.1). The form 1040A is limited to taxpayers with taxable income below $100,000 (this is expressed by the rule 2.1.). The form 1040EZ is a simplified form that can be applicable to single and joint filers with no dependents [source: wikipedia]. Under this condition, which can be expressed as a contextual rule 2.2, this form can be considered as an alternative of the obligatory 1040 form. 74 I. Rychkova Context configuration rules can be defined based on (a) external regulations imposed by a concrete location (country, state, city) of the organization implementing the process, (b) internal policies specified at a company level (its country division, local branch or agency), (c) particular case conditions (e.g. a foreign applicant, sub-prime, first-time buyer, etc). Other context rules for the mortgage approval process may include: isAvailable(InternalAppraisalAgent) = true; agencyLocation = North Carolina; etc. The context rules should be specified for a process at design, refined at deployment and then controlled at execution. 5 Conclusion and Future Work In this work the Declarative Configurable process specification language has been presented. The graphical notation of DeCo is based on BPMN, but similarities terminate here since, compared to BPMN, DeCo implements the declarative modeling principles. DeCo language is in its infancy. Validation of its modeling concepts and development of a modeling tool are the main milestones for our future research. In this paper, we specified configurable data objects and contextual configuration rules. These concepts are integrated into DeCo process models. Other concepts defined in DeCo will be presented in our next publications. Using the Mortgage approval process as an example, we illustrated how declarative modeling principles and configurability mechanisms can be used in order to improve the post-design process model adaptability - the characteristic utterly desired for knowledge- driven processes. Since the execution scenario of such processes cannot be predefined at non-deterministic design, declarative specifications become a natural solution. Though encompassing a significant ambiguity and not suitable for simulation, declarative process specifications may serve a useful tool for process validation and verification – the techniques known from software engineering [31]. As soon as new context information emerges, the declarative process specification evolves and ideally step-wise becomes more and more deterministic. This evolution can be compared to a refinement of software specifications [33]; the notion of refinement for graphical specifications is presented in [30]. The step-wise process refinement and its validation in DeCo will be addressed in our future publications. Context rules play an important role in process configuration. Providing that an organization can be exposed to various sources of regulations, thousand context rules will emerge in a process model. This increases a risk of conflicting rules leading to the process model inconsistency. DeCo provides a FOL-based semantics for the context rules. The next step of our research will be focused on modeling, validation and verification of context rules. The modeling method presented in this work is in its infancy. The modeling notation still requires major improvement in order to be adopted by practitioners. However the most important issue for us is scalability and validity of the DeCo process specifications. Modeling more elaborated (real life) case management process with DeCo and validation of results is a critical milestone in our research. Towards Automated Support for Case Management Processes 75",
  "2013-BBRWa": "1 Introduction Business rules are everywhere. Every enterprise process, task, activity, or func- tion is governed by rules. However, some of these rules are implicit and thus poorly enforced, others are written but not enforced, and still others are per- haps poorly written and obscurely enforced [1]. In this work, we propose an in- teractive, simulation-driven approach for business rules discovery. This approach will provide a domain specialist with an instant feedback helping her to reason about the existing business rules, to discover new or implicit business rules and, eventually, to improve their enforcement. In addition, \"capturing the logic of an entire business would require probably many thousands of rules; a smaller subsystem, perhaps several hundreds\" [2]. They are often made by different people at different time, driven by different business ideas in mind. The rules consistency or the need to validate that a new rule does not con(cid:13)ict with the existing rules is a real problem. Our approach [3]; use of formal is based on rule modeling and simulation in Alloy Analyzer 2 Simulation-Driven Approach for Business Rules Discovery speci(cid:12)cations and model checking techniques allows us not only to discover the new rules but also to preserve validate their consistency. We illustrate our approach on the example of the Order Processing of G(cid:19)en(cid:19)erale Ressorts SA: we formalize the existing business rules in Alloy, then we simulate the Alloy model and analyze the generated instances. We discover a cluster of instances that is derived from the Alloy model and is not explicitly covered by the existing business rules. We generalize the property of this cluster as a speci(cid:12)c business case that, in fact, must be documented (restricted) with a corresponding business rule. We show that the new rule added to the model makes the model inconsistent and, thus, requires adjustments. We provide the solution, (cid:12)rst, in Alloy, and then, explain its meaning in business terms. The reminder of this paper is organized as follows: In Section 2, we discuss our motivation and present the related works; In Section 3, we introduce our example - the Order Processing, speci(cid:12)ed for G(cid:19)en(cid:19)erale Ressorts SA; then we formalize this example in Alloy and illustrate the BR discovery with Alloy Analyzer; In Section 4, we generalize our approach and formulate our (cid:12)ndings as XX steps to interactive BR discovery based on formal model checking; in Section 5 we present our conclusions. 2 Motivation and Related Work According to [2], \"A business rule is a compact statement about an aspect of a business. It’s a constraint, in the sense that a business rule lays down what must and must not be the case. At any particular point, it should be possible to determine that the condition implied by the constraint is true in a logical sense; if not, a remedial action is needed.\" Many research and industrial publications are focused on challenges associ- ated with business rules (BR). In industry, vendors such as ILOG (currently a part of IBM), FICO Blaze Advisor and Pega Systems, Inc. have been developing business rule engines (BRE) since the late 1980s and are now leaders in an emerg- ing BRE segment [4, 5, 6]. In academia, the computer sciences and engineering outlets have been active in business rule research [7, 8, 9], with extensive studies in rule programming, meta-modeling, rule mining, rules engines, business user interfaces and their role in services orientated architectures (SOA). Furthermore, joint academic and industry developed Object Management Groups (OMG) Se- mantics of Business Vocabulary and Business Rules (SBVR) standards (released in September 2006), which is intended to provide standards surrounding BR structure, terminology, classi(cid:12)cations and meaning in BR authoring and reposi- tories [10]. Different approaches propose different phases in business rules management [2] the main phases are: discovery, de(cid:12)nition, review, life cycle (BRMLC). In maintenance. By [1] the main phases are: discovery, analysis, design, authoring, [11] these phases are: plan, capture, organize, author, validation, deployment. In distribute, test, apply. In this paper, we focus on discovery (i.e. capturing) phase Simulation-Driven Approach for Business Rules Discovery 3 of BRMLC. The goal of discovery phase is to identify the potential business rules impacting the domain segment in development. To see the effect of the introduced business rules, the designer must go through a number of phases, such as analysis, design, authorizing, validation and deployment. It would be much more effective, if the designer could get instant, visual feedback on how new business rules in(cid:13)uence the behavior of a process. Examples of the company behavior could help him to realize what constraints are missing in the model. We propose the approach based on Alloy simulation that helps to discover new BR in an interactive way, while ensuring the consistency and validity of the overall set of BR. Although BR traditionally are expressed in natural language [12], the works presented in [13] and in [10, 14] report on other forms of BR formalization: in [13], the diagramatic language is used; in [10, 14, 15] the rules are speci(cid:12)ed with formulas in modal logic. In this work, we use an Alloy speci(cid:12)cation language (based on (cid:12)rst order logic) and propose a technique for BR discovery based on model simulation and analysis in the Alloy Analyzer tool. 3 Business Rules Discovery with Alloy Analyzer In this section, we present the process of interactive discovery of business rules, which is based on formal model checking and implemented using the Alloy An- alyzer tool [3]. We start with introducing our example - the order processing, speci(cid:12)ed for G(cid:19)en(cid:19)erale Ressorts SA. First, we specify the order processing and its associated business rules in natural language; then, we introduce Alloy spec- i(cid:12)cation language [16] and model our example in Alloy. Finally, we demonstrate how the Alloy Analyzer tool can assist in interactive discovery and validation of business rules that have been missing/omitted/implicit in the initial business speci(cid:12)cation of Order Processing. 3.1 Case Study: Order Processing in G(cid:19)en(cid:19)erale Ressorts G(cid:19)en(cid:19)erale Ressorts SA is the market leader in watch barrel springs and a (cid:12)rst- class manufacturer of tension springs, coil springs, shaped springs and industry components [17]. G(cid:19)en(cid:19)erale Ressorts SA works with thousands customers and strives to ensure the highest quality both for its products and for its customer services. Order processing is one of the strategic activities in G(cid:19)en(cid:19)erale Ressorts SA: it covers a complete order life cycle, from order creation to payment and delivery. Whereas the company constantly improves its technological processes in order to shorten the production cycle, the payment may take months after the product is delivered1. Therefore, (cid:13)exible business rules for order processing and customer transactions management are essential for GR. G(cid:19)en(cid:19)erale Ressorts. 1 The \"shipping after payment con(cid:12)rmation\" policy is not acceptable for this industry in general and for G(cid:19)en(cid:19)erale Ressorts SA in particular. 4 Simulation-Driven Approach for Business Rules Discovery Order processing includes the following processes: order creation, order preparation, shipping and accounting. It is also closely related to the customer management processes in the company. The whole process, from the moment the customer makes an order to the delivery and the accounting is known as 1 [18]). order-to-cash cycle (Fig. In this paper, we de(cid:12)ne the (simpli(cid:12)ed) order processing activity that focuses on order creation, delivery and payment only: A customer submits an order request for manufacturing a watch component - a part; the con(cid:12)rmed order is then prepared and delivered to customer. As stated above, payment for the con(cid:12)rmed customer orders is a necessary condition to (cid:12)nalize the overall order processing transaction for a given order, though it is not required for order delivery. Below, we present list of business rules related to order processing: 1. Order creation BR1.1 A customer order can be created and con(cid:12)rmed only for the customers regis- tered in the enterprise information system. BR1.2 A customer order can be created and con(cid:12)rmed only for the parts existing in the product catalog. BR1.3 If an order request from a new customer is received, this customer has to be registered in the enterprise system. BR1.4 ... 2. Order delivery BR2.1 Every con(cid:12)rmed customer order must be eventually delivered to the customer. BR2.2 ... 3. Accounting BR3.1 Every con(cid:12)rmed customer order must be eventually paid by the customer. BR3.2 ... 4. Customer management BR4.1 Every customer record must contain one customer name. BR4.2 Every customer record must contain at least one valid billing address. BR4.3 Every customer record must contain at least one valid shipping address. BR4.4 Every customer record must be associated with a previous orders history. BR4.5 A customer whose transactions with GR is equal or superior to XX XXX euro per year receives a status of strategic customer at GR. BR4.6 A customer whose transactions with GR is inferior to XX XXX euro per year receives a status of regular customer at GR. BR4.7 Strategic customers must always be able to submit the order with GR. BR4.8 ... 3.2 Alloy Alloy [19] is a declarative speci(cid:12)cation language developed by the Software De- sign Group at MIT. Alloy is a language for expressing complex structural con- straints and behaviour based on (cid:12)rst-order logic. [3] is a tool for the automated analysis of models written The Alloy Analyzer in the Alloy speci(cid:12)cation language. Given a logical formula and a data struc- ture that de(cid:12)nes the value domain for this formula, the Alloy Analyzer decides Simulation-Driven Approach for Business Rules Discovery 5 Fig. 1: Order-to-cash cycle - Adapted from [18] whether this formula is satis(cid:12)able. Mechanically, the Alloy Analyzer attempts to (cid:12)nd a model instance - a binding of the variables to values - that makes the formula true. [20] The syntax of Alloy is similar to the syntax of OCL the Object Constraint Language for UML [21]. Data structures are represented with signatures la- belled by the keyword sig and (cid:12)elds. Alloy reusable expressions (i.e. functions) and constraints (i.e. facts, predi- cates and assertions) [16] can be used to reason about data structures and to de(cid:12)ne the relationships between them. In our previous work [22] we de(cid:12)ned the iterative process for service designe where the Alloy signatures, facts and predicates were used for service speci(cid:12)ca- tion. In this work, we extend the use of Alloy constructs: in particular, we use Alloy constraints for modeling and validation of Business Rules. By their de(cid:12)nition, business rules are intended to assert business structure or to control or in(cid:13)uence the behavior of the business [23]. Structural rules de(cid:12)ne the business information model. A behavioral rule, on the other hand, is about how the business reacts to business events. They are speci(cid:12)ed when something happens at the boundaries of the system [10]. In this approach we deal with both kind of business rules. Fact is a model constraint that permanently holds. We use Alloy facts to specify that must hold for entire model (i.e. structural business rules and be- havioral rules that are system invariants). For example: Every customer record must contain at least one valid billing address - is a structural business rule that can be modeled as an Alloy fact associated with a corresponding data structure specifying customer. Predicate is a constraint that holds in speci(cid:12)c context or for a speci(cid:12)c part of the model only. We use Alloy predicates to model the business rules with a clear scope or context. For example: If an order request 6 Simulation-Driven Approach for Business Rules Discovery from a new customer is received, this customer has to be registered in the enter- prise system. - this rule has a clear scope where it must be applicable. It can be modeled as an Alloy predicate. Whereas some business rules can be seen as restrictions or prohibitions and, thus, modeled with Alloy facts and predicates, other business rules have a different nature: For example \"Strategic customers must always be able to submit the order with GR\" - this business rule is not a restriction, but a necessity - a property that has to be ensured or provided de- 2 spite of any other conditions. We model this type of business rules using Alloy assertions. Assertion is a property that the designer believes should be implied from the model and can check if it can be deduced from the other (permanent or contextual) constraints. 3.3 Alloy Speci(cid:12)cation for Order Processing Order Processing in Alloy The data structure for the order processing is modeled using Alloy signatures as illustrated in Fig. 2. (a) Signature GR (b) Signatures OrderRequest Fig. 2: Design - Alloy Signatures Alloy signatures (sig) can be abstract or concrete, can have explicit cardi- nalities (e.g. only one OrderRequest object can be treated by the service at a time), and can contain one or multiple (cid:12)elds. Each (cid:12)eld indicates a relation to a corresponding object type and can be considered as an analogy of attributes in object-oriented (OO) languages: for example indicates that the name: one Name OrderRequest object has an attribute name of the type Name. We use one and set cardinalities for the (cid:12)elds to distinguish \\one to one\" from \\one to many\" relations. For the order processing example, we specify a system - GR - as an Alloy signature illustrated in Fig. 2a that is characterized by the following (cid:12)elds: partSet - the set representing all parts (watch components) that can be ordered; customerSet - the set of customers registered in the GR information system; orderCon(cid:12)rmedSet - the set of orderes created and con(cid:12)rmed in GR; 2 This distinction has been already proposed in [15], where two modal operators are de(cid:12)ned: necessity (with its negation: possibility) and obligation (with its negation: prohibition). Simulation-Driven Approach for Business Rules Discovery 7 orderDeliveredSet - the set of orders (subset of created and con(cid:12)rmed orders), which are delivered to their customers; orderPaidSet - the set of orders (subset of created and con(cid:12)rmed orders), which are paid by the customers. Similarly to [24], we adapt the state-oriented perspective and specify the execution of order processing in terms of a state transition: we de(cid:12)ne a pre- state - GR pre - that describes the state of a system (GR) before the order processing has been performed and the post-state - GR post - that describes the condition that must hold for the system upon the activity termination. The objective of our model - is to assist a business analyst in discovery of the (implicit of missing) business rules associated with order processing: to do so, we are going to investigate how the status of a customer orders is changing during the order processing activity. For a given order, this status can be identi(cid:12)ed by analyzing the orderCon(cid:12)rmedSet, orderDeliveredSet and orderPaidSet of GR. Note, that the same order can be in one or multiple sets at a time. For example, if the order is in the orderPaidSet - it is paid. Consequently, if the order is not in orderPaidSet in pre-state, but is added into orderPaidSet in post-state upon termination of a given activity, it means that it has been paid. The statuses cannot be canceled, i.e. once the order is paid, it cannot be \"unpaid\", etc. Once the data structure is de(cid:12)ned, we specify how the order processing will be executed (behavior). Following the description from Section 3.1, we represent concurrent3 the order processing activity as a execution of order creation, order payment and order delivery processes. Order processing and its three component processes are modeled as Alloy predicates. These predicates show a transition between GR pre and GR post states. The proposed speci(cid:12)cations are \"black box\" - they do not show how the corresponding processes are executed but only the (cid:12)nal result of their execution visible in GR (i.e. how the GR attributesorderCon(cid:12)rmedSet, orderPaidSet and orderDeliveredSet will \"look like\" upon the process termination). The predicates specify the conditions that must hold in GR upon order cre- ation/delivery/payment. For order creation this condition is: considering order creation business rules are respected (orderCreationBR), the new order must be created and added to the corresponding order set - orderCon(cid:12)rmedSet - in the post-state. For order delivery it is: Considering a customer with an unde- livered order in his order history (i.e. the order, which is not in the system’s orderDeliveredSet) in pre-state, this order must be delivered and added to the corresponding set in post-state. For order payment it is: Considering a customer with an paid order in his order history (i.e. the order, which is not in the sys- tem’s orderPaidSet) in the pre-state, this order must be paid and added to the corresponding set in post-state. Along those lines, the orderProcessing predicate (Fig. 3d) speci(cid:12)es that, upon the order processing termination, three processes (order creation, order delivery 3 We exploit the declarative approach to activity modeling and avoid the preordering of processes within order processing. 8 Simulation-Driven Approach for Business Rules Discovery and order payment) must be accomplished. In Alloy, this corresponds to a logical conjunction of orderCreation, orderPayment and orderDelivery predicates. (a) OrderCreation process (b) OrderPayment process (c) OrderDelivery process (d) OrderProcessing activity Fig. 3: Order Processing activity as a combination of order creation, order de- livery and order payment processes Business Rules To illustrate our approach, we use subset of the business rules given in Section 3.1. The business rules we select are: BR1.1, BR1.2, BR2.1, BR3.1 and BR4.7. These business rules are shown in Fig. 4. The business rules BR1.1 and BR1.2 are modeled as predicates, as they have an explicit scope - order creation process. In Alloy, BR1.1 states that the customer has to be registered in the customerSet in pre-state - before we start creating the order. This predicate (customerExists) is called in predicate order- Creation via orderCreationBR predicate (Fig. 3a). The predicate customerExists together with its call in orderCreation claims that the order can be created only if the customer is registered in the customerSet. BR1.2 states that for order cre- ation requested part should exist in the partSet in the order creation pre-state. Whenever we run orderCreation the rule will be automatically respected, as the Simulation-Driven Approach for Business Rules Discovery 9 new order will be created only if the customer exists in the system. Otherwise, the orderCon(cid:12)rmedSet will remain the same (Fig. 3a). BR2.1 is modeled as a fact. The fact claims that when we do delivery of order, all the existing orders will be eventually delivered in post-state. Similarly, BR3.1 is represented as a fact that claims the same for order payment. These rules have a global scope. Finally, BR4.7 is modeled as an assertion: we have to ensure that the strategic customer can always create new order despite of any other conditions. This is the \\necessity\" rule that we want always to hold. We write it as assertion, which can be checked in the model. The assertion claims that whenever there is an order request for the strategic customer for the existing part, the new order is created in EIS. If the assertion is valid, it means that this business rule is respected in the system. In case we get counterexamples, the rule is not respected and we need to revise the business rules. (a) BR1.1 - A customer order can be (b) BR1.2 - A customer order can be created and con(cid:12)rmed only for the created and con(cid:12)rmed only for the customers registered in EIS. parts existing in the product catalog. (c) BR2.1 - Every con(cid:12)rmed customer or- (d) BR3.1 - Every con(cid:12)rmed customer der must be eventually delivered to the order must be eventually paid by the cus- customer. tomer. (e) BR4.7 - Strategic customers must always be able to submit the order with GR. Fig. 4: Initial Business Rules 10 Simulation-Driven Approach for Business Rules Discovery 3.4 Business Rules Discovery for Order Processing Discovering New Business Rules Our approach to BR discovery is based on simulation. The idea is to simulate the Alloy model representing the subset of our system of interest (its structure, behavior and initial business rules modeled as Alloy constraints) and to analyze instances generated by the Alloy Analyzer tool. These instances, in our case, represent the scenarios of order processing enabled by our created model. The objective of model simulation is twofold: (cid:12)rst, to check our model for consistency (absence of contradictory constraints in business rules), and second, to test the random set of model instances generated by Alloy Analyzer for discovering (possibly) missing BRs. We call this phase a business rule discovery. Simulating order creation process, we (cid:12)nd the instances where the customer can make new order, even if he did not pay his previous orders that have been already delivered. This scenario is illustrated in Fig. 5. The scenario shows a regular customer (grey parallelogram on top) creating an order. This customer is associated with 2 orders (Order0 and Order1 ) in his history (black rectangles). Order 1 is already delivered and unpaid. Despite this, he was able to create the new order Order0. We know that this is the new order because we can see in pre-state view of the system that it was not in the orderCon(cid:12)rmedSet in pre- state and has appeared in post-state in Fig. 5. This could potentially bring to a company a lot of unpaid orders and short or long-term loses. The domain specialist decides weather he needs to de(cid:12)ne new BR to restrict this behavior to make sure the interest of the company is protected. In case he does, the designer translates the business rule into Alloy. Fig. 5: Order Accepted with Unpaid Orders in the History Business Rules Consistency Checking We provide a domain specialist with an instant feedback helping him to reason about the existing business rules, to Simulation-Driven Approach for Business Rules Discovery 11 interactively discover new or implicit business rules and, eventually, to improve their enforcement. Once the missing rule is con(cid:12)rmed by a business (domain) specialist, it can be added to our Alloy model. The new business rule covering this business case is: { BR1.3. New order cannot be created for the customer order if there are deliv- ered but unpaid orders in the customer’s order history. This business rule represented in Alloy is in Fig. 6. This business rule has an explicit context - order creation. We model it thus as a predicate. In predicate customerMustPayDeliveredOrdersBeforeNewOrder, we claim that the all orders from customer’s history that are delivered has to be paid. This predicate is called in orderCreation predicate via orderCreationBR. With combination of the three predicates, we claim the if we do creation of new order for some customer, his past orders that have already been delivered have to be paid. Fig. 6: BR1.3. New order cannot be created for the customer order if there are delivered but unpaid orders in the customer’s order history. As a result, simulating the model, we observe no cases where the order is created and unpaid delivered orders exist for a given customer. However, when we check the validity of the business rule for strategic customer by running the assertion in Fig. 4e, we receive the result that it is not valid. The counterexamples show the cases where this rule is not respected (Fig. 7). In this example, we can BR4.7 see that the new order is not created for strategic customer, meaning (Fig. 4e) is not valid. As strategic customers are of crucial importance for GR, they should by-pass the new rule and be able to order even if they have some unpaid orders. Thus, by introducing the new rule, we have created a con(cid:13)ict with the existing rules and overspeci(cid:12)ed the model. Therefore, we need to relax the rules. In order to resolve current con(cid:13)ict, GR de(cid:12)nes separate rules for regular and strategic customers: { BR7. For regular customers, the new order cannot be accepted if there are unpaid orders in the order history. 12 Simulation-Driven Approach for Business Rules Discovery Fig. 7: Example - New Order Not Created For Strategic Customers Because of Unpaid Past Orders We replace the rules BR4.7 and BR6 with the two new rules. These rules in Alloy are shown in Fig. ??. BR7 claims that if the order is created for the regular customer, all his past orders must be paid. BR8 claims that if the order is created for strategic customer, there can be maximum 2 unpaid orders for that customer. Fig. 8: BR7 - Regular Customers Must Pay Past Orders Before Placing New Order If we check now the assertion in Fig. 4e, we get no counterexamples, showing that the rule is valid again. If we run order creation process we can obtain the instances showing that the strategic customer can order even with unpaid orders in his history (Fig. 9). Option 2: BR1: For regular customers, the con(cid:12)rmed order must be delivered only upon the payment received; BR2: For large accounts, the con(cid:12)rmed order must be delivered; BR3: For large accounts, the con(cid:12)rmed order must be even- tually payed; .... BR1000045: the large accounts must always be able to submit the order. Simulation-Driven Approach for Business Rules Discovery 13 Fig. 9: Strategic Customer Can Make New Order With Unpaid Orders 4 An Interactive Discovery and Validation of Business Rules: ??? In this section, we explain the general process for interactive automated discovery of business rules with Alloy Analyzer. The basic steps of our simulation-driven business rule discovery approach are: 4.1 BR Speci(cid:12)cation in the Natural Language The business analyst speci(cid:12)es the rules in the natural language. These are the initial business rules he elicited from different roles in the company. Very often these set of rules is not covering some implicit rules. The following steps help him detect the new business rules. 4.2 Speci(cid:12)cation in Formal Language The designer translates the business rules in natural language to Alloy declara- tive language. He also specify the whole model in Alloy (including data structure and behavior), so that he can detect how the business rule in(cid:13)uence the com- pany behavior. The business rules in Alloy can be represented as Alloy facts, predicates or assertions depending on the type of the business rule, as explained in Section 3.2. This way, the designer has the power to express different scope of business rules, which can be very useful. 4.3 Simulation and Analysis The designer simulate the model with the initial business rules using Alloy An- alyzer tool. The objective of model simulation is twofold: (cid:12)rst, to check our model for consistency (absence of contradictory constraints in business rules), 14 Simulation-Driven Approach for Business Rules Discovery and second, to test the random set of model instances generated by Alloy An- alyzer for different kind anomalies. If underspeci(cid:12)cation anomaly appears - the \\unwanted/unexpected\" behavior is identi(cid:12)ed - the analyst should con(cid:12)rm that it is correct to have this instances or detect is as a problem. In the second case an explicit rule need to be speci(cid:12)ed for the cluster of \\unwanted/unexpected\" instances. 4.4 Adding New BR The business analyst speci(cid:12)es the new business rule in natural language. The designer translates the business rule to Alloy language and simulate again the model. New rule can potentially be in con(cid:13)ict with the existing one, can add unnecessary constraints to the model and overspecify the model. If the business analyst con(cid:12)rm this is the case, the business rules should be adapted. 4.5 Adjusting Model to New BR In case the overspeci(cid:12)cation appears, the rule should be replaced by considering different cases when it should apply. The business analyst gives proposition of the changes of business rules. The designer translates relaxed business rules into Alloy and simulate the model to con(cid:12)rm with business analyst everything is correct now. The steps 4.3, 4.4 and 4.5 are repeated until there is no anomalies detected and business analyst con(cid:12)rms that the system behaves by his expectations. 5 Conclusion We have presented an interactive approach for business rule discovery based on simulations in Alloy Analyzer tool. Business rules are represented as Al- loy facts, predicates or assertions, depending on the type of business rule (ne- cessity/possibility, obligation/restriction; or maybe to put our categories al- lowance/obligation/restriction/possibility - but maybe this is the same :) if the necessity is the same as allowance). This enables to de(cid:12)ne precise scope of busi- ness rule - do they apply to the whole model, just to the given context or they should always be implied from the model. The business analyst de(cid:12)nes the initial business rules in natural language, which the designer translates to Alloy formal language. The designer also speci- (cid:12)es the model of the company. The model with the initial business rules in then simulated using the Alloy Analyzer tool. During the simulation, the designer checks if the business rules are consistent and checks together with business an- alyst if some anomalies appear in the behavior. In case they appear, business analyst suggest new rules in natural language, which the designer translates to the formal language. Finally, the model should be adjusted to the new rule, since the inconsistencies could appear between the existing and the new business rule. Simulation-Driven Approach for Business Rules Discovery 15 This way, the business analyst can interactively analyze the existing business rules and discover the new and implicit business rules. The next step in our approach is to make the language for business rules speci(cid:12)cation closer to the business analyst. One way to this is to use Attempto Controlled English (ACE), a controlled natural language, i.e. a rich subset of standard English designed to serve as knowledge representation language. in this way, we could allow the analyst to express texts precisely, and in the terms of their respective application domain and perhaps to avoid having the designer as an intermediate step of business rule discovery.",
  "2013-BBRWb": "1 Introduction A considerable gap between business and technical worlds (often referred to as the business/IT alignment problem [1]) represents a serious issue for imple- menting the “co-evolution” of business and technical specification of a service in service design and development. Therefore, we explore the invariants as a linking mechanism between business and technical service perspectives. We propose a method for agile service specification that extends Systemic Enterprise Architecture Method (SEAM) [2]. SEAM models can be used by business and technical specialists to visually describe an enterprise system, its structure and services it provides. We propose a method that allows us to sim- ulate and validate visual service specifications defined in SEAM. It defines five design activities (design, simulation and simulation-based testing, analysis and anomaly resolution, validation, refinement) that can be performed sequentially 2 Biljana Baji(cid:19)c-Bizumi(cid:19)c, Irina Rychkova and Alain Wegmann or iteratively, forming a design spiral, similarly to [4]. Within this spiral, an ini- tial service model evolves in response to the changing business requirements and also makes these requirements evolve by revealing flaws and inconsistencies in them. This way, the partial specification is validated, verified and improved. We illustrate our method with the example of an “Order Creation” service, specified for G´en´erale Ressorts SA - the Swiss manufacturer of watch springs. This example is based on the consulting project we conducted with this company. The SEAM model for ”Order Creation” and the transformation of this model to Alloy remains beyond the scope of this article. This paper (a) explores the power of Alloy beyond the technical domain (b) investigates how invariants can be used as a linking mechanism between business and technical service perspectives for improved business/IT alignment. The remainder of this paper is organized as follows: In Section 2 we explain our motivation and discuss the related works. In Section 3, we present the Alloy language and discusses the role of invariants in service design. In Section 4, we introduce our method for service design. In Section 5, we illustrate this method on the case study. In Section 6, we present our conclusions. 2 Motivation and Related Work Since the first methods dealing with enterprise modeling (EM) that emerged in 1970s, a multitude of enterprise modeling approaches have been developed. e3Value [5] provides an ontology to conceptualize and visualize eBusiness idea and to be able to do an analysis and profitability assessment of the eBusiness model for all parties involved. The i* framework [6] focuses on modeling prop- erties such as goals, beliefs, abilities, commitments; and on modeling strategic relationships. Enterprise Knowledge Development (EKD) [7] is a multi-model, participatory EM approach that involves a model for conceptual structures, and interlinked sub-models for goals, actors, business rules, business processes and requirements to be stated. Business Motivation Model (BMM) [8] models several concepts from goals, down to processes and technologies. The methodology that focuses more on business processes is Dynamic Essential Modeling of Organiza- tions (DEMO) [9], which models, (re)designs and (re)engineers organizations. SEAM [2] integrates the main principles of the well known EM methods by proposing three different types of models: SAR (business value between different stakeholders-similar to e3Value), goal-belief (goals and beliefs of the stakeholders and their relation-similar to i*), and behavior model (services and processes that implement them-similar to DEMO). In this work, we extend SEAM with the spiral design process that allows simulation and validation of SEAM models in the early stage of the design. This way, the examples of the partial specification could help the designer to realize what constraints are missing in the model. i* uses the similar approach the Formal Tropos [10] to do the model-checking of the models defined in i*. However, it focuses on the agent properties such as goals, beliefs and abilities. The Role of Invariants 3 Invariants have been used both in the business and technical world: to repre- sent and check constraints [11], to model business rules [12], process invariants related to beliefs [13] etc. In requirements engineering, KAOS methodology uses invariants for object specification, domain properties specification, and indirectly for goal specification [14]. In this work, we use invariants as a a pivotal concept in improving business/IT alignment and in supporting the co-evolution of technical and business specifications. Our method is based on Alloy, a lightweight formal specification language. large1. The area of Alloy application is very To the best of our knowledge, all cur- rent Alloy applications in the domain of EM target technical specialists. In this paper, we present an agile EM method where Alloy diagrams serve as a means for communicating and evaluating both business and technical design decisions. Within our approach, the role of Alloy diagrams is two-fold: They provide an instant visual feedback to a designer that suggest new constraints to be added; They represent design artifacts for validation and can drive improvements of both technical and business specifications (like UML, BPMN). 3 Foundations 3.1 Alloy Alloy [3] is a declarative specification language for expressing complex structural constraints and behavior based on first-order logic. The Alloy Analyzer [15] is a tool for the automated analysis of models writ- ten in the Alloy language. Given a logical formula and a data structure that defines the value domain for this formula, the Alloy Analyzer decides whether this formula is satisfiable. Mechanically, the Alloy Analyzer tries to find a model instance - a binding of the variables to values making the formula true [16]. Alloy reusable expressions (i.e., functions) and constraints (i.e., facts, predi- cates and assertions) [17] can be used to reason about data structures. Fact is a model invariant: a constraint that holds permanently. Predicate is a constraint that holds in a specific context or for a specific part of the model only. It can be seen as a contextual invariant. Assertion is a property that the designer believes should be implied from the model; he can check if it can be deduced from the other (permanent or contextual) constraints. In our design process we use signatures, facts and predicates, first for partial and then for refined service specification; we use assertions in order to validate desired properties of our model. 3.2 The Role of Invariants In computer science, an invariant is a condition that must hold during the execu- tion of a program. Along these lines, in our design process, an invariant defines a 1 http://alloy.mit.edu/alloy/applications.html 4 Biljana Baji(cid:19)c-Bizumi(cid:19)c, Irina Rychkova and Alain Wegmann condition that must hold for all model instances that result from simulation. We define the role of invariants in our design process as follows: First, they imple- ment the constraints required by business specification. For example, “The order can be placed for the existing parts only”; Second, they enable the designer to efficiently manage the model complexity by assuming that some of its proper- ties always hold during an execution. For example, “To simplify the model, let’s consider that the part’s id provided by a customer is always correct ” (i.e., exists in the database). These roles correspond to the business and technical perspective. Therefore, in this approach we use them as a linking mechanism between these two worlds to restrict a model prohibiting some (invalid) instances identified during simulation (not necessarily covered by the explicit business specification). 4 Service Design Spiral We introduce the five activities of our design approach, which can be performed sequentially or iteratively, forming the loops of a spiral as shown in Fig. 1a. 4.1 Model Design We define a partial model of a service in Alloy: we specify its data structures, the initial predicate and make initial assumptions about our model defining model invariants. These invariants replace the properties required by the business specification and are used to control the model complexity. 4.2 Model Simulation We simulate our partial model by using the Alloy Analyzer tool. Technically, a partial model written in Alloy represents a logical formula; model simulation means searching for a model instance that satisfies this formula. If it exists, it indicates that the formula is consistent (i.e., no contradictory constraints are specified). In our design process, we first check our model for consistency, and then test if it corresponds to the requirements and if there are some anomalies by studying the random set of model instances generated by Alloy Analyzer. 4.3 Model Analysis and Anomaly Resolution There are two types of anomalies that can be observed: anomalies due to under- specification and anomalies due to overspecification. Underspeci(cid:12)cation means that some model instances that are prohibited by the specification still appear during the simulation. In this case, we restrict the model by adding new invariants. Overspeci(cid:12)cation means the opposite: some expected model instances are not observed during the simulation. The modeler then has to relax invariant, i.e. to replace an Alloy fact “X always holds” with an Alloy predicate “X holds when. . . ” that can be activated in specific parts of the model only. The Role of Invariants 5 4.4 Model Validation We make assertions about our model in order to test some desirable properties and business rules. Alloy Analyzer validates our assertion by searching for a counterexample: a model instance for which our assertion does not hold. If no such counterexample is found, then our assertion is valid within a given value domain. In the opposite case, the model has to be revised. 4.5 Model Re(cid:12)nement In this activity, we implement new business requirements and extend our par- tial model. We introduce new elements in a data structure and specify new constraints. Refinement increases both the model complexity and its level of de- tails, bringing it closer to its business specification. The complete design process is illustrated in Fig. 1a. (a) Service design process spiral. (b) Design process for \"Order Creation\" service. Fig. 1: Spiral design process 5 \"Order Creation\" Design: The Case Study In this section, we present an example of using our method for enterprise design, which involves both a technical and a business expert working together to define a complete service specification while maintaining business/IT alignment. We implement our service design process spiral step by step (Figure 1b). 5.1 Case Study: G(cid:19)en(cid:19)erale Ressorts G´en´erale Ressorts SA is the market leader in watch barrel springs and a first- class manufacturer of tension springs, coil springs, shaped springs and industry 6 Biljana Baji(cid:19)c-Bizumi(cid:19)c, Irina Rychkova and Alain Wegmann components [18]. We illustrate our process by applying it to the design of the “Order Creation” service for G´en´erale Ressorts SA (GR). “Order Creation” is a part of an “Order Processing”; it is followed by “Order Delivery” and “Account- ing” (order-to-cash cycle). An overview of “Order Creation” service is: “The company gets a request from a customer (OrderRequest-with customer name, address, partID and part- Info2) for manufacturing a specific watch component identified by its ID (par- tID). A company agent (OrderEntryPerson) identifies the customer and the part to be manufactured by entering the customer’s name and the partID into the enterprise information system (EIS ). The process terminates with a creation and confirmation of a customer order (OrderCon(cid:12)rmed ) in the EIS.” We specify the following business rules for our process: – BR1: The created order must include the complete part specification (to be used for the order fulfillment) and the complete customer details (to be used for product delivery); – BR2: The order can be confirmed only when the customer exists in the system; – BR3: The order can be placed for the existing parts only; – BR4: The company has to guarantee ”no faulty delivery”. 5.2 Order Creation: Model Design The data structure for the “Order Creation” service is modeled using Alloy signatures: f abstract sig GR f orderConfirmedSet: set Order, one sig GR_pre extends GR orderDeliveredSet: set Order, orderRequest: one OrderRequest g orderPaidSet: set Order, fg partSet: set Part, one sig GR_post extends GR customerSet: set Customer g Alloy signatures (sig) can be abstract or concrete, can have explicit cardi- nalities (e.g., only one OrderRequest object can be treated by the service at a time), and can contain one or multiple fields (as classes and attributes in object- oriented (OO) languages). We can also define additional constraints on the initial data structure with the invariants. We express the behavior in terms of a state transition: we define a pre-state that describes the state of a system before the service has been performed and the post-state that describes the condition that must hold for the system upon the service termination - the service result. Note, that following the declarative modeling paradigm, we do not specify how the service will change the system’s state. We model the “Order Creation” service as a corresponding predicate in Alloy. 2 We put in italic the names that will appear in the Alloy models. The Role of Invariants 7 GR_post)f 1.pred orderCreation(aGR_pre:one GR_pre,aGR_post:one 2. one aCustomer: Customer | one aPart: Part | one aOrderConfirmed: OrderConfirmed | 3. 4. aPart=findPartByPartID[aGR_pre.orderRequest.requestedPartID,aGR_pre.partSet] and 5. aCustomer= findCustomerByName[aGR_pre.orderRequest.name,aGR_pre.customerSet] and 6. aOrderConfirmed=createOrderConfirmed[aPart,aCustomer] and aGR_post.orderConfirmed= aGR_post.orderConfirmedSet=aOrderConfirmed+aGR_pre.orderConfirmedSetg 7. aOrderConfirmed and This predicate shows a transition between GR pre and GR post states; these states are indicated as predicate parameters (line 1). In this predicate, the vari- ables are declared (line 2), the customer and the part are found in the set (lines 4-5) and the order is created (line 6) and added to the set (line 7), as described in the case study. 5.3 Order Creation: Model Simulation and Anomaly Resolution We attempt to simulate this model in Alloy Analyzer: to check our model for consistency and to test the random set of model instances to check for overspec- ification and underspecification anomalies. Example 1. \\Missing Customer\" anomaly Fig. 2 illustrates an anomaly in our model behavior: In a pre-state we have Customer0, in a post-state we have Customer1. As we show exactly one execution of the service “Order Creation”, we expect both the customerSet and the partSet to remain the same in pre- and post-state. However, the generated instance suggests the opposite. NOTE: the inputs and outputs in our diagrams (e.g,, OrderRequest and Or- derCon(cid:12)rmed in Fig. 2) are depicted with black rectangles; customer data (Cus- tomer, Name, Address) and part data (Part, PartID, PartInfo) are depicted with parallelograms and diamonds, respectively. We depict the pre-state (prior to the order creation service execution) and post-state (upon the service termination) of the GR company with “houses” and the corresponding labels: GR pre, GR post. Fig. 2: Anomaly due to Underspecification: “Missing Customer” This anomaly indicates that some constraints, which should prevent the cus- tomer set and the part set from changing during the service execution, have to be specified. Thus, it is an anomaly due to the underspeci(cid:12)ed model. 8 Biljana Baji(cid:19)c-Bizumi(cid:19)c, Irina Rychkova and Alain Wegmann In fact, the declarative specification principles oblige us to explicitly state the elements that must remain “unchanged” during the state transition. Therefore, we need to add an invariant that states that the customerSet in post-state is the same as the customerSet in pre-state. The same applies to part set. customerSetSamef GR_pre.customerSetg fact GR_post.customerSet = In order to validate that we have resolved the “Missing Customer” anomaly, we create an Alloy assertion that claims that for all Order Creation executions (i.e., model instances), the customer set will remain the same in pre- and post- states of GR. checkf customerPrePostSame: all aGR_pre:GR_pre,aGR_post:GR_post | aGR_post.customerSet=aGR_pre.customerSetg orderCreation[aGR_pre, aGR_post] => Checking this assertion, we find no counterexamples. Executing ‘‘Check customerPrePostSame’’ Solver=sat4j Bitwidth=4 MaxSeq=4 SkolemDepth=1 Symmetry=20 1014 vars. 109 primary vars. 1750 clauses. 32ms. No counterexample found. Assertion may be valid. 12ms. This confirms the assertion validity (for a given model scope). We repeat the simulation until all anomalies are resolved (“design loop” in Fig. 1). 5.4 Order Creation: Model Validation and Anomaly Resolution We check the validity of each of the business rules from Section 5.1, using Alloy assertions. We show an example of BR4 validation (”no faulty delivery”). Example 2. \\Delivery to the Wrong Address\" anomaly As OrderCon- (cid:12)rmed is used for delivery, to ensure “no faulty deliveries” (BR4), we check that the customer and part data in the confirmed order are exactly the same as in the requested order. The assertion “orderConfirmedCorrect” is defined to validate this BR: f orderConfirmedCorrect: check all aGR_pre:GR_pre,aGR_post:GR_post,oReq:OrderRequest, oCurrent:CurrentOrderConfirmed | orderCreation[aGR_pre, aGR_post] => (oCurrent.ocCustomer.name=oReq.name and oCurrent.ocCustomer.address=oReq.address and oCurrent.ocPart.partInfo=oReq.partInfo)g oCurrent.ocPart.partID=oReq.requestedPartID and When we run the assertion, we obtain the counterexamples. Fig. 3 shows an example of the incorrect delivery: the order is created on the correct customer’s name, but the delivery address associated with this name does not correspond to the address provided in the OrderRequest. Therefore, the part can be delivered to the wrong address. The anomaly observed is due to model underspeci(cid:12)cation. In order to resolve the detected anomaly, we add a new invariant ”noOldAd- dress” that states that we cannot have a customer in the system with the name given in the requested order, but with an old/invalid address and vice versa: noOldAddressfall c.address=OrderRequest.address<=>c.name=OrderRequest.nameg fact c:Customer | If we check now the assertion “orderConfirmedCorrect”, we get the result “No counterexample found. Assertion may be valid.”, meaning that this assertion holds in a given domain, and all orders will be delivered to the correct customers to the correct address. The Role of Invariants 9 Fig. 3: Anomaly due to Underspecification: “Delivery to the Wrong Address” We continue “debugging” the model by running the simulations, checking if we have introduced some new unwilling behavior. We repeat the process for other BRs. After validating all BRs and finding no anomalies, we conclude that the designed model meets its business requirements at a given level of details. 5.5 Order Creation: Model Re(cid:12)nement At the refinement, we can add new data structures and behavior to our model. Then, we resolve all added anomalies, if any, in the “design loop”. The next step is to check if the BRs still hold by repeating the “BR validation loop” until all the BRs hold. The refinement specifies a new iteration on the spiral (Fig. 1). The designer can continue refining the model until the desired level of detail is achieved. The design process we propose will ensure that, upon each iteration, the model remains consistent and has no anomalies. Refinement of “Order Creation” service will not be considered in this paper. The resulting design process of ”Order Creation” (Fig. 1b) represents an instance of the spiral process illustrated in Fig. 1a. 6 Conclusion We have presented a lightweight, interactive and visual method for service design that supports the co-evolution of technical and business service specifications of an enterprise. In particular, we have explored the power of Alloy formal method beyond the technical domain and how it can be used as a toolbox for both technical and business specialists. The evolution of service model in Alloy can be seen as an iterative introduc- tion and modification of logical invariants. Invariants represent the assumptions about business or technical properties of a modeled service and, consequently, play the role of a linking mechanism between business and technical perspectives. 10 Biljana Baji(cid:19)c-Bizumi(cid:19)c, Irina Rychkova and Alain Wegmann This work has illustrated how Alloy can be used as a design environment for both technical and business specialists. For now, we expect that the Alloy diagrams are interpreted and analyzed by designers and business analysts. These specialists trace the observed scenarios back to the specification for its improve- ment. Automated interpretation and traceability between scenarios generated by Alloy and their specifications (business requirements, business rules, etc) is a subject of our future research.",
  "2013-KPR": "1 Introduction Davenport [5] [6] defines case management process as a process that is not predefined or repeatable, but instead, depends on its evolving circumstances and decisions case. regarding a particular situation, a Case management processes scenarios form dynamically, at run time, and cannot be modeled, managed or analyzed following the traditional BPM approaches [22]. This idea paper builds up on our recent work [19], where we define two forms of 1st agility, leading to more dynamic context-aware business process. The form of process agility is defined as a capacity to handle unpredictable sequences of system events, which results in a dynamically defined order for process activity invocations. 2nd selecting a right action at the right moment, The form of process agility consists in and with respect to the current situation. We define it as the ability to monitor and the process context manage and to dynamically select and/or alter the execution scenario accordingly. We argue that the second form of process agility is essential for efficient case management. This agility depends heavily on the capability of supporting systems to deal systematically with dynamic process context. Unfortunately, current approaches lack appropriate formalism and mechanisms for context management. In this paper, we explore the role of context information in the agile case management and propose an extensible meta-model and architecture for representing, capturing and exploring this information in a dynamic and systematic way. We illustrate our findings on the example of crisis management process as defined by the Emergency Plan Specialized on Floods (EPSF) of Hauts-de-Seine [15]. Y.T. Demey and H. Panetto (Eds.): OTM 2013 Workshops, LNCS 8186, pp. 144–154, 2013. © Springer-Verlag Berlin Heidelberg 2013 Dynamic Context Modeling for Agile Case Management 145 This paper is organized as follows: in Section 2 we discuss the Adaptive Case Management paradigm as opposed to traditional BPM and introduce our example. In Section 3, we define the dynamic context model and illustrate how this model can be instantiated for the crisis management process for process agility. In particular, we focus on the dynamic aspect of the context modeling and discuss the added value of the context model to the case management. In Section 4, we compare our proposals with related works before concluding in Section 5. 2 Case Management Process and Adaptive Case Management The Case Management Process Modeling (CMPM) Request For Proposal released by OMG [12] expresses the practitioners' demand in the case management solutions. \"A coordinative and goal-oriented discipline, to OMG defines case management as handle cases from opening to closure, interactively between persons involved with the subject of the case and a case manager or case team\". Case management processes \"...licensing and permitting in (CMP) have multiple applications, including government, insurance application and claim processing in insurance, patient care and medical diagnosis in health care...\" [12]. The main resource of a CMP is knowledge obtained as a result of communication between multiple actors/users. This knowledge is used for making decisions during the case handling. Business Process Management (BPM) and Adaptive Case Management (ACM) demonstrate conceptually different views on the system design. Process-centered view adapted by BPM implies that the data emerges and evolves within a process according to a predefined control flow (Fig. 1-a), similarly to a product evolving on a conveyor belt. a) BPM – activity centered view on b) ACM – data – centered view on processes processes Fig. 1. BPM vs. ACM systems, from [22] One of the major challenges identified by both practitioners and researchers in the ACM field is the attempts to deal with case management process in the industry the same way as with regular business process - i.e. by applying a process-centered view. In this work, we implement the data-centered view (Fig. 1-b) that is proposed by the 146 M. Kirsch-Pinheiro and I. Rychkova Adaptive Case Management (ACM) practitioners [22]. This view implies that the process shall be adapted at run time, according to evolution of case circumstances and case-related data. This view suits to nondeterministic, knowledge-intensive processes like case management processes [16]. 2.1 Case Management Process Example: Crisis Management in Case of Floods Crisis management process is a typical example of case management: it demands interaction between multiple otherwise independent actors (e.g. government, fire brigades, police, public transport, healthcare, electricity and water supplying services, road maintenance etc.). This process is driven by evolved information about the case rather then predefined sequence of activities and, it is thus, suitable for the data- centered paradigm (Fig. 1-b). In the example below, we consider a crisis management process triggered in case flood of flood, in the Hauts-de-Seine department of France [15]. A is an overflow of water that submerges land that is normally dry. It is happening, for example, due to an increase in the flow of a river provoked by significant rainfalls. The risk of a \"major flood\" is the main natural risk in the Ile-de-France region, particularly during the Paris1 winter period from November to March. Cities like are confronted to this risk, and, in case of flood, important damages can be expected, affecting thousand of people. In the Hauts-de-Seine department [15], the risk of flood is considered as particularly important since 1910. The EPSF (Emergency Plan Specialized on Floods) is triggered immediately when the water level rises until 5.5m at the Austerlitz Bridge and will keep rising according to forecasts. Due to its high dynamic nature, this process cannot be handled by a workflow-based approach. Indeed, activities during crisis management are not necessarily predictable and depend on numerous factors that can dynamically change. We list just a few of these factors: watercourse levels, weather conditions and forecasts, electrical outages, traffic conditions, actors’ location, equipment availability, etc. For example, depending on the water level, the crisis management process may require specific traffic control and deviation, partial or complete disabling of public transport lines (SNCF Paris Rive Gauche, RER C, RATP), blocking the highways and principal roads (A86, A14, N14, etc.). The towns affected by the flood may require regular external drinking water supply, in more severe cases - evacuation and temporary accommodation for people, healthcare and childcare facilities. Moreover, the crisis management shall be assured under condition of possible overload or instabilities in telecommunication network, inaccessible or blocked due to heavy traffic roads etc. For example, a traffic jam can put at risk the evacuation of people or deployment of other emergency services. These situations require alternative scenarios such as traffic deviation or preparation of evacuation by air etc. Such context. (dynamically changing) information defines the crisis management process 1 See http://cartorisque.prim.net/dpt/75/75_ip.html Dynamic Context Modeling for Agile Case Management 147 More formally, context can be defined as any information that can be used to characterize the situation of an entity (a person, place or object) that is considered relevant to the interaction between a user and an application [7]. We claim that the capacity to timely observe and measure the context information in a systematic way, in order to select and assemble process activities at run time is indispensible for adaptive case management and for our example of crisis management in particular. Thus, we adapt the data-centered paradigm from Fig. 1-b contextual data. by including into its core the In the next section, we introduce the concept of dynamic context modeling and illustrate this concept on the example of crisis management process presented above. 3 Dynamic Context Modeling 3.1 Context Meta-model The way context information can be exploited for business process flexibility depends on what information is observed and how it is represented. According to Najar et al. [11], the formalism chosen for representing context model determines the reasoning methods that can be used to perform system adaptation to the observed context. A context model (i) ensures the definition of independent adaptation processes and (ii) isolates this process from context acquiring techniques. The same applies to context- aware business process or case management. We claim that the process context information should be acquired, modeled and formally analyzed at run time in order to adapt process execution and to ensure its flexibility. Several context models have been proposed in the literature [11] [1]. Even if they vary in the adopted formalism (key-values, object-oriented, ontologies, etc.) and in the represented elements, we can generalize some common terms. First of all, most of context models consider a given subject that is observed, typically the user. For et al. et al. instance, Reichle [17] and Najar [11] consider both that a given “entity” (the user, a device, etc.) is observed. In other models, such as [7], the subject is implicit, since the user is considered as the main observed thing. This subject plays a central role on context modeling, as pointed out by [3] [4], since it is precisely the context of this subject that is currently been observed. Everything we observe is et al. related to this subject. Around it, several elements can be considered. Reichle et al. [17] call these observed elements “scope”, while Kirsch-Pinheiro [9] call these “context elements”. In both cases, it corresponds to what we really observe from the subject: its location (for a user), the available memory (for a device), etc. When values observing such context elements, we obtain corresponding to their status on given moment and that will probably evolve over the time. For instance, by observing context element subject values the ‘location’ for a ‘user’, we may obtain for latitude and longitude, corresponding to the current user’s location. Besides, some models et al. associate meta-data describing these values. For instance, Reichle [17] propose (e.g. to describe which representation is used for an observed value a location can be et described using latitude and longitude pair or through a postal address). Vanrompay al. [23] consider representing as meta-data a “certainty measure” indicating the reliability of the observed values. 148 M. Kirsch-Pinheiro and I. Rychkova Based on these common terms we identified in the literature, we define a common meta-model presented in Fig. 3. In this meta-model, we consider context as a set of context elements subject that are observed for a given (e.g. an actor, a device, a resource, etc.). Each subject can be associated with multiple context elements values (location, status, etc.); for each context element, we observe that can dynamically change and that can be described by meta-data. The proposed meta-model must be instantiated in an appropriate model, according to its application domain. This means to transpose and to affine elements from the meta- model in a precise model. Such model can use different formalisms, but ontologies appear as the most interesting to represent context elements [11]. They provide ways to semantically describe these elements and their relationships, as well powerful reasoning et al. “ontological mechanisms, notably inference rules. According to Bettini [1], reasoning can be executed for inferring new context information based on the defined classes and properties, and on the individual objects retrieved from sensors and other context sources”. The meta-model and an ontology model form a complementary approach, allowing a better understanding of the context modeling. Fig. 2. Context meta-model considering context as a set of context elements Thus, subject and context elements can be semantically described using ontologies. Choosing what elements will be represent depends on the application domain, in our case, the activity domain related to our case management. For instance, a context ontology for the crisis management process will describe subjects such as the different teams and actors’ roles, and context elements such as the location, the availability of a given equipment, etc. Based on such context ontology, we propose to events represent as logical constraints over context concepts and observed values. s t We formalize the context of a subject in a time as follows: Context(s,t) = { Element(s, ce) }, Element(s,ce) ce where represents the observed value of the context element for the s. subject For the crisis management, we can monitor different actors (subjects) involved into the process (e.g. police brigades, fire brigades, evacuation teams, etc.). The main parameters (context elements) to observe are location and resources available. Context (team1, t) = {Element (team1, #location), Element (team1, #vehicle), Element (team1, #agent), Element (team1, #firstAidKit)} Dynamic Context Modeling for Agile Case Management 149 Along those lines, we can observe weather conditions (subject), with temperature, humidity or rainfall prevision (context elements); general traffic conditions (subject) with the current deviations, traffic jams (context elements); main healthcare and childcare facilities (subjects) with their heating, water, electricity supply, accessibility by the roads (contextual elements indicating the need of evacuation), etc. 3.2 Marrying the Dynamic Context Model with the Agile Process Model Defining and managing an exhaustive context model along with the process model seem to be a challenging task that would potentially raise the complexity and compromise the usability of the overall model. With an appropriate modeling formalism, however, this complexity can be substantially reduced. In [19], we model a business process as a finite state machine (FSM) [14], where each state represents a process situation at a given time and state transitions define the possible process scenarios. The triggering events specify the underlying process semantics, i.e. conditions for state transitions. Fig. 3 illustrates the FSM for our example: here process states may evolve from Flood Vigilance (that corresponds to 2.5m water level at the Austerlitz Bridge) to Flood Alert (3.2m) and to Execution of Emergency plan (5.5m). We assume (although this is not documented in [15]) that Federal Alert state can be triggered in case the emergency plan execution is insufficient. Also, at any time, the system can get back to normal state. This FSM is an abstract representation of the process that can be further detailed by refining states and/or adding component state machines showing how separate case-related elements will be handled (road blocks, evacuation process, etc) as described in [20]. According to EPSF [15], at each process state, various activities must/can be executed in order to protect people and goods and to reduce the consequences before, during and after a flood (e.g. the public transport suppression, preparation and executing the evacuation, road blocking, provisioning water, electricity etc.). Due to natural evolution of the crisis situation (e.g. the water level keeps rising) or other conditions (e.g. not enough people/equipment, electricity outage, no road access etc.) execution of some of these activities becomes impossible and alternative sets of actions need to be executed in order to fulfill the crisis management objectives. subject contextual Each process state in Fig. 3 can be defined with a (set of) and its elements context events to observe. Emergent conditions can be modeled as and expressed using logical conditions on observed values of contextual elements. For example the following condition triggers a hospital evacuation: Element(#hospital, #heating) = “out of order” OR Element(#hospital, #electricity)=”out of order” OR Element(#hospital, #access)=”not available” Thus, context events are expressed referring elements from context ontology. This way, event definition can take advantage of reasoning capabilities offered by ontologies. Besides, a context query language, such as [17], allows the expression of rich context conditions. This is particularly important since context information is naturally imprecise and often incomplete. Process states or events can then be defined in terms of context elements and acceptable interval or sets of values. 150 M. Kirsch-Pinheiro aand I. Rychkova Therefore, context inforrmation plays the following roles: (i) it is a trigger for process state transitions; (iii) it is a part of process state definition; and (iii) it iis a boundary condition for execcution one or another process activity. Fig. 3. FSM for Crisis management process in case of Flood 3.3 Towards Context-AAware Case Management In [19], we present architeecture for CAPE – Context-aware Agile business Proccess Engine – that defines the foollowing elements for agile process management (Fig. 4)): - Dynamic context moonitor - Activity repository ((Process specification) - Navigation managerr. Here the Activity reposittory provides a set of activities that can (but not necessaarily will) be executed during tthe process; the Navigation manager provides a conteext- activities) based guidance, offering ann appropriate activity (or a list of alternative to execute from the repositoory; and the Context monitor is in charge of observving context elements during pprocess execution, in order to enable this context-baased plugins, the guiding. Similar to [13], it recognizes context elements using which feed inforrmation about a given context element from a subject. EEach monitor with dynamic plugin observes a given context element. It keeps Context Monitor updaated concerning changes in obseerved context values. Such values dynamically observedd by context plugins define the ccurrent position of the process in its state space. process definition, we consider that context elemeents To support agility on observed for a given proceess might vary during the process executions. As a ressult, are new context elements to be observed and new respective contextual events dynamically added to the mmodel. In practice, the Contexxt monitor can be dynamically extended by adding nnew the plugins or sensors for obserrving new context elements and subjects. Fig. 4 shows CAPE model for the crisiss management process and illustrates the dynamic conttext management: a new sensorr measuring rainfall level is added through a new conttext plugin (on the left side off the figure), extending the process definition with nnew events and, possibly, states and triggering conditions. Dynamic Context Modeling for Agile Case Management 151 Fig. 4. Context Monitor organization in CAPE architecture 4 Related Work The notion of context is not totally new on the BPM field. Other researches [18] [10] [21] have already pointed out the use of context information on process definition. Roseman et al. [18], for instance, consider that external context elements may influence business process (e.g. weather influencing a call center process in an insurance company). They propose to incorporate such elements into business process modeling. Saidini et al. [21] also consider context on business process definition, particularly on the roles played by each actor. They assume that role affectation can be influenced by context information. Unfortunately, in BPM field, context information is not always modeled appropriately. Quite often works lack of formalisms in representing context concepts and properly managing them. Besides, context models, when they are present, are not general enough, consider only a limited set of context information and focus manly on workflow based process needs. Indeed, the way context information can be exploited for business process flexibility depends on what information is observed and how it is represented. The meta-model we proposed here allows dynamic modeling of context information, supplying the high level formalization necessary for process flexibility. Identifying context information that can affect a business process execution remains problematic. Similar to [18] [11], we consider context information on concentric levels, from internal context directly related to process actors and activities till external environment context. In this sense, context can be seen as a recursive notion, a ‘local’ context referencing a more global one [3]. Such concentric and 152 M. Kirsch-Pinheiro and I. Rychkova recursive vision of context information leads us to represent internal process events and external context events indistinctly. In this way, context information becomes naturally part of the process definition and can be considered under multiple levels. Nevertheless, it is worth noting that we cannot enumerate which aspects of all situations are important, as these will change from situation to situation [7]. In other terms, it is impossible to exhaustively describe all context information needed in a general way. Model extensibility becomes then a key element for successfully representing and managing context information. Such extensibility depends not only on the context model and its capabilities to be extended with new concepts, but also in the capability of observing and acquiring such new concepts during execution time. Even if some works [18][21] have considered context information on business process definition, at design time, they rarely consider context information at run time, during process execution. Often no architecture is proposed in order to dynamically acquire and take into account context information during process execution. CAPE architecture proposes to overcome this issue by adopting an extensible plugin approach, in which new context elements can be easily observed by connecting the appropriate context plugin. Finally, a context-aware business process does not necessarily means an agile business process. Although context information may bring an interesting contribution for process flexibility, it does not guarantee that the process will be able to react to unexpected situations. Indeed, works such as [18] still describes process as a predefined and explicit sequence of activities, making it difficult to respond to unpredictable situations. We argue that, in order to support process agility, it is necessary to think business process differently, in terms of events and states instead of in terms of activities. 5 Conclusion Crisis management process is an example of a case management process, with unpredictable scenarios. Activities are chosen according to actors’ expertise, previous experiences and to multiple input events moving the situation from a state to another. It is a typical example of process that cannot be handled in a satisfactory way using traditional workflow-based approaches. A more dynamic approach is necessary. For better efficiency, especially in the situation where the time matters and complicated decision making may cost even lives, any automated guidance becomes important. Our approach represents the process as FSM, defined by its multiple states and transition events. This representation allows integrating the contextual events and contextual parameters into process definition: both process events and contextual events are handled uniformly in order to provide an automated guidance and to prescribe/suggest an activity that suits best with respect to the process objectives. In this paper, we went one step forward in modeling context information for business process. We proposed a context meta-model allowing a high level formalization of the context information, which can be combined with powerful query and ontology based reasoning mechanisms, for more process flexibility. We are Dynamic Context Modeling for Agile Case Management 153 currently specifying a first implementation for CAPE architecture, including Context Monitor elements. A first prototype is expected soon. The next step of this research is definition and implementation of techniques for automated user guidance. Formal Concept Analysis (FCA) and Galois lattices [2] [8] is one of the techniques we consider. In [19], we illustrate how Galois lattices can be used to classify/organize the case-related information providing a user with suggestions about which activity to execute at a particular situation. Along those lines, the use of Galois lattices for context analysis provides flexible guidance to end- users at run time and supports them with an expertise required for the process handling.",
  "2013-R": "I. INTRODUCTION\nConsider the following processes: crime investigation, mortgage processing, patience care, design of a new house. Being\ncompletely different by their objective, they have one important characteristic in common: unpredictability. Each of\nthese processes unfold according to a particular case and\nemerging knowledge about this case rather than according to\na predefined scenario.\nParticular importance of case management processes (CMP)\nhas been recognized since early 90x [1] [2]. Until now,\nhowever, case management processes remain largely ”pen and\npaper”.\nEfficient case management in industry is hampered by\nattempts to deal with case management process the same way\nas with regular business process. Development of the specific\napproaches for CMP modeling and analysis is therefore an\nimportant endeavor that can improve the case management\npractice.\nLamport defines process as a sequence of events occurring\nin system [3], where each event is triggered by an (internal\nor external) action. According to this definition, a business\nprocess can be seen as a sequence of events triggered by the\nactivities of (internal or external) business process actors.\nThe existing methods for business process modeling and\nanalysis (e.g. BPMN - BPEL bundle) follow the imperative\nprinciples, implying that the order of events occurring during\nthe process execution is predefined. For case management\nprocesses, however, it is not true [4]. In this work, we shift\nthis traditional modeling paradigm and exploit the declarative\nprinciples for CMP modeling and analysis. We model CMP\nas a state-transition system that allows us to handle process\nevents whose order of occurrence is undetermined.\nThe objective of this paper is to provide a mathematical\nmodel and a comprehensible formalism for reasoning about\nthe meaning - the semantics - of case management process.\nIn particular, we focus on (a) how a finite state machine\n(FSM) [5] [6] abstract model can be used to provide the\noperational semantics for CMP, (b) how CMP model can\nbe formally specified in Alloy [7] and analyzed with Alloy\nAnalyzer [8] and (c) how the results of this formal analysis\ncan be interpreted and used by business specialists for process\nimprovement.\nWe illustrate the proposed operational semantics on the\nexample of Mortgage Approval process. First, we define a\nmortgage approval as a FSM: we specify the states of a\nmortgage case and the transitions between these states. The\nstate transitions are triggered by the corresponding mortgage\nprocessing activities (e.g. application completion, validation of\napplicant’s information, property appraisal, etc.). Their order\nin the model, however, remains undetermined. This FSM,\ndescribes how the mortgage case (application file) evolves\nfrom its submission to approval or rejection.\nOnce the state and state transitions are defined, we formalize\nthe semantics of this FSM in Alloy and use the Alloy Analyzer\ntool [8] for model simulation and validation. Model simulation\nprovides us with an instant visual feedback in a form of\ndiagrams capturing the FSM execution scenarios. We use\nAlloy model checking for more specific reasoning about model\nproperties: for example, we validate if our CMP model meets\nits business requirements.\nUntil now, Alloy specification language was successfully\napplied to design and verification of software and hardware\nsystems. In this work, we position Alloy and Alloy Analyzer\nas a toolbox for both technical and business specialists. Along\nwith UML, BPMN and other conventional diagrams, Alloy\ndiagrams represent design artifacts and can be studied, referred\nto, and serve a mean for communicating and evaluating both\nbusiness and technical design decisions.\nOnce the model validity at a given level of detail is checked\n- the model can be refined. This iterative approach allows us\nto efficiently manage the model complexity.\nThe operational semantics defined in this work is a first\nstep to using formal methods; it paves the road to automated\ndesign, validation and verification of CMP.\nThe reminder of this paper is organized as follows: In\nSection II, we provide a definition for case management\nprocess, discuss the challenges related to case management\nand introduce our example - the Mortgage Approval Process;\nIn Section III, we introduce a finite state machine (FSM)\nabstraction for CMP modeling: first, we provide a mathematical model for FSM as defined by Plotkin [5], then we apply\nthis model to our example. In Section IV, we formalize the\nFSM-based specification of the Mortgage Approval process in\nAlloy. In Section V, we illustrate how the CMP model can be\nsimulated, analyzed and, eventually, improved using the Alloy\nAnalyzer tool. We show the scenarios of mortgage approval\ngenerated by Alloy Analyzer and demonstrate how business\nproperties of the Mortgage Approval process can be formally\nvalidated. In Section VI, we discuss the related works; Section\nVII presents our conclusions.\nII. CASE MANAGEMENT PROCESS AND ADAPTIVE CASE\nMANAGEMENT\nDavenport [1], [2] defines case management process as\na process that is not predefined or repeatable, but instead,\ndepends on its evolving circumstances and decisions regarding\na particular situation, a case. He discusses the need in specific\napproaches to handle such processes. The Case Management\nProcess Modeling (CMPM) Request For Proposal released\nby OMG on September 2009 [9] expresses the practitioners’\ndemand in the case management solutions. OMG defines case\nmanagement as ”A coordinative and goal-oriented discipline,\nto handle cases from opening to closure, interactively between\npersons involved with the subject of the case and a case manager or case team”. Case management processes (CMP) have\nmultiple applications, including ”...licensing and permitting\nin government, insurance application and claim processing in\ninsurance, patient care and medical diagnosis in health care,\nmortgage processing in banking...” [9]. The main resource of\na CMP is knowledge obtained as a result of communication\nbetween multiple actors/users. This knowledge is used for\nmaking decisions during the case handling.\nA. Adaptive Case Management (ACM) vs. Business Process\nManagement (BPM)\nBusiness Process Management (BPM) and Adaptive Case\nManagement (ACM) demonstrate conceptually different views\non the system design. Process-centered view adapted by\nBusiness Process Management (BPM) implies that the data\nemerges and evolves within a process according to a predefined control flow (Fig. 1a) - similarly to a product evolving\non a conveyor belt. This view suits to predictable and highly\nrepeatable processes. One of the major challenges identified\nby both practitioners and researchers in the ACM field, is\nthe attempts to deal with case management process in the\nindustry the same way as with regular business process -\n(a) Process-centered view (b) Data-centered view\nFig. 1: BPM vs. ACM systems, from [10]\ni.e. applying the process-centered view. In this work, we\nimplement the data-centered view (Fig. 1b) that is proposed\nby the Adaptive Case Management (ACM) practitioners [10].\nThis view implies that the process shall be adapted at run time,\naccording to evolution of case circumstances and case-related\ndata. This view suits to nondeterministic, knowledge-intensive\nprocesses like case management processes [11].\nThe body of knowledge on knowledge-intensive processes\nand ACM has been extensively developed by practitioners\nand published in the Internet blogs (see http://www.adaptivecase-management.com/, http://www.column2.com, and many\nothers). The group of researchers at IBM proposes an approach\nthat incorporates process- and data-centered perspectives and\nis based on concept of business artifacts [12], [13], [14].\nRecently it has been complemented by academical research\nand gave a rise to a series of publications ( [15], [10], [16],\netc.)\nIn the following sections, we define an operational semantics for case management process and illustrate this semantics\non the example of the Mortgage Approval process.\nB. Example: Mortgage Approval process\nA mortgage is a loan for buying a house. Mortgage Approval\nprocess is a typical example of a case management process.\nThe objective of mortgage approval process is to come up\nwith a right decision about the applicants request: provide\nhim/her with a loan or reject the application. In order to make\nthis decision, the potential money lender (e.g. a bank) must\nensure (i) that the applicant is credible and will be able to pay\nthe loan back and (ii) that the amount of loan is justified with\nrespect to the real cost of a property. 1\nIn [17], we provide a detailed description of mortgage\napproval process as defined by different financial institutions\nin the USA. In this work, however, we model a simplified\nversion of this process. We summarize the main steps and\nbusiness rules defined for Mortgage Approval process in the\nlist below.\n1In the USA, a bank will remain the owner of the property until the\nmortgage is fully paid back. Thus the bank shall validate that the property in\nquestion costs at least as much as the seller requests.\nMortgage Approval.:\n• An applicant submits a mortgage application where\nhe/she requests for a loan.\n• A bank can process only one application from a given\napplicant at a time.\n• The application should provide a list of documents including the applicant’s name, address, employment information, Internal Revenue Service (IRS) forms, recent\npay-stubs, medical certificate, information about the property to buy2 etc. (The exact list may vary depending on\nthe financial institution and the particular situation of an\napplicant.)\n• The application revision starts with a validation of the\napplicant’s credibility (i.e. the employment information\nmust be complete); the information about a property to\nbuy can be provided later in the process.\n• When the information on the property to buy is submitted,\nthe lender sends an appraisal agent to evaluate (appraise)\nthis property. Negative appraisal result is a sufficient\ncondition for the mortgage application rejection.\n• Preapproval - is an intermediate status of the application\nthat is given upon the confirmation of applicant’s credibility.\n• The proof of the applicant’s credibility and positive\nappraisal results are necessary conditions for application\napproval.\n• Some elements of the application (e.g. a medical certificate, family situation, etc.) may trigger the specific\nloan approval conditions (e.g. demand supplementary\ninsurance) or affect the loan type or duration.\nAs follows from the description, the process above should\nbe able to handle (in general, unpredictable) sequences of\nevents (i.e. submission of application elements, receiving evaluation results, etc) and cannot follow a predefined scenario.\nIII. CASE MANAGEMENT PROCESS: ABSTRACT MODEL\nAND OPERATIONAL SEMANTICS\nTheoretical computer science uses a concept of operational\nsemantics in order to define the meaning of a computer\nprogram. The operational semantics explains the meaning of a\nprogram ”in terms of a hypothetical computer which performs\nthe set of actions which constitute the elaboration of that\nprogram.” [18]\nIn this work, we define the meaning of the Mortgage\nApproval process by modeling it as a nondeterministic finite\nstate machine and by formalizing its operational semantics in\nAlloy specification language.\nA. Finite State Machine\nA finite state machine (FSM) or finite automaton - is a\nmodel of computation that specifies a machine that can be at\none state at a time and can change its state (perform a state\ntransition) as a result of a triggering event or condition. A FSM\n2By the property information we understand all the data that would serve\nfor further property appraisal.\nis defined by a (finite) set of its states and a set of triggering\nconditions for each of its transitions.\nMathematical model:\nA FSM can be defined as a quintuple (M = ⟨Q, Σ, δ, q 0 , F ⟩)\nwhere:\nQ is a finite set of states;\nΣ is a finite set of triggering events or conditions (input\nalphabet);\nδ : Q × Σ → P (Q) is the state transition relation;\nq 0 ∈ Q is the initial state;\nF ⊂ Q is the set of final states.\nOne can distinguish deterministic and nondeterministic\nFSM: for each state and triggering event, a deterministic\nFSM specifies only one next state, whereas a nondeterministic\nFSM can specify multiple possible next states. Using the\nmathematical model above, for deterministic FSM, we can\nwrite the state transition relation as δ : Q × Σ → Q - a\nfunction that returns a single state. For nondeterministic FSM\nit returns a set of states.\nIt is not difficult to prove that nondeterministic finite\nautomata (NFA) are equivalent (i.e. can be translated) into\ndeterministic finite automata (DFA). This translation results\nto more complex model (with increased number of states and\ntransitions) therefore in many cases (including our example)\nthe use of nondeterministic model is justified.\nApplication to the Case Management Process:\nCase management process can be modeled as a finite state\nmachine where\n- a FSM state represents ”a status” of the case (e.g.\ncase-related documents, evidences, examination results,\nreports, decisions etc.) at a given moment of time;\n- FSM triggering events represent events occurring during\nthe case (e.g. document submitted, results arrived, decision made by a manager, request canceled, etc) and\nresulting from activities performed by human agents or\nsystems involved into the case;\n- FSM transitions can be associated with one (or multiple)\nactivities and their specific outcomes (triggering events);\n- The possible case management outcomes (accepted or\nrejected claim, recovered patient etc) can be associated\nwith as a FSM set of final states.\nIn the next section, we model the Mortgage Approval process\nas a finite state machine and express its operational semantics\nin the Alloy specification language.\nB. FSM for the Mortgage Approval process\nThe Mortgage Approval process manages an applicant’s\ncase until a decision on the mortgage application is made. Here\nthe current state of the case is represented by a current state of\na submitted mortgage application. We model the evolution of\nthe mortgage application state over time as a FSM illustrated in\nFig. 2. We define the following abstract states for a mortgage\napplication:\nQ App = {Incomplete, Complete, P reapproved, Approved, Rejected}\nThe state transitions represent the activities carried out by an\napplicant (ex.: complete application) or by a bank (ex.: revise\nFig. 2: A FSM for mortgage application handling: F SM app\nFig. 3: A FSM for Employment Info handling: F SM empl\napplication). These activities are shown as transition labels in\nFig. 2.\nThe resulting FSM is nondeterministic: the same transition\ncan define more then one resulting state. For example, the\ncomplete application (completeApp) activity in Fig. 2 can\neither trigger the application transition from Incomplete to\nComplete state or can result in no transition, keeping the\napplication Incomplete. This reflects the real nature of the\nbureaucratic procedure behind.\nBased on the process description, the current state of a mortgage application depends on the states of elements constituting\nthis application: the applicant’s employment information and\nthe property information. We define the abstract states of these\nelements as follows:\nQ EmpInfo = {Undefined, Defined, Invalid, V alid}\nQ PropInfo = {NotF ound, F ound, AppraisedOK, AppraisedKO}\nOur objective is to design a process for mortgage approval\nthat will adapt to the evolving circumstances regarding the\nconcrete applicant’s case while respecting the business rules\nof the organization.\nTo ensure adaptability, we define a model where the mortgage application elements (i.e. employment info and property\ninfo) can emerge and evolve independently and with no predefined order (e.g. due to modification, validation, appraisal). For\nour example, we model the evolution of Employment info. and\nProperty info. with separate FSMs (Fig. 3, 4). The transitions\nof these machines are labeled with the names of the corresponding activities. These FSMs are also nondeterministic.\nThe state of the mortgage approval process itself can be\nnow considered as a combination of states of the three FSM\ndefined above (Fig. 5.)\nTo guarantee that our model will respect the business rules\ndefined by the organization (a bank), we express these rules\nFig. 4: A FSM for Property Info handling: F SM prop\nFig. 5: A FSM for a mortgage approval process as a combination of F SM app , F SM empl , F SM prop\nas dependencies between the state machines (red dashed lines\nin Fig. 5).\nTo specify how these machines will work together, we\nintroduce the Alloy operational semantics for our Mortgage\nApproval process.\nIV. ALLOY OPERATIONAL SEMANTICS FOR MORTGAGE\nAPPROVAL PROCESS\nAlloy [7] is a declarative specification language developed\nby the Software Design Group at MIT. Alloy is a language for\nexpressing complex structural constraints and behavior based\non first-order logic.\nAlloy Analyzer [8] is a tool for the automated analysis of\nmodels written in the Alloy specification language. Given a\nlogical formula and a data structure that defines the value\ndomain for this formula, the Alloy Analyzer decides whether\nthis formula is satisfiable. Mechanically, the Alloy Analyzer\nattempts to find a model instance - a binding of the variables\nto values - that makes the formula true. [19]\nSpecification of CMP in Alloy can be divided into four steps\nas follows:\n• Definition of data structure. In this step, the case\nelements, their structure and relations between them are\nformalized with Alloy signatures (sig) and constraints.\n• Definition of FSM states. In this step, the abstract states\nfor each case element are identified. These states will\nlater constitute the component FSMs.\n• Definition of triggering conditions. Here, the conditions\nthat will trigger a state transition in FSM are identified\nand formalized with Alloy predicates.\n• FSM assembling. In this step, the component FSMs,\nspecified for each case element are assembled into a\ncomposite FSM of the CMP. We formalize the relations\nbetween these FSMs with Alloy predicates.\nFSM-based operational semantics provides the meaning for\na CMP model and allows for further formal analysis to be\ncarried out on this model. In the reminder of this section,\nwe formalize our example - mortgage approval process using Alloy specification language. In the further section we\ndemonstrate how the formal specification can be analyzed\nusing formal model checking techniques.\nA. Definition of data structure.\nData structures in Alloy are represented with signatures\nlabeled by the keyword sig. Alloy signatures can be abstract\nor concrete, can have explicit cardinalities (i.e. one or set),\nand can contain one or multiple fields. A field can be seen as\nanalogy of a class attribute in OO languages.\nAlloy specification language is based on the set theory. From\nthe set-theoretical perspective, each Alloy signature defines a\nset of element of a given kind whereas its fields define the\nrelations with another sets (signatures).\nWe define the Employment Info and Property Info data\nobjects as Alloy signatures extending the generic Obj data\ntype:\nsig EmploymentInfo extends Obj{}\nsig PropertyInfo extends Obj{}\nAt this level of detail, we do not precise the structure of these\nelements (this can be elaborated at refinement if needed).\nWe define the Application signature by specifying the\nrelations between the mortgage application, employment information and property information:\nsig Application extends Obj{\n/*information about a house to buy */\nproperty : one PropertyInfo,\n/*information about the applicant employment */\nemployment : one EmploymentInfo\n}\nB. Definition of FSM states.\nBased on the set-theoretical perspective, we model a state\nof a data object as a set of such objects exhibiting the same\nproperties: for example, the state complete below defines a\nset of mortgage applications that are complete.\nsig State {\nundef, def, valid, invalid: set EmploymentInfo,\nincomplete, complete : set Application,\npreapproved, approved, rejected : set Application,\nfound, notFound, appraisedOk,\nappraisedKo: set PropertyInfo}\nAccordingly, we model a state transition as removing an\nelement from one set and adding it to another, for example:\ncomplete = complete + e &&\nincomplete = incomplete - e\nThe data structure State defines a state space for the\nFSM illustrated in Fig. 5. This finite state machine models\nthe Mortgage Approval case as a whole.\nWe elaborate our FSMs for mortgage application and its\ncomponents by introducing constraints on their states and on\ntheir possible state transitions:\nEmploymentInfo = undef + def\nundef & def = none\nvalid & invalid = none\n(valid + invalid) & undef = none\nThis example illustrates the constraints defined for the\nF SM empl state machine. The first two statements specify\nthat any employment info element can be either defined or\nundefined but not in both states at a time; The third statement\nspecifies that any element cannot be both valid and invalid;\nthe last statement implements a business constraint derived\nfrom the process description in section II-B: it stands that\nemployment info can be validated (and eventually can become\nvalid or invalid) only if it is defined.\nOther business properties of the mortgage approval process\ncan be also modeled as Alloy constraints:\n/*uniqueness of employment info */\nfact { all a1, a2:Application\n{a1.employment = a2.employment <=> a1=a2}}\nThis expression stands that no two applications currently\nprocessed by the bank can have the same employment info. It\nparaphrases and implements the business rule from the process\ndescription: ”A bank can process only one application from a\ngiven applicant at a time.”\nC. Definition of triggering conditions.\nWe formalize the conditions that trigger state transitions for\nF SM app , F SM empl and F SM prop shown in Fig. 2, 3, 4 with\nAlloy predicates. The triggering conditions are directly related\nto the (internal or external) activities and their outcomes.\nIn our example, one activity can trigger different transitions\ndepending on its outcome (e.g. completeApp). The converse\nis not true: each state transition in mortgage approval can be\ntriggered by one activity only.\nIn the example below, we specify a triggering condition\nfor F SM empl - a redefineEmpl activity - a submission of\nnew information about the employment by an applicant. The\ncorresponding Alloy predicate is defined with two parameters:\na current state st (the sate of a mortgage case before the\nactivity is carried out) and a next state st’ (the state of a\nmortgage case after the activity is terminated).\npred redefineEmpl [st, st’:State] {\n(some e : st.undef & Application.employment |\nst’.def = st’.def + e &&\nst’.undef = st’.undef - e ) ||\n(Application.employment = st.def )\n}\nThe FSM mathematical model defines a state transition as a\nrelation P (Q). This relation is specified in the predicate’s\nbody. In our example, it states that:\nsome (arbitrary) instances e of employment info that are\ncurrently undefined for their applications (i.e. belong to the\nset undef at the current state st), will be defined upon\nredefineEmpl termination (i.e. will be moved from the set\nundef to the set def in the next state st’).\nWe proceed with formalizing all the state transitions for our\nFSMs shown in Fig. 2, 3, 4 . This results in the following\nAlloy predicates: validateEmployment, searchProperty, appraiseProperty, completeApp, approveApp, rejectApp, preapproveApp (for the complete Alloy model, please, contact the\nauthor of this paper).\nWe specify the final states for FSMs by introducing the\nfollowing constraints:\nfact finalValid{all s: State, s’: ord/next[s],\ne:EmploymentInfo\n{e in s.valid => e in s’.valid }}\nfact finalApprKo{all s: State, s’: ord/next[s],\np:PropertyInfo\n{p in s.appraisedKo => p in s’.appraisedKo }}\nThe example above states that valid and appraisedKo\nare the final states of the corresponding FSMs (see also\nFig.3, 4). The Alloy expressions can be red as follows: once\nthe employment info is validated, it remains valid; once the\nproperty got a negative appraisal - it remains so (cannot be\nre-appraised).\nThese constraints are derived from the business specification\nfor mortgage approval (Section II-B). Once a new rule is\nintroduced: once negatively appraised, the property, can be reappraised (dashed transitions in Fig.4) - the finalApprKo\ninvariant shall be removed.\nD. Assembling the component FSMs into the Mortgage Approval FSM.\nOnce the component FSMs are specified, we assemble them\ninto the Mortgage Approval FSM as illustrated in Fig. 5 by\nspecifying the relations between their state transitions. The\nAlloy invariant run below represents such an assembly and\ncan be seen as a global state transition function for a mortgage\napproval process:\nDefinition of state transitions\nfact run{\nall s: State, s’: ord/next[s] {\n{ completeApp [ s.incomplete, s,s’]\n&& s’.approved = s.approved &&\n...\n|| {reviseApp[s.complete-s.approved-s.rejected,s, s’]\n&& ....\n}\n|| {redefineEmpl [s,s’] && ....\n|| {searchProperty[s,s’] && ....\n} } }\nThis expression specifies two groups of activities that can\nbe carried out during the mortgage approval: the internal\nactivities (complete application and/or revise application) and\nexternal activities (redefine employment, search property).\nThe corresponding predicates are related with inclusive ”or”\noperation (|| ) allowing for concurrent state transitions. From\nthe mortgage approval perspective, that means that during the\nprocess, the activities can be executed in any order - the\nessential property of CMP.\nThis run expression uses two variables - s and s’ that represent respectively a current and a next states of\nour mortgage approval FSM. Here ord/next is a predefined\nfunction in Alloy that specifies the ’next’ state as a function\nof a current state in the sequence of states to be generated by\nthe Alloy Analyzer.\nFollowing the definition of F SM app , completeApp represents an activity for application completion and specifies a\ntriggering condition for the applications, which are currently in\nthe state s.incomplete; similarly, reviseApp is applicable\nto complete mortgage applications for which the final decision\n(approved or rejected) has not been made yet. The reviseApp\npredicate specifies the revision activity with more details:\npred reviseApp [a: set Application, st,st’:State]{\n(some a1: a | preapproveApp[a1, st, st’]\n&& st.valid=st’.valid\n&& st.invalid = st’.invalid\n&& st.appraisedOk = st’.appraisedOk\n&& st.appraisedKo = st’.appraisedKo) ||\n(some a2: a | approveApp[a2, st, st’]\n&& st.valid=st’.valid\n&& st.invalid = st’.invalid\n&& st.appraisedOk = st’.appraisedOk\n&& st.appraisedKo = st’.appraisedKo) ||\n(some a3: a | rejectApp[a3, st, st’]\n&& st.valid=st’.valid\n&& st.invalid = st’.invalid\n&& st.appraisedOk = st’.appraisedOk\n&& st.appraisedKo = st’.appraisedKo ) ||\n(some a5: a | validateEmployment[a5, st,st’]\n&& st’.approved = st.approved\n&& st’.rejected = st.rejected\n&& st’.preapproved=st.preapproved\n&& st.appraisedOk = st’.appraisedOk\n&& st.appraisedKo = st’.appraisedKo) ||\n(some a4: a | appraiseProperty[a4, st,st’]\n&& st’.approved = st.approved\n&& st’.rejected = st.rejected\n&& st’.preapproved=st.preapproved\n&& st.valid=st’.valid\n&& st.invalid = st’.invalid)\n}\nThis predicate can be red as follows: for all applications considered for revision, the following activities can be carried out\nconcurrently: some applications can be approved, preaproved\nor rejected, whereas for some applications employment can be\nvalidated and/or property can be appraised. Here we explicitly specify that preapproveApp, approveApp and rejectApp\ndo not affect the employment validity or appraisal results;\nvalidateEmployment does not affect the application approval\nstatus neither it does the appraisal results etc.\nE. Refinement of the partial model.\nOnce a CMP is specified at the abstract level, we can\nprogressively refine our model by defining new case elements,\ntheir corresponding states and triggering conditions. The objective of step-wise refinement is to evolve from the partial\n(abstract) to concrete model of the process that would fully\nmeet its business specification. According to our formalism,\nthe CMP model refinement consists of two parts: refinement\nof FSM (adding states, transitions and triggering conditions)\nand corresponding refinement of the Alloy model (extending\ndata structures, adding or relaxing constraints).\nIn its current definition, our Mortgage Approval process\nmodel omits a lot of details: besides the employment info\nand property info, applicant’s bank records, tax forms, medical\ninformation, loan type and duration and many other elements\nshall be handled during the real mortgage approval. The idea\nFig. 6: A FSM for handling a medical certificate (top); A FSM\nfor handling a loan type definition (bottom)\nFig. 7: A FSM for a refined mortgage approval process\nis to iteratively refine our model by adding new FSMs for\nhandling these elements and by providing their corresponding\noperational semantics in Alloy.\nFig. 6, 7 show a possible refinement scenario according to\nthe following business specification parts:\n• A medical certificate must be provided by the applicant\nupon completion of the application file. According to the\nevaluation results, the mortgage lender can insist on the\nsupplementary insurance for the applicant.\n• The loan type shall be defined at preapproval and fixed\nupon the application approval.\nThe refined state machine for the mortgage approval process\nis illustrated in Fig.7.\nThe choice of state space and the ”relevance” of case\nelements is a separate topic that will not be discussed in this\npaper.\nAfter each design iteration, we simulate and validate the\nAlloy model with Alloy Analyzer. The main objective of the\nstep-wise refinement is to maintain the model validity while\nincreasing its complexity.\nV. USING ALLOY MODEL CHECKER FOR SIMULATION\nAND ANALYSIS OF MORTGAGE APPROVAL PROCESS\nMODEL\nOnce a CMP model is formalized in Alloy, it can be\nsimulated with Alloy Analyzer.\nA. Simulation\nDuring the simulation, Alloy Analyzer generates the set of\nmodel instances representing random CMP execution scenarios. These instances are shown in a form of diagrams and\nprovide a modeler with visual feedback. These diagrams can\nbe examined and analyzed by both technical and domain\nspecialists in order to detect unintended or implicit process\nbehavior that might indicate the errors in the business specification. Along with UML, BPMN and other conventional\ndiagrams, Alloy diagrams represent design artifacts and can\nserve a mean for communicating and evaluating the design\ndecisions.\nFor the Mortgage Approval process model, we define a\npredicate example and configure the run statement for its\nexecution:\npred example { ord/last.preapproved+\nord/last.approved +\nord/last.rejected =Application }\nrun example for exactly 1 Application,\n6 State, 1 EmploymentInfo, 1 PropertyInfo\nThe expressions above specifies a simulation of the Mortgage\nApproval FSM for 1 application, 1 employment info and\n1 property info. The Alloy Analyzer tool will search for a\nscenario that\n- starts in the initial state as specified by the initState\ninvariant,\n- follows the state transition rules for the FSM defined in\nthe run invariant,\n- involves exactly 6 states,\n- terminates in the final state, where all the applications are\nat one of the 3 states: preapproved, approved or rejected\n(as specified in the example predicate).\nThis simulation produces model instances (meaning that the\nmodel is consistent) - possible scenarios - and can be reconfigured for different number of elements and states.\nTable I shows one instance generated by the Alloy Analyzer\ntool. The instance is projected over State in order to reproduce\na sequence of state transitions generated by Alloy Analyzer.\nThe scenario illustrates the case where the employment info is\ndefined (State 1) completing the application (State 2); then the\nrevision invalidates the employment info (State 3) and, even\nthough the property info is submitted (State 4), the application\neventually is rejected (State 5). This scenario seems correct\nwith respect to the business specification of the mortgage\napproval.\nThe same simulation can be repeated multiple times producing different scenarios. TableII illustrates a model instance\nwhere two applications are handled at the same time:\nrun example for exactly 2 Application,\n8 State, 2 EmploymentInfo, 2 PropertyInfo\nTABLE I: Mortgage processing scenario\nThis scenario (some states are omitted) describes simultaneous\nprocessing of two mortgage applications: one (Application0)\nprogresses until its approval, whereas another one (Application1) is rejected. Running the simulations and analyzing the\nresulting scenarios - is an efficient technique for detecting\nerrors (manifesting as abnormal scenarios) in the model.\nB. Validation\nUsing the Alloy model checker, one can check if a CMP\nmodel corresponds to its business specification or if it has\nsome specific business properties or meets specific requirements. We express the properties to validate as Alloy assertions.\nAn assertion is a logical expression that ”asserts” a certain\ncondition to be True in the model. The Alloy Analyzer tool\nexamines all model instances (within a configured scope) in\norder to find at least one for which this condition will not hold\n(i.e. a counterexample). As for simulation, the counterexample\nis an Alloy diagram that shows the corresponding process\nexecution scenario. This diagram can be examined and analyzed by technical and domain specialists in order to detect\nthe problem source and correct the specification.\nBelow, we show several examples of model validation and\nimprovement using Alloy Analyzer.\nExample 1. Conformance to business specification.: The\nassertion below expresses the correct approval condition. It\ncan be interpreted as follows: for all applications, the fact\nthat an application is approved implies that its corresponding\nemployment info is valid and that its corresponding property\nis appraised with positive result.\napplApprovalOk: check {\nall st:State, a:Application |\na in st.approved =>\n(a.employment in st.valid &&\na.property in st.appraisedOk)\nfor a single application and six states\n} for 1 Application, 10 State,\n4 EmploymentInfo, 1 PropertyInfo\nWe check this assertion for our current model with Alloy\nAnalyzer and find no counterexamples. This validates the\ncorresponding business rule for our model. Along those lines,\nrejection, preapproval and other business rules can be validated.\nExample 2. Valid appraisal scheduling: According to the\nprocess description, a property appraisal is scheduled upon\nthe submission of the property information by the applicant.\nHowever, carrying out the appraisal for the applicants who’s\nemployment info is already known as invalid does not make\nsense for a bank. Thus, we want to ensure that the mortgage\napproval process modeled above prohibits an appraisal for the\napplications with invalid employment info. This is a business\nproperty that we express as an Alloy assertion:\npropAppraisalOk: check {\nall st:State, st’:ord/next[st], a:Application |\n((a.property not in st.appraisedOk\n+ st.appraisedKo\n&& a.property in st’.appraisedOk\n+ st’.appraisedKo) =>\na.employment not in st.invalid)\n} for 1 Application, 6 State,\n1 EmploymentInfo, 1 PropertyInfo\nThe Alloy Analyzer tool searches for an instance that would illustrate the violation of this condition (a counterexample). This\ngenerated instance (Table III) illustrates the scenario where the\nappraisal is done (State 5) whereas the employment info was\ninvalidated in the previous state (State 4) - it invalidates our\nassertion.\nTo correct the model, we revise the appraiseProperty predicate:\npred appraiseProperty[a: set Application,\nst, st’:State]{\nsome ap:a, p:ap.property &\n(st.found - st.appraisedOk - st.appraisedKo) |\nTABLE II: Mortgage processing scenario for\nTABLE III: Counterexample:\n//either the appraisal results are positive\n(( st’.appraisedOk = st.appraisedOk + p\n&& st’.appraisedKo =st.appraisedKo ) ||\n// or negative\n( st’.appraisedOk = st.appraisedOk\n&& st’.appraisedKo =st.appraisedKo + p ) ) ||\n// or no conclusion has been made\n(st’.appraisedOk = st.appraisedOk\n&& st’.appraisedKo =st.appraisedKo)\n}\nand add the corresponding constraint:\npred appraiseProperty1[a: set Application,\nst, st’:State]{\nsome ap:a, p:ap.property &\n(st.found - st.appraisedOk - st.appraisedKo) |\nap.employment in st.valid => ...\n}\nNow the assertion is validated by the Alloy Analyzer.\ntwo applications processed concurrently.\nwrong appraisal scheduling.\nExample 3. Valid preapproval status: In this example, we\ncheck that an application with already negatively appraised\nproperty shall not be preapproved for the mortgage (even\nthough its employment info got validated).\npreaprovalOk: check {\nall st:State, st’:ord/next[st], a:Application |\n( (a not in st.preapproved &&\na in st’.preapproved) =>\na.property not in st.appraisedKo)\n} for 2 Application, 10 State,\n4 EmploymentInfo, 2 PropertyInfo\nThe counterexample illustrating the violation of this condition\nis shown in Table IV. We revise the preapproveApp predicate:\npred preapproveApp[a: set Application, s,s’:State]{\na.employment in s.valid &&\n(s’.approved = s.approved &&\ns’.rejected = s.rejected &&\ns’.preapproved=s.preapproved +a)\n}\nTABLE IV: Counterexample:\nand add the constraint that states that we do not preapprove\nthe application if the property is appraised to Ko.\na.property in (PropertyInfo - s.appraisedKo)\nNow the assertion is validated by Alloy Analyzer.\nVI. RELATED WORK\nDuring the last decade, process flexibility and evolution\nsupport remains the central area of interest for many researchers: among others, numerous contributions of the groups\nat the University of Ulm (Dadam, Reichert et al) and the\nEindhoven University of Technology (van der Aalst et al) can\nbe emphasized.\nThe rigidity of work-flow based approaches as well as\nimperative modeling style is well recognized by both researchers and practitioners. In [20], van der Aalst presents\na case handling paradigm to cope with business process flexibility. The differences between case handling and traditional\nworkflow management are discussed and a metamodel for case\nhandling is presented. Similarly to our approach, this work\nlimits the formalization of case handling to activities, data\nobjects, and their interrelationships. State-transition diagrams\nare used to specify the operational semantics of case handling\nsystems. It proposes a definition of the case as a tuple CD\n= (A,P,D,dom,mandatory, restricted, free, condition) whereas\nwe formalize the case management process using more general abstraction of finite state machine. Mandatory, restricted\nand free relations defined in this work can be expressed as\nconditions in or corresponding Alloy specifications.\nThe works of Pesic and v.d.Aalst attempt to change the\nparadigm and provide an automated support for modeling\nand analysis of loosely-structured processes based on the\ndeclarative principles. The Declare framework presented in\n[21], [22] is a constraint-based system and uses a declarative\nlanguage grounded in temporal logic. The authors propose the\ndeviation mechanisms to assure the flexibility while preserving\nthe control over the process validity an correctness through\nverification at design time and performance analysis at run\ntime: ”... DECLARE can offer most features that traditional\nWFMSs have: model development, model verification (finding\nerrors in models), automated model execution, changing models at run-time (i.e., adaptivity), analysis of already executed\nprocesses, and decomposition of large processes.” [21]. In\nwrong preapproval decision\n[23], the Adept workflow management system is presented.\nAdept enables the modifications on the predefined workflow\nat the run time. Though, becoming ”more declarative”, the\nresulting approach is still based on the imperative principles\nof workflow.\nIn the research reported in [24], the authors ask the question:\n”Do Workflow-Based Systems Satisfy the Demands of the\nAgile Enterprise of the Future?” and draw the conclusion that\ndesigning and putting into operation workflowable processes\nmay neither be possible nor desirable in the enterprise of the\nfuture. In [25], process instances are represented as moving\nthrough state space, and the process model is represented\nas a set of formal rules describing the valid paths of the\npossible trajectories. This approach is grounded on the theory\nof automated control systems and implements declarative\nmodeling principles (purely non-workflow). In [12], [14], an\napproach for process management based on business artifacts\nis presented. Business entities are changing as they move\nthrough a process. They are described with an information\nmodel and a life cycle model (i.e. a mortgage case in our\nexample can be considered as a business entity). In [14], the\noperational semantics of Guard-Stage-Milestone is presented.\nThis semantics explains the interactions between business\nartifacts which is formalized following declarative principles.\nIn [26], semantics for a declarative process model based on\nGeneric Process Model (GPM) is presented. GPM uses states\nas a leading element in process representation; it captures the\nprocess context and also reasons about process goals. Similarly\nto our approach, the proposed semantics is based on the notion\nof states and set theory. Formal validation and model checking,\nhowever, is not yet realized for this approach.\nThe claim of workflow-based, process-driven or other imperative design methodology proponents is that the imperative\nspecifications can be validated, analyzed and controlled, assuring stable performance and predictable results. Similar results,\nhowever, can be assured by providing formal semantics for\n”unpredictable” declarative models and, eventually, applying\nthe formal verification approaches.\nThere are two main approaches to formal verification:\nmodel checking [27] and a theorem proving based on logical\ninference [28]. Model checking is an approach for verifying\nrequirements and design for a vast class of systems, including\nreal-time embedded and safety-critical systems. Model check-\ners include such tools as [8], [29]. The major drawback of the\nmodel checking is a state explosion problem, which originates\nfrom the fact that for real systems the size of the state space\ngrows exponentially with the number of processes [30].\nThe second approach is an automated theorem proving\nbased on logical inference. Within this approach, the fact that\nthe system specification (a model) satisfies a certain property is\nexpressed as a logical formula. The task is to prove the validity\nof this formula, deducing it from a set of axioms exist for the\nunderlying logic (e.g. first-, second-, higher-order logic etc),\nand hypotheses made about the system. Theorem proving for\nthe first-order logic is well developed; higher order and other\nlogics are more expressive and appropriate for wider range\nof problems then first-order logic; however the automated\ntheorem proving for these logics is more complicated [31],\n[32] .\nIn spite of their effectiveness, approaches based on a formal\nsemantics, model checking and verification using theorem\nproving are rarely used in practice due to the high cost.\nHowever, we agree with Colin Snook and Michael Butler\nthat ”Formal specification is the first step to using formal\nmethods and is, in itself, a useful activity even if a fully formal\ndevelopment process is not followed.” [33], [34].\nVII. CONCLUSION\nIn theoretical computer science, the operational semantics\nexplains the meaning of a program ”in terms of a hypothetical\ncomputer which performs the set of actions which constitute\nthe elaboration of that program.” [18]\nIn this work, we explained the meaning of a case management processes (CMP) in terms of a finite state machine\n(FSM), which performs the set of actions which constitute the\ncase handling. We formalized our operational semantics in the\nAlloy specification language.\nWe implemented the data-centered view (Fig. 1b) and\nproposed the model that prescribes no control flow but lets the\nprocess unfold according to a particular case circumstances.\nWe illustrated our findings on the example of Mortgage\nApproval process: first, we represented this process as an\nassembly of three independent FSMs handling the mortgage\napplication, applicant’s employment info. and property info.;\nthen, we demonstrated how these FSMs and their assembly can\nbe specified in Alloy and analyzed with the Alloy Analyzer\ntool.\nWe illustrated how model simulation and model checking\nwith the Alloy Analyzer tool can drive the case management\nprocess design:\nThe advantage of simulation is a possibility to get an instant\nvisual feedback in a form of diagrams capturing the FSM\nexecution scenarios. Model checking enables more specific\nreasoning about model properties.\nExamining the model instances generated by Alloy Analyzer, we were able to reason about correctness of our partial\nmodel; specifying and checking the Alloy assertions, we were\nable to validate business properties of our model (e.g. correct\nconditions for application approval and preapproval, correct\nappraisal scheduling, etc.).\nOnce the model validity is checked at a given level of detail,\nthe model can be refined. According to our formalism, the\nrefinement consists of two parts: definition of the new FSMs\nand formalization of these FSMs in Alloy. This approach\nallows us to efficiently manage model complexity.\nAlloy specification language was successfully applied in\nvarious domains including software and hardware system\nspecification, automatic model completion, service testing,\netc.3 Its use in business domain, however, is rather limited.\nIn this work, we position Alloy and Alloy Analyzer as a\ntoolbox for both technical and business specialists: together\nwith UML and BPMN diagrams, Alloy diagrams represent\ndesign artifacts and can be studied, referred to, and serve a\nmean for communicating and evaluating both business and\ntechnical design decisions.\nIn the mortgage approval example presented in this paper,\nwe consider that a state transition can be triggered by a\nspecific outcome of a single activity (i.e. no two activities can\nhave the same outcome and trigger the same state transition).\nPossible effects of contextual events on state transitions (e.g.\nan economic crisis may affect a mortgage approval decision)\nare not considered by this example. In general case, a triggering condition can be seen as a combination of contextual\nevents and multiple outcomes of several (internal or external)\nactivities. Therefore, a user can have a choice of activity to\nexecute. In [35], we discuss context modeling, formal concept\nanalysis and Galois lattices as a background theory for contextaware and agile process management. Definition of automated\nuser guidance for CMP is a subject of our future work.\nThe operational semantics defined in this work is a first step\nto using formal methods; it paves the road to automated design,\nvalidation and verification of CMP. This work, however, does\nnot suggest any concrete (graphical) modeling notation for\ncase management processes as we convinced that visual syntax\ncan be freely chosen according to the modeler’s taste as soon\nas the semantics that has to be encoded by this syntax is well\nestablished.",
  "2013-RKPLG": "1 Introduction Capacity to timely discover and to efficiently respond to rapid changes in the environment is a major goal of an enterprise of the future. According to [23][4], a firm’s ability to adapt to dynamic environments depends first on the agility of its business processes. Therefore, design and development of new process management systems enabling process adaptation at run time are essential. defines a process as a sequence of events occurring in system [16], where Lamport each event is triggered by an action. Accordingly, a business process can be seen as a sequence of events triggered by activities of business process actors. The majority of existing methods for business process design follow imperative principles, implying that the order of events is predefined. As a result, all meaningful process events need to be determined and corresponding actions need to be predefined at design time. At run time, processes follow the configured model with limited possibilities to deviate from the predefined scenario. Execution of a business process in a dynamic environment can be compared to navigating a ship towards its destination bay in uncertain waters. Very rarely can a ship follow blindly a predefined path: awareness about its current position and situation as well as navigation skills and dynamic path finding are essential to reach the destination. In this work, we discuss foundations and propose architecture for a system supporting dynamic and context-aware business process adaptability. First, we shift the traditional imperative paradigm for process design and exploit declarative principles: we represent a business process as finite state machine (FSM) [22] with a state representing a process situation at a given time and state transitions defining the possible process scenarios. The triggering events specify the underlying process semantics, i.e. conditions for state transitions. The FSM formalism makes the notion of process activity implicit while putting forward activity outcomes, which are modeled as triggering events. Therefore, the declarative process model focuses on “what” needs to be done in order to achieve the process goal and not on “how” it has to be done. This allows us to handle process events whose order of occurrence is undetermined and to define the corresponding handling scenarios at run time. Navigation in a stormy ocean depends on a skillful skipper and his capacity to select a right action to ensure that no incorrect scenario is executed. We design initial navigation rules for process guidance based on Formal Concept Analysis and Galois lattices [2][6]. We specify the resulting process as a set of activities that can be dynamically assembled at run time into one of the (non-forbidden) process scenarios. In general, such process specification can offer infinitely many alternative scenarios and a possibility to deviate from one scenario to another during the execution. We 1st formalize these properties of a process as the form of agility. Navigation in a stormy ocean depends on the capacity of the skipper to select a right action at the right moment, and with respect to the current situation: we define the 2d form of process agility as the ability to monitor and manage the process context and to dynamically select and/or alter the execution scenario accordingly. We extend the declarative process specifications with dynamic context models and mechanisms for dynamic context management [3][19][20]. We design navigation rules for processes guidance that handle both process events (events resulting from execution of process activities) and context events in a unified way. This is compatible with the FSM formalism: the nature of events triggering the state transition has no importance. The navigation rules ensure that no incorrect scenario will be executed with respect to a given context situation. Novel combination of declarative modeling principles, context-awareness and Formal Concept Analysis is the main research contribution of this work. The architecture for a context-aware business process engine (CAPE) summarizes our findings. The remainder of this paper is organized as follows: in Section 2, we discuss the state of the art related to business process design and associated agility problems; in Section 3 we formalize two forms of business process adaptability. These theoretical foundations are used in Section 4 to specify the architecture of our context-aware agile business process engine (CAPE). We finally illustrate our findings on an example and present the perspectives of this work. 2 Motivation and Related Work Workflow-based approaches are highly efficient for process design and management assuming that: (i) all the events triggered by process activities are well determined; (ii) human involvement is limited to “accept”, reject” or “select from the list” types of decisions; and (iii) the system within which a process is executed can be considered as closed: no external event affecting the process execution can occur. For a large category of processes, however, these assumptions do not hold: in health care, the same therapy may have different effects on different patients; in insurance, claim processing extensively involves human expertise and decision making; in business, many processes have to cope with evolving economic conditions and frequent changes of customer requirements. The limited capacity of imperative methods to deal with changes has been recognized by both researchers and practitioners [30]. Numerous approaches propose to improve the dynamic process adaptability: In [25][15][26] the concept of configurable process is presented and different modeling formalisms to deal with process configurability are defined. Process configurability allows to instantiate a process model according to the context and to improve the process adaptability. At execution, however, the process will either follow the preconfigured model or select from a limited set of variations [26]. Other works aim at improving run time adaptability through modification of the predefined workflow during execution [21][29]. The authors propose deviation mechanisms to ensure flexibility while preserving the control over the process validity and correctness. As above, at the run time the process instances follow the preconfigured model (imperative) and support process adaptability only to some extent. Solutions for processes characterized by “unpredictability” are reported in numerous works [28][17][1][7]. In [28], the foundations and collection of experience reports on Adaptive Case Management are presented. These works emphasize run time adaptability. Markus et al. [17] propose a framework to support emergent knowledge processes, implemented in the TOP Modeler tool. In [1] process instances are represented as dynamically moving through state space. This approach relies on automated control systems and implements declarative modeling principles. Burkhart et al. [8] propose to explore the capabilities of recommender systems to provide the user with intelligent process-oriented support at run time. While handling dynamic activity selection and configuration of processes “on the fly”, the majority of proposed solutions demonstrate only limited capacity to deal with process contextual information in systemic and dynamic way. In [24][27], authors use context information for process definition. Roseman et al. [24] consider that external context elements may influence business processes (e.g. weather influences processes of a call center in an insurance company). They incorporate such elements into business process modeling. Saidani et al. [27] also consider context in business process definition, in particular, the roles played by actors. In these works context information is specified only at design time. Mounira et al. [18] propose a process mining architecture to identify context variables influencing process activities. However, no specific model formalizing these variables is proposed. 3 Foundations for CAPE In this section we define two forms of process agility and present our vision of context-aware business process management based on a fully declarative modeling combined with innovative context management and formal concept analysis. 3.1 Process Agility at Work: Agile Patient Care series1, As we could see in “House” American TV Patient diagnostics and treatment processes in a medical ward only partially rely on imperative procedures. The main challenge is to be aware of the patient situation and its evolution and to adjust the 1 http://en.wikipedia.org/wiki/House_TV_series treatment accordingly. Contextual parameters that might be relevant and should be managed include (but are not limited to): Patient’s measurable body conditions (temperature, blood pressure, heart rate); - Patient’s medical record; - Patient’s life style; - Information about recent workload, leisure activities, trips. - Some of these parameters are stable (e.g. predispositions, allergies, etc.), others can evolve (e.g. new information about the patient’s medical history, recent activities), and some others may change several times a day (e.g. body temperature). The capability to immediately react by canceling/prescribing new tests or medications is essential. 3.2 First Form of Process Agility We define the first form of business process agility as a capacity to handle unpredictable sequences of system events. This implies that the order of process activity invocations is defined dynamically, at run time, and depends uniquely on the current situation (process state) rather than on a predefined execution scenario(s). 3.2.1 Declarative Approach to Process Specification To ensure the first form of agility, we shift the traditional imperative paradigm in process specification and exploit declarative principles: we represent a business process as a finite state machine (FSM) – a state-transition system - that allows us to handle process events (and context events – see Section 3.3) with undetermined order of occurrence and to define the corresponding scenarios at run time. A FSM [22] specifies a machine that can be at one state at a time and can perform a state transition as a result of a triggering condition (event or group of events). It is defined by a (finite) set of states and a set of triggering conditions for each transition. Mathematical model: A FSM can be defined as a quintuple < Q, ∑, δ, q , F > where: 0 Q = {S , S , … S }- is a finite set of states; 0 1 n ∑ = {E , E , … E }- is a finite set of triggering events (input alphabet); 1 2 m ∑ δ: Q x P(Q) is the state transition relation; S Q is the initial state; ∈ 0 F Q is the set of final states. ⊆ A business process can be modeled as a finite state machine where: A FSM state s Q represents a state of the process at a given time. It is - ∈ described by the states of its parameters (e.g. order: {in preparation, delivered, paid}, patient: {admitted, in diagnostics, under treatment, discharged}). FSM triggering events ∑ represent events that may occur during the process - execution (e.g. change of patient’s body temperature resulting from medication etc.). Note, that process activities are implicit within FSM formalism: only the events resulting from an activity execution are observable. FSM transitions represent transitions between the process states. - δ FSM final states F represent the process outcomes (order delivered, patient - discharged, etc.) and can be associated with the process goal. Representation of a business process using states and transition is not new: [31] presents DecSerFlow language for Declarative modeling of service. In [21] authors present the DECLARE system to support declarative (loosely-structured) process models. Both use formal semantics based on labeled transition systems. In [1][5], the process execution is presented as a trajectory in the process state space. The process model is specified as a set of formal rules describing the valid paths of the possible trajectories. Our approach extends this paradigm and uses the FSM formalism as a common ground for process model, context model and formal concept model. Example: Agile Patient Care (continued) To illustrate the FSM formalism, we develop our example on agile patient care and model a (simplified) patient treatment process as a FSM illustrated in Fig. 1: After a patient is admitted (S ) to a medical ward (the initial state), a physician 0 examines him in order to obtain information for diagnostics (S ) and further treatment 1 (S ). Diagnostics may involve one or multiple examinations and/or generic or specific 2 tests. The patient’s case is then assessed and a treatment is prescribed. During the treatment, additional examinations can reveal new patient’s condition and require to modify the assigned therapy and, even, to repeat diagnostics and assessment. Once the therapy is terminated and the patient’s good condition is confirmed, the patient is discharged (S ) from the ward. In this example, we identify four states: Q={S , S , S , 3 0 1 2 S } and six process activities that can be executed during the process and trigger state 3 transitions from S :Admitted to S :Discharged states (Table 1). 0 3 Fig. 1. A finite state machine (FSM) representing a patient treatment process. The events E and the state transitions they trigger are shown as labels According to our formalism, an activity A is described with a pair <S, E > where: A ! S Q is the set of states from which this activity can (but not necessarily will) be ⊆ invoked;! ! E ∑ is the set of events that can result from the activity execution and can ⊆ ! potentially trigger a state transition. For each activity A, the state transitions that can be triggered upon its termination can E be calculated as: : S x P(S). δ A For example, the activity A2 (Medical laboratory test) is specified as follows: S: {S1, S2}; E:{E5, E6, E7}. Note that some events can result from a process activity and can be context events (cf. Section 3.3) - independent from activities. (e.g. E10 – condition improved). Table 1. Abstract activities and events defined for the Patient treatment process Activity Avail. at: Process events (Activity outcomes): A1 Physical S0, S1, S2 E1 Confirms the declared symptoms examination E2 No problem found E3 New symptoms emerged E4 Supplementary medical tests are required A2 Medical S1, S2 E5 Positive results (anomalies detected) laboratory E6 Negative results (no anomalies detected) test E4 Supplementary medical tests are required A3 Specific S1, S2 E5 Positive results (anomaly detected) medical tests E6 Negative results (no anomaly detected) E4 Supplementary medical tests are required A4 Case S1 E7 Diagnose confirmed, treatment assigned Assessment E8 Diagnose not confirmed, patient discharged E4 Supplementary medical tests are required A5 Therapy S2 E9 Condition declined (e.g. symptoms increasing) E10 Condition improved (e.g. symptoms decreasing) E11 Side effects emerged E3 New symptoms emerged E12 Stable situation E13 End of therapy A6 Recovery S2 E3 New symptoms emerged E14 End of recovery therapy Context events: E15 New medical / personal evidence received E3 New symptoms emerged E9 Condition declined, (e.g. symptoms increasing) E10 Condition improved (e.g. symptoms decreasing) 3.2.2 Formal Concept Analysis and Galois lattices Within our model, the partial ordering of process activities is determined by the state transition relation P(Q). This relation specifies the valid transitions with respect to the process goal (its final state) and ensures that invalid state transitions (e.g. to discharge a patient with critical temperature) will be avoided. This relation can be specified with Formal Concept Analysis (FCA) [2] [6]. FCA is a mathematical theory relying on the use of formal contexts and Galois lattices defined below. The use of Galois lattices to describe a relation between two sets has led to various classification methods [8] [33]. Since then, they have been used in numerous contexts to extract hidden knowledge from data. Let us first introduce FCA terminology [12]. Mathematical model: Let K = (G, M, I) a formal context, where G is a set of objects, M is a set of attributes, and the binary relation I G x M specifies the attributes of the different ⊆ (.)I objects. Derivation operators noted are defined for A G and B M as follows: ⊆ ⊆ AI BI = {m M│∀ g A : g I m} = {g G│∀ m B : g I m}, ∈ ∈ ∈ ∈ AI BI where is the set of attributes, which are common to all objects from A and is the set of objects which share all attributes from B. A formal concept of the context (G, M, I) is a pair (A, B), where A G and B ⊆ ⊆ BI AI. M, A = et B = The set A is called the extent of concept (A, B) and B is its intent. A concept (A, B) is a specialization of concept (C, D) if A C, which is equivalent to ⊆ D B. This is noted (A, B) (C, D). Reciprocally, the concept (C, D) is a ≤ ⊆ generalization of concept (A, B). The set of all concepts and their partial order relation constitutes a lattice, called Galois lattice of the context K. The major interest of a Galois lattice is the structure it provides through the conceptual clustering of objects according to their common attributes. This allows the identification of the most conceptually significant objects and attributes. Another interest of Galois lattices is that association rules can be inferred automatically from them. Several works have indeed applied FCA to the extraction of relevant association rules [13] or to perform sequential pattern mining [11]. Within our approach, process states Q, triggering events ∑ and process activities defined in Section 3.2 form a formal context and can be analyzed using Galois lattices. Process states and activities can be clustered revealing their conceptual properties: For example, we can determine activities that can be executed (or suggested for execution) under given conditions and with an objective to trigger a desired state transition. Fig. 2 represents Galois lattices built from the formal context defined in Table 1. The lattice in Fig 2a clusters activities into (possibly overlapping) groups according to the common events shared by the activities within each group; The lattice in Fig 2b represents the sets of activities enabled for each state. We exploit Galois lattices as a recommender mechanism, which makes suggestions about activities to execute. This mechanism is explained in Section 4. Each concept of the lattice contains a set of activities (white labels) described by the events they share (grey labels). The links between concepts are generalization/specialization links. Note that nodes inherit events from concepts above them and that they inherit activities from concepts below them. The lattice has an upper bound (top concept), which contains all activities (through inheritance) but no event (i.e. there is no event associated with all activities). The lower bound of the lattice (bottom concept) contains no activity and all events. a) Activity – Event formal context b) Activity – State context Fig. 2: Galois lattices. Let us consider the concept containing only activity A1 and the concept containing only A4 in its extent (Fig. 2a). We see from the intent that A1 may have events E1, E2, E3 or E4, and A4 – events E4, E7 or E8 as their outcomes. These two concepts are generalized by the concept {(A1, A2, A3, A4), {E4}}, which contains activities A1, A2, A3 and A4, all described by the common event E4. Along those lines, the concept {(A1, A2, A3), {S1, S2}} in Fig.2b shows the activities described by common states S1 and S2 – the states where these activities can be executed. 3.3 Second Form of Process Agility We define the second form of business process agility as a capacity to adjust the process execution scenario according to the current contextual situation. Process activities are assembled at run time, according to observed context and with an objective to trigger a state transition required for achieving the process goal (defined as one of the final states of a FSM). Dey [9] defines a context as any information that can be used to characterize the situation of an entity (a person, place or object) that is considered relevant to the interaction between a user and an application. The notion of context adopted in the literature is mostly user-centric and limited to physical aspects (e.g. location, user preferences, or user device)[19]. Together with Dourish [10], we argue that the notion of context is larger and includes information related to organization and processes: “context – the organizational and the cultural context, as much as the physical context – plays a critical role in shaping action and also in providing people with the means to interpret and understand action”. In our example, patient treatment process can be influenced by the emergence of new symptoms or the arrival of new resources (e.g. new medical/personal evidence, etc.). The second form of business process agility consists in taking into account such context information during process execution. The context parameters reflect our awareness about external and internal information about the process; they can be observed and measured. Even though context-awareness for business processes is addressed in the literature [24] [27] [18], the lack of formalism for context representation and management persists: many of the proposed context models are static (need to be defined at design), incomplete (consider only limited context information) and specific to workflow-based processes. We argue that the number and kind of context parameters may vary from one situation (or process state) to another [9] making it impossible to exhaustively model all required context information within a single (static) context model. The context model, therefore, needs to be dynamically instantiated from an appropriate metamodel according to specific (evolving) context dimensions. 3.3.1 Dynamic Context Modeling According to Najar et al. [19], the formalism chosen for representing context model determines the reasoning methods that can be used to perform system adaptation to the observed context. A context model (i) ensures the definition of independent adaptation processes and (ii) isolates this process from context acquiring techniques. The same applies to context-aware business process. We claim that the process context information should be acquired, modeled and formally analyzed at run time in order to adapt business process execution and to ensure business process flexibility. Several context models have been proposed in the literature [19] [3]. Based on these works, we define a common meta-model presented in Fig. 3. In this meta- model, we consider context as a set of context elements that are observed for a given subject (e.g. the patient, a device, a resource, etc.). Each subject can be associated with multiple context elements (location, status, etc.); for each context element, we observe values that can dynamically change. Semantics of context elements can be described using ontologies [19]. Ontologies also provide reasoning mechanisms, i.e. inference rules. Fig 3: Context meta-model considering context as a set of context elements Mathematical model: We formalize the context of a subject s in a time t as follows: Context(s,t) = { Element(s, ce) }, where Element(s,ce) represents the observed value of the context element ce for the subject s. In our approach, process states Q defined using FSM formalism in Section 3.2 can be extended with the context information: Each state S Q can be associated with a ∈ (set of) subject and its contextual elements to observe: S = {Context(s,t)} FSM triggering events ∑ represent both (i) context events occurring during the process execution and/or (ii) events resulting from executed process activities – process events. Based on context ontology, we represent context events as logical constraints over context concepts and their observed values. As a result, a FSM processes both contextual and process events in a unified way. These events can be expressed using logical conditions on observed values of contextual elements. Note that process activities are implicit within FSM formalism: only the events resulting from an activity execution are observable. In our example, a subject s – patient – is observed in all the process states. The contextual elements ce associated with a patient include his body temperature, blood pressure, patient record etc. Patient’s condition can decline (patient’s temperature may evolve from ‘36.5°C’ to ‘39.7°C’) or new evidence about the patient can be received by e-mail. Triggering event E9 (condition declined – see Table 1) can be expressed using the following condition: Element(#patient, #temperature) > 38.5. 3.3.2 Context Model and Concept Analysis Fig. 1 shows both the process and the context events that are taken into account by the patient treatment process. Context events affect state transitions and play the role of triggers, similarly to activity outcomes. For example, the patient treatment (S2) can be changed either as a result of physical examination outcomes (E1) or as a response to the new symptoms (E11), or because a new evidence about the patient (e.g. allergies, predispositions, past incidents, etc.) (E15). Galois lattices are particularly adapted to our context-aware approach: we use Galois lattices to analyze FSM triggering events (see Section 3.2) regardless their nature (process events or context events). Indeed, FCA has already been successfully coupled to context modeling [32]. The authors have proposed a spontaneous, personalized and dynamic way of defining and joining user communities. They use a lattice-based classification and recommendation mechanism that analyzes the interrelations between communities and users, and recommends new communities, based on user interests and preferences. 4 Architecture for CAPE In this section, we define the primary elements and the architecture for context-aware business process engine (CAPE) that is grounded on our theoretical foundations. To support the forms of agility introduced above, we define the following primary elements for a context-aware process engine: activity repository, context monitor and navigation manager, as illustrated in Fig. 4. Fig. 4. CAPE architecture: the context monitor, the activity repository and the navigation manager. 4.1 Activity Repository Activity repository represents a set of activities related to a business process. Each activity is specified with states S from which it can (but not necessarily must) be executed during the process, and with the set of events E representing its possible outcomes as defined in Section 3.2.1. The pair <S,E> is an activity signature. These signatures are further used by Navigation Manager. The activity repository represents crew skills and technical capacity of a ship. 4.2 Context Monitor Context monitor is in charge of observing, at run time, context elements and subjects from the environment. Its role in CAPE architecture is similar to a watchman in a ship: it observes navigation conditions and reports them to the navigation manager. Context monitor is based on the context meta-model described earlier. Similar to [20], it recognizes context elements by plugins, which feed the monitor with dynamic information about a given context element from a subject (e.g. a plugin reading patient’s heart rate or medical resources’ location from their id card). Context monitor can be dynamically extended by adding new plugins for observing new context elements and subjects. Context values dynamically observed by context plugins define the current position of the process in its state space. This position is further used by the navigation manager for calculating/adjusting the next process stepsand for recommending the activities to execute. 4.3 Navigation Manager Navigation manager makes navigation decisions, like a skipper in a ship. It determines one (or several) plausible activity to execute with respect to the process goal and the contextual situation. Specifically, navigation manager takes into account the current state of the process, the context events and the signatures of activities defined in the activity repository. Based on the navigation rules, it determines a set of activities enabled in a given situation and calculates those of them that will have a highest probability to result in the desired outcome. Navigation engine is the core element of CAPE architecture: it links together the other elements. The functionality of CAPE can be described as follows: this engine enables the activities for execution, captures the events (internal or external) and generates state transitions (Fig. 4). Considering that the process objective is specified by its final state (or states), the engine should also assure an appropriate guidance for the decision maker (a human agent): it recommends him the activities that would maximize chances to trigger a transition leading towards the process goal. We illustrate the functioning of CAPE on our example: Example: Agile Patient Care (continued) Once a patient has been admitted (S0) and the result of his physical examination (A1) has confirmed the declared symptoms (E1), he is in diagnostics (S1). The targeted final state for our process is S3: discharge the patient. The goal of the navigator is to provide recommendations in order to reach this state as quickly as possible, while respecting medical protocols and taking into account the contextual situation. In the following, we describe the sequence of operations performed by the navigator to meet this goal. 1. Selection of the next transition towards the targeted final state Fig. 1 illustrates the various possible paths from one state to another. The underlying graph is directed, nondeterministic, with possible cycles. From this graph the navigator computes the shortest path from S1 to S3, which consists in using the direct link from S1 to S3 (any other path goes through a higher number of states). The next step, in our example, is to trigger the transition from S1 to S3. 2. Identification of relevant activities may2 The navigator now has to identify the events, which trigger a transition from S1 to S3. According to our FSM (Fig.1) and the state transition map, the triggering conditions for a transition S1 - S3 are the following: Additional physical examinations and/or medical laboratory tests reveal no ! anomaly or problem and the patient himself feels better - (E2 | E6)&E10, or Case assessment does not confirm the diagnosis (E8) ! 2 Our graph is nondeterministic: one event can trigger multiple state transitions (e.g. E11 – side effects emerged – can be handled both in the therapy states or can trigger a transition to the diagnostics state. This nondeterminism, in general case, can be resolved by refining the states and transitions, and by specifying the new navigation rules. Alternatively, a probability p can be associated with each of the possible transitions. To identify the activities that can be of a maximum utility with respect to our objective (transition S1-S3): Navigator identifies the activities in our repository that can ensure a desired ! outcome (a combination of events described above) using the Galois lattice illustrated in Fig. 2. For our example, the event E2 can result from the activity A1; E6 from A2 or A3; E10 from A5 or spontaneously as an external event, as the patient’s condition may improve independently from any activity from the medical staff. Navigator selects those activities that are enabled at the state S1 (according to ! their specification) using the Galois lattice illustrated in Fig. 2. For our example, the activities A1, A2, A3, A4 are available in S1 (In Diagnostic) state, whereas A5 is not. Thus, the activities A1 – A4 are potential candidates for execution in the state S1. 3. Selection of recommended activities based on their utility We calculate an utility of each scenario as its likelihood to trigger the transition S1- S3. For our example, A5 is not available at S1 and therefore, E10 can be expected only as a context event (i.e. we can observe this event when it happens but cannot control it). The following viable scenarios can be evaluated: a) A physician prescribes medical tests and/or makes additional physical examinations according to the declared symptoms. The spontaneous improvement of the patient is expected. b) A physician assesses the case based on the patient’s history (and examinations made upon patient’s admission to the ward). A combination of these scenarios is also possible. Depending on the probabilities of transition S1-S3 resulting from each of the aforementioned scenarios and also, on the probability to obtain the event E2 as a result of A1, E6 from A2 or from A3, and E8 from A4, the navigator may recommend specific activities to the medical staff. 5 Discussion and Future Work While providing a rich toolbox for process modeling, verification, and post-execution analysis, traditional workflow-based information systems are considered to be inflexible. Therefore, organizations nowadays are searching for a compromise between the automated user support and process flexibility at run time. Fig. 5a illustrates this dependency: the x-axis shows a degree of process flexibility (from highly structured to unstructured, ad-hoc processes); the y-axis shows a level of automated user support that can be assured by the system. The solid line depicts the dependency: the more flexible a process, the lower the user support provided for it; and conversely: the higher the expected level of user support, the more structured a process has to be. According to Burkhart et al. [7], “With increasing runtime flexibility, employees are confronted with an expanding decision space and they are needed to possess more expertise in dealing with the processes they are involved in.” Fig. 5b illustrates this dependency: the x-axis shows a degree of process flexibility; the y-axis shows a level of user expertise required for handling this process. The solid line depicts that more flexible a process is, the higher expertise level of an employee should be. The two forms of agility formulated in this paper aim at relaxing the requirements both for human expertise for a given level of process flexibility and for process rigidity (predefined structure) for a desired level of automation. This is depicted with dashed lines in Fig. 5a and 5b. Declarative approach for process design and use of formal methods enable a set of automated techniques for process analysis and validation based on model checking and theorem proving. Thus, they improve the level of automated user support allowing maximum run time flexibility. Context awareness and formal concept analysis enable automated recommendations and identification of alternatives. Their joint use provides flexible guidance to end users at run time and supports them with an expertise required for the process handling. This idea paper reported on research, which is currently at its early stage of development. According to the IS research framework [14], (i) we specified our business problem as a lack of automated support for business process agility; (ii) then we defined a relevant knowledge base for our research and outlined the foundations for CAPE; (iii) we built our design artifacts: we introduced the two forms of process agility – the constructs to be used for reasoning about business processes; we also defined a model and a method for specification of agile business processes based on FSM abstraction and formal concept analysis. We extended this model with the dynamic context model. We combined these artifacts in and proposed the architecture for context-aware agile business process engine (CAPE). a) Process flexibility vs. Automated user support b) Process flexibility vs. Level of employee expertise required Fig. 5. Relations between process flexibility and automated support / required level of user expertise. The solid line depicts the current trends in BPM; the dashed line depicts how CAPE architecture presented in this work can possibly change these trends. This work accomplishes the first part of the “build-evaluate” loop [14]. Evaluation of our designed artifacts and their refinement will be addressed in future work. More specifically, we envisage to demonstrate the utility of our proposed architecture, first, by developing detailed scenarios, then, by simulating them, and, eventually, by implementing CAPE architecture and studying its usability in real business environment. Usability metrics for CAPE will be also discussed in future publications.",
  "2013-RZS": "1 Introduction According to [1], in the coming years enterprise software systems will not be able to continue to evolve along the beaten paths, because there is an urgent need for new directions in the ways enterprise software is conceived, built, deployed and evolved. This contention is becoming materialized even presently, when the boundaries of companies gradually fade away paving the road to liquid enterprises having fuzzy boundaries in terms of human resources, markets, products and processes which require adequate Internet-based Enterprise Systems. Decentralization of organizations and subsequent changes of their management require major changes in organizations’ processes and heavily involves the use of IT. Between traditional (highly centralized) and decentralized or “liquid” enterprise, many other organizational structures can be identified [2]. In these work, we analyze three forms of organizational structure: centralized, federated and decentralized. This work studies the conceptual differences between these organizational forms with focusing on how these differences affect creation, maintenance and evolution of the IT within the corresponding types of organizations. Our objective is to make an explicit link between the structure of an organization and its EA, ensuring thus better support for federated and decentralized organizations. We envision an architecture- driven corporate and IT governance involving adequately performed communication with a set of policies, multi-level decision making, knowledge management, automation of tasks by taking advantage of IT infrastructure, human management, etc. The main research question addressed in this paper is: How to integrate the decentralization concepts into EA methodologies? The proposed solution follows Design Science research framework [18], which suggests that an innovative solution is proposed to solve a problem of general interest. Following the framework, in our study (i) we identify a problem from the real world - a need to support the modern types of enterprises characterized by increasing decentralization and demand in flexibility and agility of their IT; (ii) then we define a relevant knowledge base for our research that is grounded on organizational science, and the enterprise architecture discipline; (iii) we build design artifacts: the two constructs to be used for reasoning about organizational structure in general, and IT organizational structure in particular; (iv) to evaluate the created artifacts, we apply them in the environment of a Federated Organization in the Swedish Higher Education sector. This paper reports on the research in progress and will be organized as follows: in Section 2, we outline the theoretical foundations for this work and discuss the related works. In Section 3, we define a relationship between a structure of organizational IT and an EA: first, we present the concepts of center and steering forces that link organizational structure, IT Governance and EA; than we use these concepts to identify conceptual problems related to IT decentralization and to propose solutions. These findings are illustrated with a real organization case in Section 4, which is followed by our conclusions and the direction of future work in Section 5. 2 Theoretical Foundations and Related Work In this section brief overviews of the topics and the results related to the research of this paper are presented. 2.1 Centralized, Federated and Decentralized Organizations The organizational structure defines the rules according to which allocation of responsibilities and resources, coordination and supervision, is made for an organization (and - in case of IT - for the IT). Many popular organizational types are defined in the literature [3]-[6] In this work, we focus on three types of organizational structure: centralized, federated and decentralized organizations [2]. Centralized organizations lean towards a vertical style of coordination [7], characterized by formal authority, standardization, and planning. Decentralized organizations lean towards lateral coordination, characterized by meetings, task forces, coordinating roles, matrix structures, and networks [7]. An example of decentralized organization is a collaboration of partners working on a concrete set of problems (e.g. research collaborations, virtual labs) or forming in response to a particular customer need or market situation (e.g. virtual organizations, coopetitions [8]). Besides this collaboration, missions and objectives of each partner can be completely different and even concurrent. Federated organizations combine characteristics of centralized organizations (e.g. centralized planning, standardization, etc.) and decentralized organizations (e.g. local leadership, competitive local objectives, etc.). One example of federal organizations is a research institution that is formed by multiple schools, centers, and labs. 2.2 Enterprise Architecture (EA) The role of EA discipline is to provide the organizations with a roadmap for creation and evolution of their information systems. EA of an organization changes and grows together with the organization, its structure, vision and operating model [9]. Fig. 1. Enterprise Architecture of an organization contains three interrelated parts: EA Method , EA Description and EA Engine . EA “defines the underlying principles, standards, and best practices according to which current and future activities of the enterprise should be conducted” [10]. EA methodology and tools produce artifacts to specify the current state of a company’s architecture (“as-is”), the target architecture (“to-be”), identify how to best cross the gap between them (architectural roadmap), and set up the standards and rules to follow during this transformation (EA principles). These elements are often addressed in literature as EA description; the process that an organization has to execute in order to obtain its EA description is called EA method (Fig.1). A traditional EA project consists in implementing an EA method and producing an EA description. To assure that the organization will continuously follow the EA principles and achieve the designated goals (architecture “to-be”) a third element has to be defined: EA engine. The presence of this element in our model in Fig. 1 reflects the fact that EA is not static: it makes the organization change while changing itself over time. Dedicated structures and procedures have to be defined in an organization in order to continuously steer this organization towards its target architecture. 2.3 IT Governance According to [17], IT Governance is a subset discipline of corporate governance focused on information systems and their performance and risk management. The discipline describes how people authorized over some domain of business should consider IT in the monitoring, control, and improvement of the business. Architecture governance is a key aspect of IT Governance – it is responsible to create and manage policies for the structure and content of IT in an organization, and to enable their reuse in the form of best practices. Service Oriented Architecture (SOA) governance is a well-known example where the architecture, i.e. SOA and further up an EA that incorporates SOA, drives IT governance to ensure service orientation. 2.4 Peer-to-Peer In [2], we claim that the structured and disciplined approach to IT evolution not necessarily has to rely upon IT centralization: novel EA concepts are needed to ensure the harmonization of development and evolution of IT with the properties of decentralized and federated organizations. We argue that peer-to-peer is a relevant concept to decentralization in EA for two reasons. First, units in decentralized organization are able to contribute to the enterprise in a manner that is completely up to them. This is similar to peers in a peer-to-peer system, where the peers participate in a voluntary manner. Second, the challenge that peer-to-peer systems overcome is similar to decentralized organizations: “to figure out a mechanism and architecture for organizing the peers in such a way so that they can cooperate to provide a useful service to the community of users” [11]. Therefore, we consider peer-to-peer principles [12], [13] applicable to EA for enhancing their support of decentralization. 3 Organizational Structure and EA The objective of EA methodologies created in early 1990s was to align the IT capabilities with Biz needs via IT centralization. The main price to pay for IT centralization was the loss of flexibility and the inertia in decision making in IT. By that time, however, this was much less critical than to make the IT \"disciplined\" and to justify the investments in IT. Today, the flexibility in IT becomes more and more strategic. For modern organizations with transparent boundaries, it is simply impossible to centralize IT for literally independent partners. On the other hand, it is still crucial to maintain \"disciplined\" approach in IT evolution so that the partners not only remain independent but could also efficiently work together as a \"virtual whole”. Fig. 2: IT Organizational structure, IT Governance and EA form a triangle where EA relies upon the IT Org structure. The works presented in [4], [14], [15] and [16] focus on the relation between the structure of an organization and its IT. Following these works, we claim that the notions of Organizational Structure (in IT), IT Governance, and EA are interrelated: IT governance is defined by the IT Org. Structure and has to comply with the vision of Architecture to-be and the EA principles; EA principles, in turn, should reflect the style of IT Organizational Structure. This relations form a triangle as shown in Fig. 2. The question is: how EA should reflect the change in the IT Org. Structure in order to support the \"disciplined\" IT evolution? Upon what alternative mechanisms EA should rely when centralized strategic and resource planning is getting replaced by local planning; does central management replaced by the management on the operational level and centralized coordination and top-down decision making gives its way to self-organization and ad-hoc partnership? To answer these questions, we define the concepts of center and steering forces (Section 3.1), and using these concepts, we represent the three types of organizational structures (Section 3.2). Than we formulate the problem related to mismatch between the organizational structure and the EA in use (Section 3.3). 3.1 Concepts for Reasoning about Decentralization We consider three generic forms of organizational structures: centralized, federated and decentralized. We focus on the elements of these structures that impact the definition (EA method) and then implementation (EA engine) of the EA principles driving the organization to its target architecture: the center and the steering forces. We define Center as a part of organization (a person, a group, or a unit), which plays the role of a leader, supervisor or coordinator, and possess some power to steer the other parts of the organization. Center can be implicit or explicit. Organizations with centralized IT (Fig. 3-a), have explicit center (e.g. EA department; EA steering committee etc). This center initiates, supervises and validates the changes in the organizational IT and in the EA itself. It steers all the organizational units by setting rules and checking for compliance. We can also say that there exist steering forces between the center and the non-central units. Fig. 3: Three types of organizational structure described with the notion of center and steering forces. Organizational units are depicted with filled circles. A circle in the centre stands for the “Center”. The arrows relating the circles depict the steering forces. Steering forces can be defined as explicit and implicit protocols, policies, rules and procedures regulating the flow of communication and control between organizational units. These forces can be characterized by their direction (top-down, bottom up, sideways) and their strength. In organizations with centralized IT, the strong steering forces connect the center with the other units forming a hierarchy (radial forces). In Fig. 3-a, a simple model with two levels of hierarchy is presented. These forces can be both top-down (supervision, decision making, task/resource planning) and bottom-up (local initiatives leveraged to the center for approval). In federated organizations (Fig. 3-b), the center remains explicit but the radial steering forces connecting the center with other units are weak since decision-making and prioritization in IT can be also done locally. On the other hand, sideways - steering forces appear in this model since more and more interactions are joint projects emerging locally, between units and without passing by the center. In decentralized organizations (Fig. 3-c), the center disappears (or becomes implicit) and neither overall commitment to a given set of EA principles nor centralized control over IT evolution can be ensured. The only type of steering forces that makes the organizational IT evolve is strong sideways forces. 3.2 EA for Centralized, Decentralized and Federated Organizations Based on the conceptual representation of the three organizational types from the previous section, we explore how these characteristics of centralized, federated and decentralized organizations can be reflected by the EA methodologies. Centralized organization: EA Method should set up a structure aligned with the structure in Fig. 3-a: to define a project leader or a sponsor (center) that will occupy a high hierarchical position in the organization and will automatically provide the top-down steering forces (decision making, resource allocation); to identify data/process owners in the local units that would provide the bottom-up steering forces and actively participate in the EA creation; to assign responsibilities and define protocols that would help to reach a consensus about the EA description to produce (radial steering forces). EA Description has to focus on company-wide, long-term master plan for IT development that fits the global vision of the organization. EA principles have to define a single standard to be followed by all organizational units. EA Engine, similarly to EA method, needs an explicit center (controlling authority) and strong radial steering forces (protocols, instrumentalized processes and resources to ensure compliance with EA) to be defined. The center will steer the organization by promoting initiatives, making decisions and validating results. The organizational units will leverage their initiatives to the organization level (bottom up) for further approval. Federated Organization: EA Method should set up a structure aligned with the structure in Fig. 3-b: define a project leader (center) who will ensure the alignment between the EA project and the objectives of the organization. Since the radial steering forces are weak and can only partially ensure communication and coordination of efforts between organizational units, no centralized control or validation of EA description can be achieved. Therefore, sideways steering forces have to be developed in to complement the lack of radial steering forces. Within an EA methodology, new protocols for negotiation, information sharing and cooperative decision making have to be elaborated. EA Description has to focus on company-wide, short-term master plan for IT development that fits the global vision of the organization. EA principles should support variability in processes and resources instead of a single standard that “fits all”. For example: the central unit decides on generic process and resources, but the units implement their own variants. EA Engine should rely upon both center and local leadership and define two types of steering forces complementing each other. EA methodology has to specify tools and activities based, for example, on the peer-production principles, and supporting both centralized and user-driven (collaborative) change management. Decentralized Organization: EA Method should set up a structure aligned with the structure in Fig. 3-c, where no center is explicitly defined and only cooperation-driven sideways steering forces are enabled. While possibly maintaining their own, local EA, the partners in a decentralized organization has to be able to “connect” their architectures and to achieve interoperability. EA methodology should provide metrics for assessing the interoperability and alignment between local EA and global EA. EA description has to focus on local short-term master plans for IT development that are aligned with the objectives of an organization (a partnership). Organizational EA principles should support variability in processes and resources allowing the partners to implement their own variants of a given process with respect to their local architectures and local EA principles. EA Engine should rely upon strong sideways forces, where EA methodology has to specify tools and activities supporting user-driven change management. 3.3 Mismatch Between the IT Organizational Structure and the EA on Place Based on the theory above, many practical problems related to the EA implementation can be explained by a mismatch between the IT organizational structure and the EA in place. In particular, we identify two types of problems: ─ Problem A: IT initiatives fail and decisions in IT become inefficient when federated/decentralized organization uses the EA that (still) relies on centralized coordination and control; ─ Problem B: Poor or no strategic alignment can be guaranteed when centralized/federated organization relies uniquely on local leadership and implements solutions that require purely decentralized management. We reformulate these problems in terms of misbalanced steering forces in the organization. This leads us to a solution that can be summarized as follows: Problem A: A misbalance between the organization with weak or non-existing radial (top-down and bottom-up) steering forces and its EA that relies upon strong radial forces only; The solution is to revise EA Method and EA engine by involving sideways steering forces that would compensate the lack of radial forces. More concretely, the organization has to replace some (al for decentralized organizations) mechanisms of centralized control and coordination by their decentralized equivalents (e.g. cooperative decision making, peer-production etc) Problem B: A misbalance between the organization with weak or non-existing sideways steering forces and its EA that relies upon strong sideways forces; The solution is to revise EA Method and EA engine by involving radial steering forces that would compensate the lack of sideways forces. More concretely, the organization has to reinforce the mechanisms of centralized control and coordination. In the section below we illustrate our theory on the case of an organization for Higher Education reflecting a federated organizational structure. 4 Case Study We have analyzed a prominent university for higher education in Sweden. As common, the university includes a number of units - faculties, and faculty departments. Nowadays, the units are becoming more independent than before due to several factors: Geographical dislocation. Some faculty departments have been moved out of the • main university campus. An example is the Computer and Systems Sciences department located in Kista, the leading Swedish IT cluster. This proximity enables cooperation between IT companies and students through mentoring programs, internships, graduate work opportunities, guest lectures, etc. Decentralization of management. Coordination and decision-making are through • delegation pushed down to the faculties and further to faculty departments. Concretely, the decisions are delegated by the principal to the faculty boards and deans, and to the faculty departments’ heads and their administrations. Both formal and informal communication patterns. Formal hierarchical • communication from the faculty to its departments and informal direct communication between the departments are present. For example, the administrative tasks (e.g. registration for graduate courses, or postgraduate research etc) is primarily formal, whereas the course curriculum can be established between departments cooperatively, using informal communication links. According to the theory presented in Section 3, the organization above is a federated organization with explicit center and both radial and sideways steering forces defined. Below, we present some examples illustrating IT projects conducted by the university and the difficulties encountered. We will explain these difficulties using our theory and demonstrate that their origin is a mismatch between the organizational structure in place (federated) and the EA engine exploited for making decisions/developing solutions. Example 1: Room reservation (over-centralization). The central (university) IT department has purchased a packaged IS to be used for room booking university wide. Some departments already had their local solutions for room booking, which were better adapted for their needs. As a result of this initiative, the departments ended up paying for the new system (due to centralized budgeting) but kept their own system and refused integration (due to decentralized decision making). This example illustrates the Problem A from the previous section - A mismatch leading to inefficient and finally abandoned solution. The decision about purchasing the university-wide system relied uniquely on radial forces (centralized), whereas sideways forces (negotiation with departments, collaborative requirements gathering, etc) have not been exploited at all. Example 2: Publication cataloguing (over-decentralization). In the past, some faculty departments developed local IT solutions for cataloguing research publications. Over time, this multitude of local solutions became problematic due to numerous mappings and data inconsistencies. Recently, the university brought the decision to allow the faculties and their departments to continue to store and assess publications’ data in the way that suits best to them, while requiring a workable mapping to a central catalogue structure that follows the required standard regulated on the state level. The coordination and decision-making here exploited the sideways forces only. Since the publications meant to reflect a common face of the university - their central management using radial forces was desirable. In organizations with the federated structure, the problems above can be avoided if the EA methodology properly integrates the decision-making patterns that rely on both radial and sideways steering forces. In the first example, the centralized EA principles have been implemented (whereas the real organizational structure is federated). The correct solution would be to exploit both radial forces and sideways forces (to involve the departments into cooperative requirements gathering, solution evaluation etc). In the second example, in contrast, the decentralized EA principles have been implemented. The correct solution could be, for example, to centrally define a common standard for publications (radial forces) and to let the faculties implement this standard in the way that fits their local architectures. 5 Conclusion and Future Work This paper outlined the challenges related to increasing demand in process flexibility and the emergence of novel generation of organizations with transparent boundaries. To meet these challenges, the IT structure of organizations has to change: the centralized organizations characterized by strong top-down coordination and control, now tend to move towards more agile (decentralized) structures, where new communication, coordination and decision making patterns are used. We claim that the structure of organizational IT not only defines the IT Governance style of the organization, but it also has to be explicitly reflected by the Enterprise Architecture of the organization and supported by an EA methodology. In this work we defined the concepts of center and steering forces and modeled organizations with different degree of centralization in their IT: centralized, federated and decentralized. Using these concepts, we identified the problems that result from mismatch between the organizational structure and the EA in place. As we explained in Introduction, the work follows the Design Science IS research framework [18], in the problem definition, the use of relevant knowledge base, development of main research artifacts - the two constructs (center and steering forces) which allowed us to identify the problems related to a misfit between IT organizational structure and EA in use, and to evaluate in on an real case in the Swedish Higher Education sector. For the future work, we plan to elaborate on the proposed concepts and to identify metrics that would allow us to assess the centralization/decentralization more precisely (to measure the strength of steering forces, etc.). We also envisage to study the concrete mechanisms and patterns for communication, coordination and decision making in centralized, decentralized and federated organizations, and to see how they can be transformed into concrete EA principles or explicitly integrated into EA methodologies. For example, process variability as a mechanism to handle local differences while complying with global standards in federated organizations.",
  "2013-SRZN": "I. INTRODUCTION\nOrganizations with rigid centralized management style\nfail to sustain dynamic environments due to their inertia\nin decision making and lack of agility. Political, social\nand economic systems progressively transforming to distributed, network, and novel organization forms accordingly\nare emerging [1]. Recently coined terms such as “proactive\nenterprise” or “liquid enterprise” describe the nature of\nsuch organizations. Transparent or dynamically changing\nboundaries, agile processes, interactions aligned with realtime business goals, and virtual collaborations are all ITenabled capabilities of emerging organization forms [2].\nIn [3], organizational structure is defined as “institutional\narrangements and mechanisms for mobilizing [...] resources\nat all levels of the system”. The changes to management\nand operational styles brought on by decentralization requires major changes in organization processes and heavily\ninvolves IT.\nWhile emerging technologies serve as the main catalyst\nfor organizational transformations, utilizing the right technologies and evolving thus to digitized business processes to\nautomate organizations core capabilities [4] – is primordial\nfor organizations.\nTraditionally, this is addressed by the enterprise architecture (EA) discipline. EA “defines the underlying principles,\nin Decentralized Environments:\nArt\nZdravkovic (cid:3) and Selmin Nurcan y\nand Systems Sciences\n15, Kista, Sweden\njelenaz@dsv.su.se\nen Informatique\n90 rue Tolbiac, 75013 Paris, France\nnurcan@univ-paris1.fr\nFigure 1. Enterprise Architecture\nstandards, and best practices according to which current and\nfuture activities of the enterprise should be conducted” [5].\nEA methods and tools produce artifacts to specify the current\nstate of a company’s architecture (architecture as-is, specify\nthe target architecture (architecture to-be), and identify how\nto best cross the gap between them (architectural roadmap).\nThese produced artifacts are often addressed in literature\nas EA description; the process that an organization has to\nexecute in order to obtain its EA description is called EA\nmethod (Fig.1). A traditional EA project, though, consists\nin implementing an EA method and producing an EA\ndescription. To assure that the organization will continuously\nfollow the principles and achieve the designated goals after\nthe termination of the EA project a third element has to be\ndefined. We call this element EA engine.\nCreated in early 1990s, the de-facto EA methodologies\nsupport organizations in creation and evolution of their IT in\nstructured and disciplined way: they focus on centralization\nof IT and its tight binding to the organizational structure and\nproperties (e.g. centralized management). Such properties,\nhowever, do not necessarily exist in decentralized organizations. Consequently, implementation of these methodologies\nin organizations of 2010s becomes difficult and inefficient,\nand the role of EA as a driver for IT transformations is\ncompromised.\nAs an example, consider a public organization acquiring\na software system with the objective of integrated facility\nmanagement across divisions. Divisions were not involved\nin the decision making process (due to centralized strategic\nplanning) and eventually refused to shut down their local\nsystems and switch to the global one (due to decentralized\nIT management). As a consequence, the strategic initiative\nfor integration failed; the divisions protected their interests\n(local systems tailored for their needs), but were still charged\nfor the acquired system they never used (due to centralized\nbudgeting). This example demonstrates a mismatch between\nthe architecture principles (tendency towards integration),\narchitecture engine (the process of prioritizing, evaluating\nand eventually deciding on the global system to adopt) and\nthe organizational structure that inherits some centralized\nmechanisms while being strongly decentralized.\nWe claim that structured and disciplined approach to IT\nevolution not necessarily has to rely upon IT centralization.\nTherefore, novel EA concepts are needed to ensure that the\ndevelopment and evolution of IT is harmonized with the\nproperties of decentralized organizations.\nIn this paper, we analyze various forms of organizational\nstructures presented in literature, outline the characteristics\nof decentralization, and identify the challenges related to\ndecentralization in organizational IT. We define decentralization in organizational IT as a continuum with three phases:\nCentralized IT, Federal IT and Decentralized IT. The main\ncontribution of this paper is an assessment of the three\nprevalent EA methodologies (TOGAF, Zachman and FEA)\nand their capacity to support the decentralization. These\nthree frameworks were chosen as they are highly influential,\nas evidenced by their extensive coverage in literature, for\nexample [5]–[9]. Furthermore, Sessions [9] identifies them,\nalong with the Gartner Methodology, as making up about\n90 percent of the field.\nThis assessment finally leads us to a set of recommendations for decentralized-aware EA. This paper is part of\nresearch work still in its early phases, and as such, empirical\nvalidation has not yet been done at this stage.\nThe remainder of this article is organized as follows:\nIn Section II we discuss the role of EA in organization and provide an overview of three EA methodologies:\nTOGAF, Zachman and FEA; in Section III we discuss\ndifferent forms of organizational structure (generic and ITspecific) presented in the literature focusing on their degree\nof (de)centralization. We outline the main characteristics\nof decentralization and highlight the challenges related to\ndecentralization in IT. In Section IV we examine how\nthe presented EA methodologies support (de)centralization.\nIn Section V, we propose a set of recommendations for\ndecentralized-aware EA.\nII. ENTERPRISE ARCHITECTURE FRAMEWORKS\nA. A Common Perspective on Enterprise Architecture\nWhile there is no singular agreed-upon definition for EA,\ndifferent definitions [4], [5], [8]–[12] do have much in common. EA is a discipline that takes a holistic, design-oriented\napproach to transforming high-level business vision and\ngoals into the integration of an enterprise’s organizational\nstructure, business processes, and information systems. This\ntransformation involves identifying and implementing the\nnecessary change for this to occur. This paper will break the\nframeworks down into three separate components (Fig.1):\nEA method, EA description, and EA engine.\nThe Method aims to lay the groundwork for the EA\nproject. Typically, this involves setting up teams, responsibilities, and the overall process of collecting and approving\nthe EA artifacts (e.g. as-is and to-be architectures) which\nform the second component, the EA description. The Engine\ninvolves setting up a support structure for ensuring the\nongoing adoption of the to-be architecture. The remainder\nof this section will look at three different EA frameworks\nfrom the perspective of of these three phases: The Open\nGroup Architecture Framework (TOGAF), the Zachman\nFramework, and the Federal Enterprise Architecture (FEA).\nB. TOGAF\nThe Open Group Architecture Framework (TOGAF) is an\nEA framework created by The Open Group.\n1) EA Method: TOGAF includes a very detailed EA\nmethod, called the Architecture Development Method\n(ADM), made up of a preliminary phase and eight core\nphases (labeled A–H) [12, Ch. 5-15]:\nPreliminary\nestablish initial commitment and governance;\nA set a high-level vision for the future architecture (includes management approved goals and\nrequirements) to create a “Statement of Architecture Work” for forming contracts and obtaining\napproval from project sponsors;\nB–D create the as-is and to-be architectures, and analyze\nthe gap between them;\nE–F create plans for crossing the gap;\nG–H concerned with implementation, ongoing governance, and change management.\n2) EA Description: TOGAF views architecture from\nthe perspective of four different architecture domains [9]:\nbusiness (processes and functions), application, data, and\ntechnical (specific infrastructure).\nThe various architectural artifacts in TOGAF are organized across an Architectural Landscape [12, Ch. 20].\nHere, TOGAF specifies three levels of detail: strategic, for\noverall direction setting at the executive level; segment, for\narchitectures at the level of a specific project or program; and\ncapability, for governance related activities. Additionally, the\nArchitecture Landscape can be partitioned for supporting\ndifferent organizational units [12, Ch. 40].\nTOGAF’s Architecture Content Framework describes the\noutputs of the architecture efforts from the ADM. This\nincludes the “deliverables” that are specified in the contracts\nand subject to formal approval [12, Ch. 33].\n3) EA Engine: TOGAF outlines an ADM phase concerned with the ongoing change management process for\nthe architecture of an enterprise [12, Ch. 16]. In this phase,\na governance body sets criteria for determining if a change\nrequires an architecture update if a new cycle of the ADM\nneeds to be started.\nTOGAF describes a formal review process for determining compliance with the goal to “first and foremost,\ncatch errors in the project architecture early, and thereby\nreduce the cost and risk of changes required later in the\nlifecycle” [12, Ch. 48].\nTOGAF outlines a formal approach to architecture governance led by an “Architecture Board” [12, Ch. 47]. The\nTOGAF Architecture Governance Framework [12, Ch. 50]\nsuggests guidelines for developing a formal architecture\ngovernance structure.\nC. Zachman\nThe Zachman Framework was the first EA, first introduced by John Zachman in 1987 [9], [13]. It consists only\nof a taxonomy, and as such only fits into the EA Description\naspect of EA.\n1) EA Description: The Zachman Framework breaks\ndown EA into a grid of perspectives. Each perspective is\ncharacterized by its target audience and the issue it is aimed\nat. ZF covers six issues: What (data and entities), How\n(functional), Where (locations and networks), Who (people\nrelationships), When (events and performance criteria), and\nWhy (motivations and goals) [5]. Each issue is viewed from\nsix different perspectives: executive, business management,\narchitect, engineer, technician, and enterprise users.\nThe executive perspective provides an estimate of a system’s functionality and cost [5]. The business management\nperspective is a view of how an owner thinks the business\noperates [14]. The architect perspective takes a systems\nviewpoint and describes the operations and interactions of\nthe enterprise’s systems. The engineer perspective describes\nthe technology and design of individual systems. The technician perspective takes the technically detailed perspective\nof a “sub-contractor” who is implementing a system. The\nenterprise users perspective describes the perspective of the\nsystem users.\nD. FEA\nThe Federal Enterprise Architecture (FEA)is an effort\nby the federal government of the United States to create\nan EA for the entire government. FEA is a complete EA\nframework, covering all three components of EA. FEA takes\nan approach where individual organizational units develop\ntheir own architectures that fit into an overall framework of\ncommon standards and interoperability.\n1) EA Description: FEA develops architecture for segments and enterprise services. A segment is a “major line-ofbusiness functionality” [9] for an individual organizational\nunit (such as an agency or department).\nThe EA artifacts defined by FEA include baseline segment\narchitectures, target segment architectures and transition\nstrategy (for achieving the target architecture).\n2) EA Method: FEA defines a four step iterative process\nfor creating architectures for each segment and service [15]:\n1) analysis, where the baseline architecture, current problems, and vision for the target architecture are outlined;\n2) definition, where the detailed target architecture of the\nsegment and how to get there is defined; 3) investment\nstrategy; and 4) project planning and implementation.\n3) EA Engine: FEA describes “EA governance and management processes” [15, Sec. 2] to control architecture development. These process are implemented to manage standards, enforce compliance, manage collaboration between\nagencies, and approve architectures for implementation.\nFEA defines a value measurement process: “a continuous,\ncustomer-focused process relying on feedback from EA\nstakeholders and other value measures to increase the quality\nand effectiveness of EA products and services to support\nbusiness decisions” [15, Sec. 5].\nIII. ORGANIZATIONAL STRUCTURE AND\nDECENTRALIZATION\nThis section will first discuss the forms of organizational structure defined in the literature. Second, the\n(de)centralization of current organizations and, as a consequence, their styles of IT governance will be explored.\nWe conclude this section by underpinning the challenges\norganizations have to face due to decentralization.\nA. What is a Decentralized Organization?\nMuch research has been done on specific forms of organizational structure. Taxonomies of organization forms\nare defined in [16], [17]. Classic and modern types of\norganizational structure are often recognized. Classic types\ninclude simple centralized organizations [18], bureaucratic\norganizations [19], divisional structure and functional structure. Modern types include matrix structures, flat organizations and adhocracies. New forms of organizational structure\ninclude collaborative networks, virtual organizations and\ncoopetitions.\nAccording to Robbins [20], organizational structure has\nthree components: complexity, formalization and centralization. Complexity refers to the degree to which activities\nwithin the organization are differentiated; Formalization\nrefers to the degree to which work is standardized; Centralization refers to the degree to which decision making is\nconcentrated at one point in the organization.\nFollowing Luthens [21], (de)centralization can be also\ndefined according to three factors: geographical or territorial\nconcentration of operations, functions, and extent of concentration of decision making powers. In [11], the following\ncharacteristics of centralization are defined: the allocation\nof decision rights, the structure of communication lines, and\nthe choice of forms of coordination.\nIn a completely centralized organization, all decision making authority would reside with a single, top-level authority.\nIn a completely decentralized enterprise all members would\nhave equal decision making rights. Here, hierarchy manages\nthe interdependencies between the different subunits of the\norganization and often makes direct interactions and communications unnecessary [22]. Decentralized organizations\ninstead have less formalized communication lines [11], and\nmore fluid, project oriented teams [23].\nCentralized organizations lean towards a vertical style of\ncoordination [24] characterized by formal authority, standardization, and planning and control systems. Decentralized\norganizations lean towards lateral coordination characterized\nby meetings, task forces, coordinating roles, matrix structures, and networks [24].\nBelow, we will consider popular forms of organizations,\nfocusing on their degree of centralization.\nB. Forms of Organizational Structure and Decentralization\n1) Classic Organizational Structures: Pearlson and Saunders offer a thorough description of a pure hierarchical organization structure [11]: Except for the top level position, each\nposition has one superior and zero or more subordinates.\nDecision rights and communication lines are strictly defined\nand work their way down from the top. The scope of a\nposition is specialized and strictly defined by your superior\nand one works in assigned teams. The primary benefit of\na hierarchy is that the high levels of management have\nstrict governance and control over the company. Hierarchical\norganization structures are suited for stable environments.\nHierarchical organizations an be subdivided into simple\ncentralized and bureaucratic organizations: In simple centralized organizations, both strategic planning and operational\ndecision making authority belong to one person at the top.\nThis structure can be found in small and single-owner\norganizations with only two hierarchical levels. Bureaucratic\norganizations [19] are characterized by a multi-level hierarchical structure and use of standard methods and procedures\nfor performing work.\nHierarchical organizations generally divide their labor\neither in terms of common activities or in terms of output. Two organizational structures (Functional structure and\nDivisional structure) can be identified accordingly.\n2) Modern Organizational Structures: Matrix structure is\nanother popular style of organization structure [11] that can\nbe seen as a mixture of functional and divisional structures.\nIn this form, individuals are assigned two or more supervisors covering different dimensions of the enterprise. Pearlson\nand Saunders state that matrix organization structures are\nsuited for dynamic environments with lots of uncertainty,\npresumably because their authority structure allows them to\ncover multiple aspects when making decisions. However,\nlike a hierarchical structure, a matrix structure is a rigid\nconstruct with strictly defined roles, communication lines\nand decision rights. Authority still comes from the top in a\ncentralized manner, even though it becomes more distributed\namong matrix managers at the lower levels.\nA flat organization employs a novel type of structure\nwhere only one or two hierarchical levels are defined.\nValve Corporation, a software company in the video game\nindustry [25], employs such a structure and is an example of\nhigh decentralization. In contrast with the simple centralized\norganization described above, individual employees have\ncomplete freedom: Nobody reports to anyone, and everyone\nis free to choose their own projects.\nAdhocracy [11], [23] aims to discard traditional hierarchies in favor of decentralized decision rights and flexible\ncommunication lines connecting the entire enterprise. An\nadhocracy has a rapidly changing set of project oriented\ngroups that have decision making authority and other powers\n[20]. Mintzberg describes an adhocracy as “a loose, flexible,\nself-renewing organic form tied together mostly through\nlateral means” [18].\n3) Post-Modern Organizational Structures: New forms of\norganizational structure enabled uniquely by modern information and communication technologies Internet emerged\nrecently: collaborative networks [26], virtual (boundaryless)\norganizations and coopetition [27].\nCamarinha-Matos and Afsarmanesh define collaborative\nnetworks (CNs) as being composed of “a variety of entities\n(e.g., organizations and people) that are largely autonomous,\ngeographically distributed, and heterogeneous in terms of\ntheir: operating environment, culture, social capital, and\ngoals” [26]. Three common characteristics in various CNs\nare autonomy in the individual entities, a drive towards\nmeeting common or complementing goals, and the use of\nan agreed-upon framework for collaboration.\nA virtual organization is a group of independent organizations working together to achieve some goal. Coopetitions,\nas described in [27], are characterized by a complex relationship between firms where they simultaneously compete and\ncollaborate and benefit from both. These entities are engaged\ninto collaboration in response to a specific market situation,\ncustomer demand, etc. Such organization structures are\ngrounded on a sustainable collaboration between partners\nwithout any centralized control.\n4) Decentralization in Organizational IT: According to\nRockart et al. [28], changes in business and technology\nas well as progressive decentralization of organization as\na whole drives the changes in roles and structure of IT\nunits. The works presented in [28]–[31] focus on the relation\nbetween the structure of an organization and its IT.\nFulk [29] discusses the interplay between communication\ntechnology and various organizational forms. The authors\nconsider communication technologies as one of the key\nenablers of inter- and intra-organizational changes.\nIn [30], authors study how different organizational forms\naffect the knowledge transfer in organization. They claim\nthat “Organizational forms enable different kinds of motivation and have different capacities to generate and transfer\ntacit knowledge.”\nWeill [31] defines six forms of organizational structures\nin IT: business monarchy, IT monarchy, feudal, federal, IT\nduopoly and anarchy. In a business monarchy all IT related\ndecisions are made in a centralized manner by the top-level\nexecutives. In an IT monarchy, a group of IT professionals\nare responsible for making the decisions in a centralized\nmanner. An IT duopoly is characterized by two groups (IT\nand business executives) collaboratively making decisions.\nThe feudal form is much less centralized; individual organizational units are responsible for their own decisions.\nFederal IT aims to balance these through a combination of\ncentral IT and IT in the business units. Anarchy is a style\nof governance where small teams or even individuals are\nresponsible for their own decisions.\nMany organizations today tend to combine both centralization and decentralization in order to obtain the advantages\nof both styles: global integration and efficiency due to\ncentralized management in some key areas, and agility and\nhigh quality of local customer services due to decentralized\ndecision making in others [28].\nFigure 2 maps the organizational forms presented above\nto an axis depicting the degree of (de)centralization.\nFor the purpose of our study, we consider three types\nof organizational structure in IT: Centralized IT, where\nall IT related decisions are made in a centralized manner\nby the top-level executives, Decentralized IT, where each\norganizational subunit manages its own IT in a completely\nautonomous and independent manner, and Federal IT that\ncan be seen as a combination of central IT management and\nIT management in the subunits.\nC. Challenges of Progressive Decentralization in Organizational IT\nThe emergence of decentralized organizational structures\nmeans significant changes for organizations that adopt them.\nHowever, this transformation is not a mere question of\n“flattening” the organization by shifting authority and decision making power from the top to the bottom. In classic\norganizations, not only does hierarchy ensure control and\nFigure 2. Organizational taxonomy: From Centralized to Decentralized\ncoordination, it also manages interdependencies between\ndifferent subunits of an organization and often makes direct\ninteractions and communications unnecessary [22].\nTherefore, the main challenge related to decentralization\nand “weakening hierarchy” is a lack of interaction and\ncommunication between organizational subunits.\nThe decentralization of an organization and subsequent\nchange to its management and operation style requires\nmajor changes in organization processes and heavily involves IT. Caruso, Rogers and Bazerman [32] highlight\nthe importance of information sharing and coordination for\nthese organizations. According to [28], a major risk of IT\ndecentralization is lack of synergy and integration due to a\nlack of standardization.\nEnterprise Architecture is a discipline that allows an\norganization to develop and evolve its IT in a manner\nharmonized with it. It provides methodology and sets up\nstructures for assessing current state of IT (architecture AsIs); for planning, agreeing upon and communicating its\nfuture state (architecture To-Be); and for carrying out this\ntransformation. In order for this harmonization to be successful in decentralized organizations, it is important that EA\nmethodologies and structures acknowledge decentralization\nand help the organization to tackle its related challenges.\nIn the next section we examine how decentralization in\nIT is addressed by current EA practices.\nIV. EXISTING EA PRACTICES TO SUPPORT\nDECENTRALIZATION: ANALYSIS AND SHORTCOMINGS\nThe field of Enterprise Architecture (EA) emerged in order to combat two increasingly prevalent problems facing enterprises: system complexity and business-IT alignment [9].\nAs enterprises rely more and more on information systems\nof increasing complexity, these problems become even more\nimportant.\nA. TOGAF\n1) Concepts supporting a centralized organization:\nEA Method and EA Engine: TOGAF’s approach to\narchitecture governance involves an Architecture Board\n“to oversee the implementation of the [architecture] strategy” [12, Ch. 47]. This board has an important role in\nArchitecture Governance, such as “[p]roviding the basis for\nall decision-making with regard to the architectures” [12,\nCh. 47] and enforcing architecture compliance.\nHaving a single entity responsible for high-level decision\nmaking suits well for organizations with strong centralization in IT (Centralized IT to Federal IT in Fig. 2).\nTOGAF does suggest that the board has enterprise-wide\nrepresentation [12, Ch. 47] which may support some level\nof decentralization, however it suggests the representation\ncomes in the form of “senior managers”; a concept primarily\nfrom traditional organization structures.\nThroughout TOGAF, references are made to the existence\nof a bureaucratic or hierarchical centralized structure in\nplace; some examples:\n(cid:15) A formal governance framework for all architectural\nmaterial is specified in the preliminary phase, a concept\nrelated to the rigid forms of traditional organizational\nstructure.\n(cid:15) After the completion of “Phase A: Architecture Vision”,\nTOGAF requires approval of the current vision of the\narchitecture. This requirement assumes there is someone with a higher level of decision-making authority to\ngive approval.\n(cid:15) The set of strategic architectures from the Architecture\nLandscape which is meant for the “executive level” [12,\nCh. 20].\nEA Description: TOGAF suggests the development of\narchitecture principles that “define the underlying general\nrules and guidelines for the use and deployment of all IT\nresources and assets across the enterprise” [12, Ch. 23].\nHaving a central set of principles that is to be applied to\nan entire organization supports centralization.\nTOGAF includes the concept of an Architecture Repository for storing all architectural artifacts and deliverables.\nHaving a single place to store all information is highly\nsupportive of centralization.\n2) Concepts supporting decentralized organization:\nEA Method and EA Engine: TOGAF supports decentralization through the concept of partitions. The Architecture Landscape can be divided into separate parts in order to\nsupport “federated architectures – independently developed,\nmaintained, and managed architectures that are subsequently\nintegrated within an integration framework” [12, Ch. 40].\nThis supports the idea of different organizational units developing their own individual architectures. The mechanism\nfor integrating the individual architectures under the roof of\nthe corporate architecture is not explicit.\nTOGAF indirectly supports decentralization through the\nsuggestion that it be tailored to fit the needs of the enterprise.\nThis allows TOGAF to support any kind of enterprise, but\nthe guidelines provided are minimal.\nB. Zachman\n1) Concepts supporting centralized organization:\nEA Description: The Zachman Framework aims to\nmodel a complete enterprise using a matrix of 36 elements,\nwith alignment and composite integration relations defined\nbetween these elements.\nThe perspectives of Zachman Framework line up with\na bureaucratic organizational structure: the defined views\n(from executive to user) constitute an explicit organizational\nhierarchy. Clear separation between domains make this\nframework suitable for matrix organizations as well.\nThe lack of flexibility in definition of domains and views\nand the requirement to fill in the matrix is perhaps the\nZachman Framework’s main shortcoming with respect to\ndecentralization. A primary aspect of decentralized organizations is their high level of flexibility. For a decentralized\norganization where both roles and domains are not uniformly\ndefined (implicit) for sub-units, the use of the Zachman\nFramework becomes difficult, if at all possible.\nEA Method and EA Engine: Providing a schema for\norganizing architectural artifacts of an enterprise, the Zachman Framework does not imply any particular method for\ncollecting these artifacts (what we call EA Method in Fig.\n1). Neither does it suggest anything the set of structures that\nwe call EA Engine.\nTherefore, tailoring and implementation of Zachman\nframework for a concrete organizational structure depends\non experience of the EA (consultancy) team.\nTo summarize, the Zachman Framework provides a detailed taxonomy of EA artifacts that supports a hierarchical\nview on the organization. The application of this framework\nin decentralized organizations remains unclear.\nC. FEA\n1) Concepts supporting centralized organization:\nEA Description: Through the use of a common set\nof reference models, FEA prescribes standards that are to\nbe followed throughout the organization. This limits the\nflexibility that the individual organizational units have and\nmakes this framework suitable for bureaucratic organizations\nwith a high level of standardization of its processes.\nIn FEA, however, individual organizational units have\nthe freedom to develop their own architecture as long as\nit fits in to the set standards. This supports some level\nof decentralization and suits to organizations with federal\nstructure, where individual units have input into decisions.\nEA Method and EA Engine: Segment architecture development is defined by FEA as a collaborative approach\nconducted by a team composed of business experts, enterprise architects and technical experts. FEA defines a\nset of segment architecture stakeholders and their roles (\n[15, Table 2-2]) in segment architecture development. For\nexample, the role of senior management is defined to set the\nagency strategic goals. These roles naturally line up with the\ncentralized to federal organization of IT (Fig.2).\nThe mechanism for integrating the segment architectures\nunder the roof of the corporate architecture is assured\nby specific governance and management processes which,\nthough implying different stakeholders, remain centralized.\nAll steps of segment architecture development are supervised by a manager, pointing on centralized management\nand budgeting.\nTransition strategy is defined for the agency level though\nit is assessed on the global level. Governance-wide collaboration and reuse based on standards is outlined by FEA as\nan important part of EA transition strategy.\n2) Concepts supporting decentralized organization:\nEA Description: The resulting segment architecture is\npositioned by FEA as a shared vision for business and\nIT transformation within a core mission area or common\nservice. Each segment can have its own architecture that\nresponds to its business needs.\nEA Method and EA Engine: The development of segment architectures is described as a collaborative process\nbetween EA architects and other stakeholders. The focus is\nplaced on the “reconciliation” of the segment architecture\nwith an agency architecture and cross-agency initiative,\nemphasizing the importance of cross-agency collaboration,\ncommon opportunities and initiatives.\nArchitectural analysis and architectural definition steps of\nsegment architecture development involve business owners\nat the agency level who define business and information\nmanagement requirements for the segment. This ensures\nlocal interests are looked after within a corporation.\nFEA is targeting groups of independent federal agencies\nwith the objective to increase their interoperability and quality of service. Among three EA methodologies considered in\nour study, FEA is the only one recognizing the need of interand intra-agency cooperation and communication. Nevertheless, many of the concepts on which the EA method and EA\nengine of FEA are grounded remain strongly centralized.\nV. TOWARDS DEFINITION OF DECENTRALIZED EA\nThe challenge of decentralization is not a new one;\nother efforts have been able to address their view on it\nwith success. The specifics of the challenge varies between\ndomains, however there may exist general principles that can\nbe taken and applied to EA.\nOne such effort is peer-to-peer architectures, which “typically lack dedicated, centralized infrastructure, but rather depend on the voluntary participation of peers to contribute resources out of which the infrastructure is constructed” [33].\nWe argue that peer-to-peer is a relevant concept to decentralization in EA for two reasons. First, individuals in\nhighly decentralized organization are able to contribute to\nthe enterprise in a manner that is completely up to them.\nThis is similar to peers in a peer-to-peer system, where the\npeers participate in a completely voluntary manner. Second,\nthe challenge that peer-to-peer systems overcome is similar\nto the main challenge faced by decentralized organizations:\n“to figure out a mechanism and architecture for organizing\nthe peers in such a way so that they can cooperate to\nprovide a useful service to the community of users” [33].\nThis is similar to the main challenge facing decentralized\norganizations – a lack of cooperation – previously identified.\nWith EA being a potential solution to this challenge of\ndecentralization in organizations and the parallels between\nthe domains of peer-to-peer systems and decentralized organizations, we propose that peer-to-peer principles may be\napplicable to EA for enhancing their support of decentralization. This section will briefly present and discuss two\nrelevant principles from peer-to-peer.\nA. Peer production\nBenkler defines peer production as “production systems\nthat depend on individual action that is self-selected and\ndecentralized, rather than hierarchically assigned” [34]. Peer\nproduction works on the idea of the individuals willingly\ncoordinating with one another by expressing their own views\nwhile understanding the views of others.\nPeer production takes many different forms. One example\nare user-driven media sites such as Reddit and Slashdot,\nwhich follow a peer-production model for producing “relevance/accreditation” [34] on user-submitted content. On\nthese sites, the users have the ability to vote on the submitted\ncontent in order to decide on the content’s relevance or\ncredibility. Another example of relevance production are\ncrowdfunding sites such as Kickstarter where individuals\ndecide on the funding of user-submitted projects by giving\ntheir own money. Peer production is also used to produce\ncontent, such as in the case of Wikipedia, a completely userdriven encyclopedia.\nIf we view enterprises as being composed of peers, the\nidea of peer production becomes useful for EA. For example,\nthe EA Engine of TOGAF relies on an Architecture Board\nresponsible high-level decisions and governance. Instead\nof a central board responsible for making decisions, a\nmodel based on the principle of peer production for relevance/accreditation could be used instead. This would better\nsupport decentralization as decision making would then be\ndistributed amongst the peers that make the organization.\nB. Trust management in peer-to-peer\nDue to the fact that peers in peer-to-peer systems are able\nto operate in a completely independent manner, there exists\nthe problem of knowing whether or not the contribution\nmade by a peer is trustworthy or not. Consequently, some\nresearchers have proposed various methods for determining\ntrust in a peer-to-peer environment. For example, Aberer and\nTable I\nEXISTING AND PROSPECTIVE SUPPORT OF PROGRESSIVE DECENTRALIZATION BY EA FRAMEWORKS\nEA component Existing support for centralized or-\nganizations\nExisting support for decen-\ntralized organizations\nApplicable P2P principles for a solution\nEA method: Approval process based on hierar-\nchy; architecture development is coor-\ndinated, supervised and evaluated by\nwell-defined roles in a company; EA\nteams coordinate architectural work\nand communicate results; results are\ncontrolled and evaluated centrally by\nprogram manager)\nFederated architectures; pos-\nsibility to adapt ADM for a\nspecific organization; archi-\ntecture development process\ninvolves multiple stakehold-\ners\npeer production principles for creation and eval-\nuation of EA artifacts; P2P trust management\nreplacing approval mechanism\nEA description: Strategic level architectures; hierarchy\nof architecture principles; a common\nset of reference models; hierarchi-\ncal organization of EA artifacts with\nexplicitly defined roles and domains\n(Zachman)\nArchitecture partitions; archi-\ntecture reference models; seg-\nment architecture; the con-\ncept of “shared vision”\nUser-driven content submission and change\nmanagement of the content (i.e. the structure is\ndefined by the users)\nEA engine: Architecture board; formal governance\nframework; common principles for an\nentire organization (i.e global commit-\nment is taken for granted); centrally\nintegration of various (seg-\nment) architectures is assured\nby (centralized) management\nand governance\nPeer production for relevance/accreditation (e.g.\ndecision making in budgeting, strategy, opportu-\nnity evaluation, solution evaluation); user-driven\ncontent submission and change management of\nTable I\nEXISTING AND PROSPECTIVE SUPPORT OF PROGRESSIVE DECENTRALIZATION BY EA FRAMEWORKS\nmanaged architecture repository\nthe content; P2P trust management\nDespotovic [35] have proposed determining whether a peer\nis trustworthy or not based on a peers history of interactions\nwith other peers in the system. This assessment is performed\nby the individual peers, and as such, is appropriate for a\npeer-to-peer environment. TOGAF employs the idea of an\napproval process grounded on the presence of centralized\nauthority to ensure that the presented architectural material\nis valid for the enterprise. In a decentralized environment,\nthis central authority is not likely to exist. Peer-to-peer trust\nmanagement may offer a solution here: Instead of explicit\napproval, the acceptance of a peer’s contribution to EA can\nbe based on a peer’s level of trustworthiness.\nVI. CONCLUSION\nIn this study we have analyzed the problem of non-fit between emerging decentralized organizational environments\nand established EA methodologies.\nWe have argued that decentralization in organizational\nstructures and IT governance is common in many modern\norganizations. These organizations are following different\npatterns by fostering entirely new relationships between\nbusiness processes and IT. The classification of organizational forms of IT presented in Section III, was used to\nassess if the dominant EA methodologies can support them.\nCurrent EA frameworks fail to solve this major concern\nin decentralized environments. We have surveyed Zachman\nFramework, TOGAF and FEA, and concluded that the first is\nunable to support any significant aspect of decentralization.\nWhile the latter two provide some basic flexibility, in TOGAF, it is mainly facilitated by the ability to have a different\narchitecture for organizational units and by providing space\nfor new methods for the architecture development; in FEA,\nthe conclusions are similar, while the top-level organization\nstandards need to be obeyed by all units. Consequently, im-\nI\nDECENTRALIZATION BY EA FRAMEWORKS\nplementations of these methodologies are heavily limited to\nsupport new decentralized organization patterns fostered by\nvirtual organizations, collaborative networks, coopetitions,\nand others. Lastly, we discussed how the application of specific peer-to-peer architecture principles–in particular peer\nproduction and trust management–could be projected onto\nthe problem of employing EA in organizations following a\ndecentralized organizational pattern.\nThe aim of this research is to contribute to a ‘state of\nthe art’ on enterprise modeling methodologies by analyzing\nthe decentralization of organizations and supporting business\npatterns and technologies, and thereby the consequences of\nthis trend to the requirements for new approaches to the use\nand management of IT resources. Regarding future work,\nour next steps involves contrasting the presented theories and\nargumentations empirically, i.e. by mapping them to EA of\ndifferent organizations. Such an ongoing study concerns an\norganization in the public sector of Sweden, exposing many\nof decentralized behavior as discussed in this paper.",
  "2013-TMRW": "1 Introduction Today organisations rely on information technology (IT) as a support for their business goals, regardless of the services and products they provide. Business and IT alignment play a vital role in an organisation’s success. IT provides business value, so the IT strategy should be carefully developed together with the business strategy. All the work presented here is based on a case study. The case study is the ´ enterprise architecture (EA) of the Ecole Polytechnique F´ed´erale de Lausanne Camera ready for the BUSITAL 2013 Workshop 2 Gorica Tapandjieva et al. (EPFL). On the main campus there are over 11,000 people including students, researchers and staff. The work of all these people is supported by IT systems. Some of these IT systems are designed for all of the people on campus, some are used only by students and professors, and some are used only by the adminis- trative staff. In general, IT strategy includes multiples aspects spanning from technology selection to business process structure. In this project we are focusing mostly on how to best structure the relations between the business processes and the IT services, through the implementation of the architecture principles proposed by Ross, Weill and Robertson [6]. Our goal is to illustrate our proposed IT strategy. We present the goals and the plan of actions through working on this case study at EPFL. First, in Section 2, we focus on understanding the challenges. Then, in Section 3, we list the initiatives at EPFL that currently address these challenges. In Section 4, we present the basic elements that construct the IT strategy based on EA principles [6], and we suggest a way of communicating the IT strategy within the organisation. For this communication we use the Solu-QIQ tool, based on the methodology of urbanisation of IT systems (URBA), and the systemic enterprise architecture methodology (SEAM). In Section 5, we illustrate the application of our suggestions on a specific example of the business process, the process of hiring new Ph.D. students. We finish with the overall conclusions and future work. 2 Main Challenges EPFL is a large organisation (with over 4500 employees) and it faces many IT challenges. For example, currently at EPFL there are around 100 different systems (both business and infrastructure) that provide services to EPFL’s com- munity. They can be grouped in several categories. Some of these systems are listed in Table 1. The table shows systems managed by the Central Services. In addition to these systems, each laboratory within an institute or a school, has its own re- sources. The data about IT systems at EPFL is gathered by EPFL’s information system’s coordinator, Didier Rey. From this data, we see that all of the resources involved in the operation, maintenance and development of IT systems are highly distributed. Additionally, new emergent technologies regularly appear. When used, they can cause changes in the current landscape of systems. In terms of resources, the situation becomes more complicated. The number of new IT projects continu- ously increases, as well as the time required to complete them. A major challenge is to manage even the new projects with the current resources. Some of these resources are the most basic ones: electricity and cooling. Furthermore, EPFL is a complex system that has an impact by delivering services that support its three core missions: education, research and technology The Case of IT Strategy at EPFL 3 Table 1. Some of the IT systems at EPFL, grouped in several categories Mission: IS-Academia Academic management tool used by students, lecturers, re- searchers, administrative staff, etc. Infoscience Database used to archive and report works and scientific publica- tions. Moodle Learning management system and course sharing platform used as course support. GrantsBD Portal for uploading and following research and scientific equip- ments requests at EPFL Administration: SAP Finances and human resources management system IS-Academia The same academic management tool can be used for room book- ing, courses and exams schedule, grades management, etc. Sharepoint Content and document management system Infrastructure: Active Directory Directory service. VPN Virtual private network. CAMIPRO Systems around EPFL student (or staff) card with electronic chip, that allow people to use a variety of functionalities like: payment in the miscellaneous campus shops and stores without any ready cash, access to some protected rooms, borrowing a bike, etc. transfer. To ensure the quality of service (QoS), EPFL must protect its bound- aries and be efficient inside this boundary. We can achieve this efficiency by optimizing the business processes and the resources used. Indeed, the current IT systems work well. The greatest challenge appears when business processes need to be integrated across organisational units. We see integration as a problem, because people and IT systems involved in busi- ness processes are typically not in one organisational unit. When people and IT systems are in separate units, it is difficult for them to work together and communicate. EPFL has already made efforts to address all these challenges. These efforts are explained in the following section. 3 Current Initiatives At EPFL exists an information systems coordination body (Coordination des systmes d’information – CSIN) that has the mission of aligning the development of IT systems with EPFL’s strategy and of assuring the coordination between different IT units. This body has several committees, commissions and manage- ment groups. CoordAppl – Coordination des applications is one of them. CSIN organised several workshops at EPFL with target to: – define high-level strategy by reviewing the impact of the applications on the return of investments (Gartner, early 2012) 4 Gorica Tapandjieva et al. – define the set of principles on how to select a project by giving specific axioms (2011) – provide project management training (Hermes training, 2011) – build a prototype to evaluate the integration of IT systems, by using the open source enterprise service bus (WSO2 ESB) [11], a part of a master’s thesis project. – propose a service-oriented approach to a help desk (ITIL transition workshop at DIT, in progress) – identify gaps in the existing processes and find solutions for them (CoordAppl SEAM workshop on Ph.D Hiring, 2012 – present) The conclusions from these workshops apply only to their specific problems and do not offer long-term solutions for the overall IT landscape. We propose an approach that offers a durable solution for the organisation of IT systems at EPFL. The concrete example upon which we elaborate our work is based on the progress and outcome of the ongoing CoordAppl SEAM workshop. In this workshop, we work with people responsible for the main IT systems and the heads of certain IT departments at EPFL. The combination of their knowledge, expertise and experience helps us to identify problems and solutions. In most of the workshops, we use SEAM [10, 9, 7], a family of methods for strategic thinking, business / IT alignment, and requirements engineering. We use SEAM to communicate about the organisation and the IT systems. Fur- thermore, SEAM conceptualizes different aspects of an organisation: its busi- ness, enterprise architecture and software development. Also, it shows different organisational-level hierarchies, and it enables designers to choose how to see the system: as a whole (a black box), or as a composite (a white box). In addi- tion, SEAM shows the behaviours of the systems, the properties representing the information exchanged, and the actors participating in the process. The origi- nality of SEAM is in the integration of generic system thinking principles into discipline-specific methods. [8] 4 Basic Elements of the Proposed IT Strategy In the aforementioned workshops, we address the gaps inside business processes themselves and the gaps between business processes and IT. We followed Ross’s, Weill’s and Robertson’s [6] recommendations of using architectural princi- ples. These architectural principles ”provide a rallying point for managers re- sponsible for building out and exploiting the enterprise architecture”[6], which consists of the following: 1. Core business processes 2. Shared data driving core processes 3. Key linking and automation technologies 4. Key customers The IT strategy we implement is our interpretation of these architectural principles. We explain: The Case of IT Strategy at EPFL 5 1. Core business processes – For each core business process, there has to be someone who takes care of the user’s needs, who knows every detail of the process, its rules and regulations. This person should be able to design the process and know how to change it when the business strategy changes. This is the role of a process owner [4]. 2. Shared data driving core processes – Data is an essential asset of every or- ganisation. Its integrity, quality, security and utility are the most important features. The person responsible for the data used and exchanged in a busi- ness process is the data steward [3]. 3. Key linking and automation technologies – Our key concept is service. We base our understanding of a service on a definition from systems thinking, [2] ”The service concept represents a unit of essential functionality that a sys- tem exposes to its environment”; where a system, ”is a group of interacting, interrelated, and interdependent components that form a complex and uni- fied whole”. Based on the given definition, the key linking and automation technologies represent the connection of IT systems (done by a middleware) and the connection of services supporting processes. From this, the people (and systems) who participate in providing one service or process should belong to one (virtual) unit. 4. Key customers – As many people are involved in the business process, it is very important to identify the key actors. In Section 2, we have seen the challenges EPFL faces. In this section, we pro- pose a solution that requires changes in EPFL’s enterprise architecture, which involves identifying a process owner, a data steward, key customers and a link- ing technology around the services. The change would not take place all at once, especially not when a large number of people and systems are involved. Edu- cating people about this proposed strategy is an important part of a successful change. By learning about it, people can better accept the change. Once they see that their individual goals are aligned with the business goals and they observe their contribution to the strategy, the process of educating them should be an easy task. Therefore, applying the architectural principles in EPFL’s IT strategy requires two actions: 1. Create a service-oriented enterprise architecture (EA) based on the architectural principles presented. At EPFL, there are over 50 separate ongoing projects that are part of the current IT strategy. Having synergy among these projects would improve the overall IT strategy. We use our interpretation of the architectural principles to achieve this synergy. As an enterprise is a complex system, its main charac- teristic is continuous evolution [9]. We manage this evolution by using SEAM representation of the architectural principles. In the various SEAM models, an enterprise’s evolution and the changes we propose are easily reflected. We use SEAM because it is systematic and it is recursive from business down to technology. In a traceable manner SEAM explicitly shows the services provided and the processes that implement these services. Moreover, SEAM 6 Gorica Tapandjieva et al. models service systems allowing to modify the organizational boundaries. All SEAM models are made with the SeamCAD tool. 2. Create a shared vision of the service architecture by communicat- ing about this vision with everyone involved. We use, together with SEAM, another EA approach: the urbanisation of IT systems (URBA). URBA is a methodology used mainly in France. We apply URBA’s suggestion only for using a cartography tool. For this, we chose Solu-QIQ [1], a tool successfully used by many companies (including RATP [5]). The tool offers an iterative approach for building a cartography of the information systems. As an output, the tool automatically generates a navigational web site that we use to communicate with people involved in building the systems. The advantages of Solu-QIQ over other tools is that it can be used for massive modelling and that the meta-model in the back- ground makes it fully customisable. This customisation allows Solu-QIQ to support SEAM, so we use it as a database to store our SEAM models. 5 Illustrating the IT Strategy: Example IT strategy is a very broad field. When many systems are involved, as is the case with EPFL, the IT strategy has several dimensions. This is the reason we focus only on specific parts and domains of EPFL’s business. The IT strategy we suggest can be applied, however, to all domains and functionalities. We show the work we did on one business process; it can serve as an example for working on other processes. In this section, the spotlight is on the process of hiring a new doctoral (Ph.D.) student. As mentioned before, this process is used in the CoordAppl workshop. We use a SEAM model to better perceive the anomalies and problems in the process. This process is useful for evaluation because is a common process at EPFL. From the Ph.D. hiring process, we learned which services and sub-processes can be also used in post-doc and professor hiring processes. Moreover, the problems (encountered by the actors in the process) demonstrate the need for integration of systems, alignment of IT with business process, and alignment of processes with business strategy. In Subsection 5.1, we describe the flow of specific actions in the process. In Subsection 5.2, we identify the specific problems. This is followed by the SEAM representation of the as-is and the to-be models, shown in Subsection 5.3. In Subsection 5.4, we illustrate the basic functionalities of the Solu-QIQ cartography tool. In addition, we present an example of the service view. This view is important for communicating with people about our service-oriented EA vision of the IT strategy. We complete the section with suggestions on how to apply the IT strategy based on the lessons learned from working on this example. The Case of IT Strategy at EPFL 7 5.1 Description of the Ph.D. Hiring Process Inscription: The process begins when an applicant fills an application record on IS-Academia (Table 1). Record validation is possible only if three reference letters are com- pleted on IS-Academia directly by the applicant’s references. Afterwards the doctoral program committee (including the doctoral program assistant) analyses all application records and decides who is admissible to the program. The doc- toral program assistant informs the applicants about this decision by e-mail. The doctoral program assistant also informs the lab professors by e-mail. Selection: After having identified potential (admitted) applicants, the professor organises interviews (with or without the help of the lab’s administrative assistant). After the interviews, matches are made with one professor and one student interested in working together. This part of the process is not supported by any existing system. IS-Academia is used only to insert some notes about candidates the professors are interested in, and of course, for reading the candidate’s data from the application record. Employment: The professor informs the lab’s administrative assistant of his decision, and this assistant asks the doctoral program assistant to prepare an admission letter that confirms the hiring of the selected candidate by the professor’s lab. This letter has to be signed by the doctoral program director and the professor hiring the candidate. This is the key step in the actual hiring of the Ph.D. student. The lab administrative assistant, asks the future students for the usual required docu- ments (CV, passport copy, etc.). These documents, together with the admission letter are sent to the HR assistant, who is responsible for making the contract and arranging for visa, if needed. Once the contract is ready, it is sent for sig- nature to the future Ph.D. student and a new record in SAP is made for him. From this point on, the employment record of the Ph.D. student is in SAP, and his academic record is in IS-Academia. 5.2 Ph.D. Hiring Process: Problems We identify, together with the CoordAppl workshop participants, most of the problems. These problems have two facets. The first facet shows the difficul- ties actors are facing. These actors are those directly involved in the process (professor, applicant, i.e., future Ph.D. student, doctoral school administrative assistant, HR assistant, etc.). Some of the problems are: – Physical documents get lost during the process. – The same data from the future Ph.D. student’s record is inserted by different people in two separate systems: IS-Academia and SAP. – Confidential data is easily accessible. – After the applicant has been selected, he is not aware of the status of the process, so he cannot know when something goes wrong. 8 Gorica Tapandjieva et al. – Getting the hard copy of the contract is critical for a future Ph.D. student. He needs it to apply for housing and permit, if needed. Also, he must be officially enrolled in the doctoral program to benefit from free language courses. The second facet shows the business and technical reasons these problems are difficult to solve: – Different administrative procedures and rules in EPFL’s 18 doctoral programs. – No document management system connected with IS-Academia and SAP. – Low level of coordination between the people involved in the separate units. – Not integration between IS-Academia and SAP, leading to redundancy and duplication of data and documents. – No person responsible for the business process, someone who knows the rules in depth and is able to make decisions (business process owner). The same applies to the data (data steward). 5.3 Systemic Enterprise Architecture Methodology (SEAM) We use the SeamCAD tool to build the SEAM models that conceptualizes the business process. Fig. 1 shows the as-is model of the Ph.D hiring process. We do not show the details about the execution of the process and the documents exchanged. What can be seen is the separate organisational units that provide services and participate in the process. The problems identified are caused by lack of integration between these units. The solution to this example problem is depicted on Fig. 2. Everyone pro- viding services to the process is put in one virtual integrated unit. The SEAM representation of being in one container (called working object) means that ev- eryone shares resources and is aware of what is going on inside. A + sign on an object represents what is added by the solution, and a ∼ sign represents what needs to be changed. Now, in the to-be model we have the following: – a virtual Ph.D. hiring service unit is added, which groups everything involved in providing services to the Ph.D. hiring process; – an additional role of a business process owner is attributed to the EDOC deputy dean; – a data steward is added; – an enterprise service bus (ESB) is added that links processes and services from different systems in the virtual Ph.D. hiring service unit; – all the heads of the current organisational units are now added to the man- agement of the integrated process. Fig. 2 shows the service organisation of the Ph.D. hiring process. The only remaining challenge is moving people from the as-is to the to-be, hence we use the cartography tool. The Case of IT Strategy at EPFL 9 Fig. 1. SEAM as-is model of the Ph.D. hiring process. The separate organisational units can be seen. Fig. 2. SEAM to-be model of the Ph.D. hiring process. The virtual service unit inte- grating people and systems from several organisational units can be seen. 5.4 Solu-QIQ - Urbanisation of IT Systems (URBA) The Solu-QIQ [1] cartography tool outputs a navigation web site which makes our IT strategy easily accessible and transparent. The first step is to define a 10 Gorica Tapandjieva et al. meta model, the graphical representation of organisation’s structure, showing the four basic urbanisation views: business, functional, application and infrastruc- ture. In our meta model we additionally have a service view and an organisational view. The meta model is only a database schema, so data has to be inserted into the tool in order to get an output. Data gathering requires interviewing involved people (business-process actors, department heads, IT specialists, etc.). In order to ensure that the views of the participants have been correctly captured, additional in-depth interviews are necessary. Subsequently, the output output1 is regenerated and rechecked. An example of the tool can be seen in Fig. 3. It shows a concrete example of a SEAM model: the IT systems (IS-Academia and Moodle) and actors (Alain Wegmann and Didier Rey) that provide a service (Recuperation des fichiers), projects related to this service (GED and Support ´evolution...), and the process in which the service is involved (S´election des candidat-e-s). Fig. 3. One output of the web site that the Solu-QIQ tool generates, showing the applications and actors providing a service, projects around this service, and the process that consumes that service 5.5 Suggestions From the Ph.D. hiring example, we are able to provide guidelines and describe the future work for employing the IT strategy. Here are our suggestions: 1. Construct the SEAM as-is model following a pattern based on the example shown here. Communicate with all people involved in providing a service by 1 All of the work connected with the cartography and the Solu-QIQ tool is done as a master’s thesis project by a LAMS student. The Case of IT Strategy at EPFL 11 using Solu-QIQ’s output. Through interviews, all of these people contribute to making the cartography of the IT systems. This way, they see all the people and all the IT systems involved in providing the same service. If the output does not show their view of the service, they have the power to react (during the interviews) and even modify the shape of the cartography. After several iterations of interviews, the cartography output will converge to one common view. This is the most accurate view of the EA. Then, we can construct the SEAM to-be model, identify the process owner and the data steward. For the linking technology, once an appropriate middleware is found, it can be used for other business processes too. With these actions we suggest what to do. 2. Reorganise the organisational units into one virtual unit based on the services they provide (one unit per service). We recommend service-orientation in the whole enterprise, captured by SEAM usage. After the cartography is completed, actors can continue communicating by using SEAM models. This set of actions explain why we do what we do. 3. Obtain relevant information about certain IT systems (their use and inter- action) by working on one functionality at a time. We work only on the pro- cesses that belong to that functionality. After the models have converged, we choose another business process belonging to that same functionality. This action explains how we plan to work across whole EPFL. 4. This work is currently done by a research lab (LAMS) acting as consultant, but later it will be performed by someone within the governance organization (CSIN). Appropriate person needs yet to be identified, so only in the future we can specify who will do the work. 6 Conclusions In this paper, we first present the main challenges that EPFL faces with design- ing an IT strategy and the initiatives currently taken to overcome them. Then we address EPFL’s enterprise architecture, according to our proposed IT strat- egy and our interpretation of the architectural principles. These architectural principles require that a process owner, a data steward, key linking technology and key actors are identified. We suggest service-oriented enterprise architecture and organising the units around the services they provide. These units should use a linking technology for integrating the different systems interacting in the process. After defining this IT strategy, we need to communicate and share the vision. This is achieved by using SEAM and the tools SeamCAD and Solu-QIQ. Using the Ph.D. hiring business process as an example, we describe the ap- plication of our proposed IT strategy. In several figures we show the SEAM as-is and to-be models. We illustrate a sample output of the Solu-QIQ tool and suggest steps to be taken for working on other processes. From the overall work done so far, we conclude that the communication and the adoption of our approach is the most crucial part of the project. 12 Gorica Tapandjieva et al. 7 Future Work We have presented how to optimize one business process within EPFL. But an overall IT strategy will be complete only if we address all core business processes. For the future work, we propose to iteratively follow the suggestions from Subsection 5.5 for the remaining processes. In parallel, to ensure the QoS of the provided services, we must also focus on protecting EPFL from its environment. This mainly includes working on several security standards and protocols. Finally, until now, we haven’t done a validation of our approach, but we have the full support of EPFL’s CIO to carry on with this project as a mean to have visibility and transparency on the services defined, the services’ providers and the services’ users. The validation will happen after the Solu-QIQ tool is used routinely and after we gather the first feedbacks from the people involved.",
  "2014-RKPLG": "WHAT MAKES PERFECT CASE MANAGER? 1. THE To provide a common ground for our readers, we start with a definition of Case Management: OMG defines case management as “A coordinative and goal-oriented discipline, to handle cases from opening to closure, interactively between persons involved with the subject of the case and a case manager or case team”. Case man- agement processes have multiple applications, including “licensing and permitting in government, insurance application and claim processing in insurance, patient care and medical diagnosis in healthcare, mortgage processing in banking...” (OMG 2009). Navigating a spacecraft to a distant planet can make a perfect example of case management process too. Efficient case management in industry is hampered by attempts to deal with case management process the same way as with regular business process. The main feature that distinguishes case management processes from traditional business processes is their unpredictability. Unlike a business process that, once designed successfully, can function for years following a predefined scenario, case manage- ment processes have to constantly adapt to various “unknowns”. These “un- knowns” may include client situation and needs, fashion, economical and techno- logical trends, expert skills, available equipment, environmental conditions (tem- perature, humidity, pollution, radiation), etc. Here the term Adaptive Case Manage- ment (ACM) comes to play. In the literature, Adaptive Case Management discipline is often compared to Busi- ness Process Management (BPM). From the theoretical standpoint, BPM and ACM demonstrate conceptually different views on the system design. Similarly to the “chicken or egg” dilemma, they try to resolve a question about what comes first: process or data? The view adapted by BPM is process-centered; it implies that data emerges and evolves within a process according to a predefined scenario, similarly 67 A G C M : S F ? UTOMATED UIDANCE FOR ASE ANAGEMENT CIENCE OR ICTION to a product evolving on a conveyor belt. ACM view, in contrast, implies that process scenario evolves at run time in response to emerging circumstances or case-related data. Dependence on changing data and other (possibly unknown) circumstances makes automated support for case management processes extremely challenging. There- fore, the features of a case management support system often include case artifacts organizer/repository, task scheduler/allocator, report generator, document shar- ing, business calculator based on business rules etc, while definition and assess- ment of alternative scenarios and decision making remains a responsibility of a human expert – a case manager. Let’s look in detail at what makes a perfect case manager: The first characteristic we derive based on our experience and discussions with professionals is objective and well-argued reasoning. A case manager has to be well informed about the case: he/she must take into account multiple variables in order to evaluate a situation and to decide on the further case handling scenario (e.g. an activity to perform, information to request, expertise to call for etc). He should also possess a vast experience in similar cases in order to predict possible alternative scenarios and estimate their likelihood. This resumes in the second characteristic: capability to value alternatives under uncertainty. And the last characteristic we add here is capacity to learn fast turning both good and bad experience into expertise that will eventually improve quality of manager’s future decisions. HAL 9000 could probably make a perfect automated case manager but this is still fiction; what about science? 2. FICTION SCIENCE? OR We wish to assert from the beginning: no, we are NOT going to replace a case man- agement expert by a program. However, we intend to push the limits of existing case management systems so that they could guide this expert and help him/her to learn quicker and to make better decisions faster. Imagine an IT system that drives you and your team through a case like a GPS Navigator: it collects data from the different sources, identifies your current position (i.e. case status), anticipates traffic jams (i.e. resource deficiencies or other potential risks), calculates alternatives routes (i.e. case development scenarios), values them with respect to your current situation and objectives and proposes you options (i.e. set of activities) to help you in achieving your objectives with a maximum likelihood. Compared to a GPS Navigator, the Case Navigation System (Figure 1) has to process parameters and events, whose sources can vary from customer calls, social media posts, stock market feeds and traffic reports to messages from RFID and other mo- bile sensors. Due to heterogeneity and complexity of processed information, this system has to come up with much more sophisticated navigation scenarios, too. Nevertheless such a vision is far from fiction: it is grounded on the intersection of mature scientific disciplines that we are going to examine below. 68 A G C M : S F ? UTOMATED UIDANCE FOR ASE ANAGEMENT CIENCE OR ICTION Fig 1: Case Navigation System guides an expert providing him/her with a set of activities leading to a (current) objective with a maximum likelihood. Before discussing the scientific background, however, we take a closer look at the components of the Case Navigation System we propose. The logical architecture of our system follows the principles of Event-Driven Architecture (EDA), a successor of SOA. It is based on the capacity to sense the environment, produce, detect, pro- cess the events and react to these events. Dynamic Context Manager (DCM) Similar to a GPS Navigator, the Case Navigation System operates based on accurate definition of the current state (or position) of the case in its relevant coordinate system as well as its next state. Such state is defined by values of different case- related variables that can be of the two types: a) internal or controlled by the case (e.g. absolute coordinates, speed and direction of a car, generated report, calculated price, discount, offer) and b) contextual, or produced by the case environment (e.g. accidents, traffic jams, changing weather conditions, stock market fluctuation, new evidence, change of customer’s situation). The contextual variables can change spontaneously or due to external reasons; they can have mild or severe effect on the case and therefore need to be considered in order to define the case scenario. On the other hand, considering too many con- textual parameters can make the case management unbearably tough. The idea is to select, measure and monitor “right” contextual variables at the “right” case states. This is the role of dynamic context manger (or DCM) component (Figure 1). At a given time, the context variables processed by DCM plus the values of internal case variables are transmitted to the navigation manager (NM). 69 A G C M : S F ? UTOMATED UIDANCE FOR ASE ANAGEMENT CIENCE OR ICTION Navigation Manager (NM) The Navigation Manager (Figure 1) makes navigation decisions and: it determines one (or several) plausible activity to execute with respect to the current objectives conditions1. Specifically, when such a decision is required, NM and navigation takes into account the current case state, the context of the case defined by the DCM, and the available activities (i.e. what can currently be done?). It runs a pre- dictive algorithm and calculates different scenarios of case development and their possible outcomes. Eventually it selects those scenarios that will have a highest probability to result in the desired outcome. It suggests corresponding activities and their assignment to actors from the activity/resource repository (Figure 1). As an expert uses his/her intuition or previous experience to make the right guess, the accuracy of scenario/outcome predictions of the NM can be improved by using previous experience in similar cases that is stored in the Log and history compo- nent (Figure 1). Log and History This component is the memory of the case manager: it contains descriptions of scenarios of previous cases (successful or not) and can be “mined” in order to an- swer the questions “was similar situation already happen?” and if yes, “what did we do?” and “what happened next?”. Log/history mining techniques are a valuable toolkit for determining patterns in cases and predicting further development of the case under certain initial conditions. Therefore it is important to accurately record every event, decision made and activity that follows to this log at run time. This is depicted with a dashed line tagged “feedback loop” in Figure 1. Activity/Resource Repository This element represents a catalog of activities that can be performed during the case handling and resources that can be used (actors, equipment etc.). Each activ- ity is characterized by: A set of conditions under which it can be performed (e.g. when a specific • event happens, when a certain resource is available, when the value of x is greater than…etc); this set of conditions is typically called precondition. A set of conditions or outcomes that will be produced upon its termina- • tion; this is typically called postcondition. NM uses preconditions and postconditions to determine the “right” activities to per- form in the current case state. Resources are also characterized by the conditions when they can be used. For example, actors (or human resources) are described by their skills: this enables a dynamic activity-actor assignment. Activity/resource repository can be extended any time by defining new activities and adding new resources. Algorithm Following the concept of Navigation System, we represent case management pro- cess as moving in a coordinate system where each coordinate takes values of some case-specific parameter (e.g. client income, inflation rate, time of the day, availabil- ity/amount of some resource etc.). A single point or a group of points in this coordinate system corresponds to a case state (or status) at a given moment of time. We say that “the case develops” if its We consider that objectives of the case are not fixed and can change during the 1 case handling. 70 A G C M : S F ? UTOMATED UIDANCE FOR ASE ANAGEMENT CIENCE OR ICTION state changes over time. There are three types of triggers that can lead to state change: case management activities, internal events and external (contextual) events. The examples of these triggers and their corresponding state transitions are shown in Table 1. Table 1: Examples of Triggers Trigger type Trigger example and subsequent state transition External events Applicant files complementary information about his income -> clerk processes the information -> the application gains “high in- come” status and “Priority processing” state of the case is trig- gered Temperature rises above critical -> “Emergency” state of the case is triggered Interest rate of the bank changes -> “Contract re-validation” state of the case is triggered Internal activities Manager invalidates the application -> “Demand refused” state is triggered Agent (or system) recalculates discounts -> “Contract re-valida- tion” state of the case is triggered Team executes an emergency operation procedure and reduces the temperature -> “Normal operating” state is triggered. Internal events Contract validity date has expired -> “Contract re-validation” state is triggered “Out of stock” reminder is generated by the system -> value of the corresponding “resource amount” variable is changed to “0”. The case management can be seen as navigating from one state to another, aiming to achieve some case objectives (a target state). The functionality of our Case Navigation System can be summarized in the follow- ing abstract algorithm: 1. Select the relevant case-specific parameters, observe and measure them and identify the current case state (DCM + NM); 2. Identify probable scenarios (sequences of states) taking into account, if possi- ble, previous experience (NM + Log&History); 3. Exclude the scenarios that are forbidden or not feasible for the current case according to business rules, regulations, availability of resources etc. (NM); 4. Select the scenarios that can lead the case towards its target state with the highest probability (NM); 5. Identify one or several alternative activities that trigger such successful sce- narios and recommend them to the case manager (NM + Repository); 6. Record the case manager’s decision (Log&History). The algorithm above should be repeated in a cycle, and can be triggered by a case manager requiring an assistance; • by DCM, after registering some potentially important contextual or inter- • nal event. SCIENTIFIC BACKGROUND 3. The idea of automated guidance for ACM is grounded on the intersection of several scientific disciplines. In particular, we propose to explore formal methods, formal concept analysis (FCA) and dynamic context modeling. Table 2 shows the steps of the algorithm for the Case Navigation System presented 71 A G C M : S F ? UTOMATED UIDANCE FOR ASE ANAGEMENT CIENCE OR ICTION above and indicates the research challenges and theoretical background for imple- menting these steps. Table 2: Theoretical Foundations required for Case Navigation System Step System Ele- Scientific Objectives ment DCM + NM Research objectives: Dynamic model of the Select the relevant context; Representation, capturing and pro- case-specific param- cessing of complex internal and contextual eters, measure them events; Rules for inclusion/exclusion context and identify the cur- subjects/elements into consideration; Formal rent case state Specification of case coordinate system. Disciplines: Dynamic context modeling; Com- plex Even Processing; Formal specification languages. NM + Log&His- Research objectives: Rules for state transi- Identify probable tory tions; Specification of case objectives, final scenarios (sequences states and case management scenarios; Dis- of states) taking into covery of the case abstract states and case account, if possible, management scenarios from the log and his- previous experience tory. Disciplines: Formal specification languages; Process mining; Formal Concept Analysis NM Research objectives: Formal specification of Exclude those sce- business rules; simulation and validation the narios that are not case scenarios against these rules; feasible/forbidden for current case (i.e. Disciplines: Formal specification languages business rules, regu- and model checking. lations, availability of resources) NM Research challenges: Identification of the best Select the scenarios next state given the current state; that can lead the case towards its tar- Disciplines: Graph analysis; Formal Concept get state with maxi- Analysis mum probability NM + Repository Research objectives: Specification of activities Identify one or sev- (preconditions + postconditions) and re- eral alternative activ- sources; Identification of the activities that ities that lead to lead to required state transition with the such successful sce- maximum likelihood; narios and recom- mend them to the Disciplines: Formal specification languages; case manager Formal Concept Analysis Record the case NM + Log&His- - manager’s decision tory Formal Concept Analysis (FCA) Automated guidance for ACM proposed in this paper depends heavily on the sys- tems capability to analyze, look for dependencies and classify vast amount of data considering the case, its context, states, log and history. Navigation system also has to efficiently predict the successful scenarios and recommend to a user some course of actions based on a certain criteria. Formal Concept Analysis discipline proposes a set of methods and tool for such data processing and predictive ana- lytics. 72 A G C M : S F ? UTOMATED UIDANCE FOR ASE ANAGEMENT CIENCE OR ICTION FCA is a mathematical theory relying on the use of formal contexts and Galois lat- tices (Birkhoff 1940). The use of Galois lattices to describe a relation between two sets has led to various classification methods: a Galois lattice gathers elements, which have common properties in clusters, called formal concepts. A partial order exists among these concepts, which form a lattice with an upper and a lower bound. Since their creation, Galois lattices have been used in numerous contexts to extract hidden knowledge from data. Formal concept analysis provides a universal tool for clustering the objects as well it can be used as underlying semantics for a recommenders system, providing a selected subset of elements that correspond to a certain criteria. Its joint use with formal methods and context modeling is very promising in the context of case man- agement. Dynamic context modeling The main characteristic that distinguishes Adaptive Case Management from Busi- ness Process Management is its dependency from the complex events and contex- tual parameters. The more complete information we have about the case context the more accurate the decision we can make about what to do next (Vanrompay 2011). However, too many contextual parameters can make the case management unbearably tough. Designing a context model that will adapt to the case needs by dynamically including new relevant parameters into the consideration and exclud- ing the irrelevant ones – is an important endeavor for efficient Case Navigation Sys- tem. The process context information should be acquired, modeled and formally ana- lyzed at run time in order to adapt case handling scenario and to ensure its flexi- bility (Bettini 2010). Most context models consider a subject or a number of sub- jects (e.g. a person, a physical or information object, a phenomenon etc.). Each subject can be associated with multiple context elements (location, status, etc.); for each context element, we observe values that can dynamically change and that can be described by meta-data. Both subject and context elements can be semantically described using ontologies. The subjects/elements present in a current context model depend on the activity domain related to our case. An important advantage provided by dynamic context modeling is a possibility to add/remove subjects and elements to/from the context model at run time. Complex Event Processing (CEP) Capturing and processing complex events in business organizations have been re- cently addressed by Complex Event Processing (CEP) discipline (Luckham 2011) (Bates 2012). The goal of CEP is to handle the streams of information from different sources, to identify meaningful events and to respond to them as quickly as possi- ble. Complex events can emerge at different layers of an organization and can com- bine various simpler events such as customer calls, messages, social media posts, stock market feeds, traffic reports, weather reports, etc. Integration of the CEP tech- nology into BPM has a great potential (Janiesch 2011) as it can result in develop- ment of decision-support systems and other recommendation systems for efficient business process management. Detection-oriented CEP is a class of CEP solutions focused on identification of pat- tern situations from combinations or sequences of events. Complemented by dy- namic context modeling for run-time definition of complex contextual events and by FCA for identification and evaluation of case handling scenarios that can follow 73 A G C M : S F ? UTOMATED UIDANCE FOR ASE ANAGEMENT CIENCE OR ICTION a detected pattern situation, detection-oriented CEP represent a valuable technol- ogy for automated ACM. Formal Methods Process modeling (graphical or formal) prior to its implementation is important since it helps to understand and communicate the meaning of the process. Formal modeling (or specification) is yet more beneficial: it allows a process designer to check if a given process meets specific conditions, respects (business) rules and if it can result in desired outcome. Formal specification of a case management process assumes definition of the coor- dinate system, abstract states and conditions that trigger the state transitions dur- ing a case handling. This specification can be done using a formal specification language and further validated using formal methods. In computer science, formal methods are a particular kind of mathematically based techniques for the specification, development and verification of software and hard- ware systems (Clarke 1996). The examples of formal methods include Z notation, B-method, Alloy specification language etc. Formal methods can be used for step-wise system design: they provide a formal specification of the system to be developed at different levels of details and allow for accurate refinement (transition from one level to another). The resulting formal specification can be used to verify that the requirements for the system being de- veloped have been completely and accurately specified. Along those lines, formal methods can be used to verify that the proposed case-handling scenario meets the objectives of the organization while respecting the contracts and regulations asso- ciated with it. 4. DISCUSSION The vision presented above results from an academic research of our group and its implementation only begins. Our objective is to extend the capability of ACM sup- porting systems and to transform them from useful assistants to experienced guides and reliable advisors. While trying to answer the question “what makes a perfect case manager?” we high- lighted the following characteristics: objective and well-argued reasoning; • capability to value alternatives under uncertainty; • capacity to learn fast from the experience. • The Case Navigation System pictured above focuses on these characteristics: dy- namic context manager supports objective and well-argued reasoning about the case; thanks to predictive scenario calculation and mining of previous scenarios, accurate predictions about case development can be made by NM; thanks to regis- tered log and history, scenarios can be improved in the future. So what? This is probably the main question that bothers a reader during the last couple of pages. Could these ideas, which look like a mixture between good imagination and scientific terminology, leave the research lab one day? We will try to answer this question. Our group works in collaboration with other researchers in order to provide solid theoretical foundations, algorithms and working prototypes for the Case Navigation System depicted in this article. First, we are going to create models and algorithms for a generic system, which does not consider any concrete application domain. The 74 A G C M : S F ? UTOMATED UIDANCE FOR ASE ANAGEMENT CIENCE OR ICTION next step is to create a prototype of the Navigation System for a concrete domain (e.g. banking, insurance, administration, legislative etc.). The prospective prototype, however, cannot be considered a working system until it is simulated, validated and tested on multiple, first, toy and then, real life exam- ples. Simulation and validation phase will be followed by the learning phase, where the algorithms will be tuned based on the existing cases. This will potentially im- prove their accuracy. These research and development activities are summarized in Figure 2. Figure 2: Research and development activities for the Case Navigation System Challenges The main challenges we have to face can be roughly divided in three categories: Theoretical challenges related to overall problem complexity and number • of “unknowns” to manage. Technological challenges (e.g. scalability and robustness of algorithms, • availability and quality of appropriate ICT technologies) Challenges related to adoption by the users (e.g. user perception of com- • plexity versus utility, appreciation and willingness to adopt). We give some examples related to our scientific objectives first: In spite of their effectiveness, approaches based on a formal semantics, model checking and theorem proving are rarely used in practice due to their complexity and high implementation cost. Indeed, formal specification languages are typically based on first or higher order logic: reduction of a real life problem (i.e. a case and its handling) to a logical formula represents a complex task and requires specific skills from a designer. So far, formal methods have been successfully used in design and development of safety critical systems. The main critique of the approaches based on Formal Concept Analysis is related to their scalability. Computation and update of Galois lattices are complex tasks, especially if the input data is large and evolves frequently. This can be seen as a serious drawback in development of automated guidance for ACM. To cope with this problem, several approaches based on filtering techniques have been proposed. Recent works demonstrate a possibility to generate Galois lattice from a data stream in real-time. In particular, some tools like FCA Stream Miner tool (Melo 2013), which implement this approach, have been successfully used for anomaly detection in telemetry data. Scalability of context acquisition platforms can be pointed out as an issue for con- text-aware ACM. Indeed, the DCM component of the Case Navigation System de- scribed above needs to observe an important number of elements, identify and han- dle complex business events. This potentially involves connecting multiple distrib- uted information sources. Recent solutions developed in the fields of pervasive sys- tems and sensor networks demonstrate that it is possible to handle hundreds of sensors for observing context information. Another critical challenge is related to the heterogeneity of contextual information: the DCM component has to deal with a multi-scale context, where information can 75 A G C M : S F ? UTOMATED UIDANCE FOR ASE ANAGEMENT CIENCE OR ICTION range from simple coordinates coming from a GPS to complex client or market de- scriptions coming from cloud platforms, social networks, RFIDs and personal mo- bile devices. Such heterogeneous information has to be represented using multiple scales, demanding expressive models. This is an ongoing issue considered by nu- merous research teams. Complex Event Processing addresses the issues related to handling multiple heterogeneous information sources. The problems described above mostly illustrate the theoretical challenges. Regard- ing the technological and adoption challenges we refer to Gartner’s Hype cycle. Fig- It evaluates the ure 3 illustrates the Hype cycle for emerging technologies for 20132 maturity, adoption and future direction for technologies and trends and helps us to illustrate some technological challenges. In particular, the Hype cycle from 2013 highlights the evolving relationships between humans and machines. According to this research, Predictive Analytics technologies already reached their plateau of productivity phase and are currently becoming the mainstream technology. Com- plex-Event Processing and Content Analytics technologies that also pave the road to context-aware ACM are currently on their peak of inflated expectations phase, meaning that the value of these technologies is not yet correctly estimated and their reputation is grounded on both success and failure stories. According to Gartner, both Content Analytics and CEP technologies will reach their maturity (the plateau) in 5 to 10 years (Figure 3). Figure 3: Gartner’s Hype Cycle for emerging technologies in 2013. The passage from the purely theoretical Case Navigation System described in this article to a concrete system that deals with a real life example presents a whole set of new challenges. We highlight just two of them: To select the right level of granularity, we have to identify relevant in- • ternal and contextual parameters that will affect the produced recommen- dations and determine case scenarios. This task is not easy even for a professional case manager with years of experience. The main risk is to (Source: Gartner, August 2013 http://www.gartner.com/newsroom/id/2575515). 2 76 A G C M : S F ? UTOMATED UIDANCE FOR ASE ANAGEMENT CIENCE OR ICTION get the system that is either too naïve (abstracts away too many parame- ters and produces only trivial recommendations) or too complex (produces scenarios and recommendations impossible to decipher or validate). To specify and further improve the system logic, we have to collect a • substantial amount of data on case management history (past cases). Un- fortunately, organizations often do not collect such history. Even if they do - very rarely they keep trace in a standard format that can be easily re- used/understood. In this paper, we presented the concept of Case Navigation System, which is grounded on academic research of our group. Our idea is to extend the capabilities of ACM supporting systems of today and to transform them from useful assistants to experienced guides and reliable advisors. Though a perspective of having fully automated case management sounds like science fiction and we are still far from inventing HAL 9000, some exciting functionalities such as context-aware scenario prediction and evaluation can be envisioned.",
  "2014-ZRS": "1 Introduction Enterprises have traditionally implemented formal, centralized forms of organizational structure [1], such as hierarchical or matrix structures. In these structures, communication patterns, roles and decision rights are strictly defined. This allows for management to have a high degree of control over the enterprise and therefore enforce compliance with standards, procedures and policies which results in a highly stable enterprise. However, this comes at the expense of agility; it is difficult for these organizations to quickly adapt to a changing environment. While centralized structures were appropriate for the business environments of the past, modern business environments demand a high level of agility. Common components of modern business environments include cooperation with different organizations, rapidly changing business activities and processes, and a rapidly changing competitive landscape [2]. In order to properly handle these components, a high level of enterprise agility is necessary. In centralized organizations, decisions need to be discussed at all levels of the hierarchy in order to 130 Pre-proceedings of CAISE'14 Forum obtain the appropriate justification and approval. This takes time; by the time a decision is made, it is often too late for it to be effective. In contrast, having decision making on the operational level allows for quick decisions enabling an organization to take advantage of opportunities quickly. More decentralized structures, such as networked organizations [1], are examples of this. It is important to note that a lack of rigidity and formal structure does not mean a lack of organization. It is still important for a decentralized enterprise to maintain order in its activities; the governance (and IT governance) just needs to be based on an underlying decentralized structure instead of centralized one [3, 4]. Consequently, decentralized organizations need solutions to the same problems faced by centralized organizations – such as business-IT alignment – but the solutions need to be supportive of decentralization over centralization. This can be addressed by the practice of Enterprise Architecture (EA) [5]. Today’s EA frameworks and methodologies need hence to be able to handle these environments, where rapidly changing business conditions have been identified as an important problem in EA in this context [6, 7]. For these reasons, ensuring the suitability of modern EA frameworks for decentralized organizational structures and governance which are highly dynamic, is becoming increasingly relevant. This study reports the alignment between a decentralized organizational structure and an EA in use in a real organization; elicited problems are further analysis in respect to the support from current EA frameworks, as well as from other architectural principles that were considered to be able to solve the problems. The paper is organized as follows: Section 2 reports a summary from a case study research in a Higher Education organization, in the requested STARR form: situation – task – approach – result – reflection. Section 3 provides conclusions and the directions of future work. 2 Case Study The organizational structure defines the rules according to which allocation of responsibilities and resources, coordination and supervision, is made for an organization. Three key organizational properties differentiate between centralized and decentralized organizations: geographical dispersion, coordination (authority, decision rights, standards and regulations), and communication patterns. These properties were used as the base knowledge to assess the style of the case organization, and further to analyze the IT governance rules in place. 2.1 Situation We have analyzed a prominent university for higher education in Sweden. As common, the university includes a number of units - faculties, and faculty departments. Nowadays, the units are becoming more independent than before, due to several factors: IT Governance in Organizations Facing Decentralization 131 • Geographical dislocation. Some faculty departments have been moved out of the main university campus. An example is the Computer and Systems Sciences department located in Kista, the leading Swedish IT cluster. This proximity enables cooperation between IT companies and students through mentoring programs, internships, graduate work opportunities, guest lectures, etc. • Decentralization of management. Decision rights are of the type “push-down” delegated by the principal to the faculty boards and deans, and some to the faculty departments and their groups. • Both formal and informal communication patterns. Formal hierarchical communication from the faculty to its departments and informal direct communication between and within the departments are present. For example, the administrative tasks (e.g. registration for graduate courses, or postgraduate research etc.) is primarily formal, whereas the course curriculum can be established between departments cooperatively, using informal communication links. Hence, the organization is seen having high decentralization structure tendencies. 2.2 Task The notions of organizational structure, IT governance, and EA are interrelated: EA principles should reflect the style of IT organizational structure; IT governance follows the organizational structure, and at the same time has to comply with the architecture to-be and the adopted EA principles. EA includes governance processes such as IT principles regarding operations, data, architecture, infrastructure etc. They are to an extent similar to the processes of IT governance. However, EA governs the development and implementation throughout the organization directing the evolution of the IT and business environment towards a desired design of a future (i.e. primary strategic), while IT governance handles the everyday IT operations within the organization (i.e. primary operational). The study was to analyze the aspects of university’s EA in order to assess the decentralization support provided, in contrast with what is needed; to elicit conflicts between the architecture’s principles in use, and the organizational structure and the governance rules, and thus provide a basis for the guidelines for an EA that can provide the needed support. 2.3 Approach Four separate interviews were conducted in one of university’s departments in order to get a holistic view of the way of work across the whole university. The roles of the interviewees were: vice division lead, head of postgraduate studies, head of undergraduate studies, and head of IT. The interviews were conducted in a semi- structured manner, starting with a set of open-ended questions that promote the interviewees to elaborate on their views to organization’s processes, decision making, coordination, etc. In addition, many official documents are available on the 132 Pre-proceedings of CAISE'14 Forum organizational structure, thus making a document study viable. The documents that formed this study are described in Table 1: Table 1. Documents used in the documentation study Document Description Contains descriptions of the different organizational areas of Institution’s homepage the institution as well its organizational structure Publicly available documents specify authority and delegations of Authority delegation said authority of the insinuation’s organizational units documents The official rule book of the institution detailing the rules and Rule book decisions that must be followed by the institution 2.4 Results According to the EA related literature, enterprise architectural principles are established to define the general rules and guidelines for the use of assets across the enterprise. For the purpose of this study, we have chosen to concentrate on the following adopted EA principle: ─ Integrated IT systems across the university. Owing to a decentralized organizational structure described in 2.1 and as in more details uncovered during the interviews, some decision rights are pushed down to the operational level, which for the IT-related organizational structure has resulted in a highly decentralized governance: Table 2. In-place IT governance framework Name Org. Property / Description Centralization Authority The department and the university have Coordination / Decentralized separate IT and the departmental IT does not structure report to the university IT adoption Coordination / Department IT does not dictate all IT used in Decentralized the department; research projects and centers; (department) for example, groups can develop and use their own IT systems should they desire Approval Coordination / IT projects are run by independently by Mixed groups, though they sometimes need approval (department) from the department if they are expensive IT Governance in Organizations Facing Decentralization 133 IT collaboration Coordination / Any decision to cooperate with other Decentralized departments or with the university IT is made by the departmental IT itself and is based on cooperation resulting on mutual benefit Management Coordination / “Essential” systems (e.g. administrative Centralized systems such as HR) for the whole university of “essential” are controlled by the university board. The central IT systems department is required to pay for and use these systems. Management Coordination / “Non-essential” systems (e.g. course portals Mixed and schedules) are centrally budgeted, but of “non-essential” departments are not required to use them. central IT systems Use of IT systems Informal communication patterns are used, Communication / Decentralized i.e. when changes are performed on systems, (department) they are informally spread to those who use those systems. In the practice, the governance structure described in the table has become in the mismatch with the settled EA principle to integrate IS systems. This mismatch has resulted in wasted financial resources. For example, we consider a situation outlined in the interview with the vice-head of the department which concerned the acquisition of a software system with the objective of integrated facility management across departments (i.e. “integrated systems” principle). Following the principle, a software system has been bought for university-wide use; since the principle holds for the whole enterprise, the purchase was the decision of the university-board, i.e. the departments were not involved in the decision making process. In contrast, following the decentralized IT governance in place for the use of “non-essential” software systems (Table 2), a subset of them consequently refused to shut down their local systems and switch to the global one. As a consequence, the principle of integration failed; the departments were able to protect their interests (local, decentralized systems tailored for their needs), but were still charged for the acquired system they never used. Another important mismatch comes from the fact that the centralized decision making (i.e. faculty level) uses formal, hierarchical communication patterns, while decentralized, such as in case of IT governance relies on informal communication (see Table 2) which in practice has no supporting mechanisms. Hence, important decisions on changes in IT are not well communicated (not on time, or not at all) having severe working consequences for employees and students using it. 2.5 Reflections The case has many of the properties of a decentralized organization and therefore needs an EA supportive of this decentralization. Currently this is not the case because: 134 Pre-proceedings of CAISE'14 Forum ─ The EA is implicit and does not elaborate in details the adopted principles, ─ The EA maintains some centralized principles and is therefore not fully supportive of the decentralization in place. As a consequence, IT governance initiatives fail, and decisions in IT become inefficient. Hence it has been relevant to investigate how existing EA frameworks are supportive for decentralized organizations. This question was systematically addressed in [8] where the three key organizational properties – a) geographical dispersion, b) coordination (authority, decision rights, standards and regulations), and c) communication patterns, were used to assess three wide-know frameworks - TOGAF [9], FEA [10], and Zachman Framework [11]. While the analysis revealed some support for decentralization, the main conclusion drawn is that the EA frameworks of TOGAF, Zachman, and FEA are primarily supportive of centralized (and federated) organizational structures, and therefore fail to address the demands of decentralized organizations. Zachman is unable to support any significant aspect of decentralization due to its reliance on traditional organizational roles and structures on the high centralization end. TOGAF does provide some basic support through its ability to have a different architecture for organizational units and by providing space for new methods for the architecture development; it however still mainly relies on hierarchy and central roles responsible for overall coordination and approval. In FEA, the conclusions are similar as it primarily supports federated organizational structures where individual units have their own architectures that are coordinated through centralized standards that must be followed. As shown earlier, an important property of a decentralized business environment that needs to be supported by EA is horizontal coordination (recall the IT governance from Table 2). However, the three EA frameworks primarily support vertical coordination in their governance styles. Therefore, the addition of specific guidelines to these frameworks that are supportive of decentralization would improve their support of decentralized organizations. Drawing parallels between the domains of peer-to-peer systems used to provide a mechanism and architecture for organizing the peers in such a way so that they can cooperate to provide a useful service to the community of users [12] and decentralized organizations, we think that the peer-to-peer concept may be a source of principles that could form the basis for evolving current centralization-focused EA frameworks into ones that are supportive of decentralization. ─ Peer Production: we view enterprises as being composed of peers (a peer could be individual or an organizational unit), For example, TOGAF relies on an Architecture Board responsible for high-level decisions and governance. Instead of a central board responsible for making decisions, a model based on the principle of peer production [13] for creation and evaluation of EA artifacts could be used instead. This would better support decentralization as decision making would then be distributed amongst the peers that make the organization. In the university case, the department members could produce strategy, or IT Governance in Organizations Facing Decentralization 135 budget, using peer production (such as for use of information systems). Eventually, faculty or university boards could have control/advisory roles. ─ Peer trust management: TOGAF employs the idea of an approval process grounded on the presence of centralized authority. This is to ensure that the presented architectural material is in fact valid for the enterprise. According to peer trust management [14], whether some content proposed by a peer is of sufficient quality to be included in the overall architecture, is determined by other peers. In the studied case, this principle could provide a formal mechanism for communication among peers when needed, hence avoid the situations when other peers are not informed about a new proposal (such as a change in IS use). The suggested peer-to-peer principles will seek to maintain the departmental- independence becoming prevalent at the university, while addressing the incompatible architecture components this results in. This would be accomplished through a cooperative classification of essential and non-essential systems by the departments, for example by giving each department a vote. Systems classified as essential are required to be used or integrated by the departments, while departments have the option to choose if they want to utilize systems classified as non-essential. These changes would help at reconciling differences between the architecture principle emphasized in the case without actually changing it. Decision rights are still pushed down, and IT systems are still integrated throughout the organization, but this change in IT governance at the university level addresses the conflict that can arise when a decision is made to use a decentralized system that the rest of the organization is integrating (as occurred in the current situation). 3 Conclusion and Future Work While technology serves as a catalyst for organizational transformations, it is important to utilize the right IT resources in a manner that is supportive for the organization. To accomplish this in decentralized organizations, adequate EA processes, principles and concepts are needed to be employed to both handle the IT resources and to foster business/IT co-evolution in decentralized environments. Current EA frameworks rely on organizational properties that are becoming less useful with progressive decentralization. Due to this, implementation of these frameworks in decentralized organizations becomes difficult and inefficient, and the role of EA as a driver for IT transformations is becoming compromised. In order to deal with decentralization, some changes, or additions to these EA frameworks are necessary in order to improve their support for decentralized business environments, to reflect the style of organizational structure and operational IT governance rules in place Two specific principles of peer-to-peer architectures were outlined, peer production and peer-to-peer trust management; and indicated how they could be used as potential principles for an EA that is supportive of decentralization. The reflections of this study may be of interest to three groups: the case organization, researchers in the field of EA, and, potentially, other organizations with decentralized structures interested in implementing some form of EA. For the case 136 Pre-proceedings of CAISE'14 Forum organization, the proposed EA principle of peer-to-peer might be of interest, as the application of this principle could offer some improvements to their governance structure. For researchers, this study work might be of interest as it highlights some potential issues with traditional EA knowledge, while giving some initial insights into how they could be solved. These insights are not conclusive; this research should be positioned as a starting point for future research in the topic of decentralization in EA. This work may be of interest to organizations that have adopted, or are interested in adopting a decentralized structure and are looking for the insights into how governance can be successfully done in this environment. For the future work, we envisage to propose the concrete mechanisms and patterns for communication, coordination and decision making in centralized, decentralized and mixed (federated) organizations, and to see how they can be transformed into concrete EA principles, or explicitly integrated into EA methodologies.",
  "2014-ZRSa": "1 Introduction Enterprises have traditionally implemented formal, centralized forms of organizational structure [1], such as hierarchical or matrix structures. In these structures, communication patterns, roles and decision rights are strictly defined. This allows for management to have a high degree of control over the enterprise and therefore enforce compliance with standards, procedures and policies which results in a highly stable enterprise. However, this comes at the expense of agility; it is difficult for these organizations to quickly adapt to a changing environment. While centralized structures were appropriate for the business environments of the past, modern business environments demand a high level of agility [2]. The objective of EA methodologies created in early 1990s was to align IT capabilities with business needs via IT centralization. The main price to pay was the loss of flexibility and the inertia in decision making for IT. By that time, however, this was much less critical than to make the IT \"disciplined\" and to justify the investments in IT. Today, the flexibility in IT becomes more and more strategic. For modern organizations with transparent boundaries, loose business units and agile processes, it is impossible to centralize IT. On the other hand, it is still crucial to maintain \"disciplined\" approach in IT evolution using appropriate IT governance principles so that the organizational units not only remain independent but could also efficiently work together as a whole [3], [4]. Rapidly changing business conditions and structures have been identified as an important problem in EA [5, 6]. For these reasons, ensuring suitability of EA frameworks for decentralized organizational structures and IT governance which are highly dynamic, are increasingly relevant [7]. Our research has envisioned to addresses the problem of suitable EA and IT governance principles for decentralized organizations. The three concepts in focus interrelate - EA should be compliant with IT governance by including its principles or correlating with them, and in the way to reflect a given Organizational Structure. Upon the described challenge, we have defined the following research question: Do, and if yes - how existing EA frameworks need to be extended in order to support IT Governance in decentralized organizations? Using a Design Science research method [8], [9,] including literature studies, interviews and document studies from an empirical case for data collection, and a qualitative approach for data analysis, we propose the artifacts summarizing shortcoming of current EA frameworks and formulating the requirements for IT governance for decentralized environments. The paper is organized as follows: section 2 gives an overview of different organizational structures; in section 3 a brief description of the research method and proposed artifacts is given; in sections 4, we describe the deficiencies of conventional EA frameworks for providing decentralization support; section 5 illustrates misalignment between EA principles, organizational structure, and IT governance principles on the example of one organization in Higher Education sector; based on that study, in section 6, we revisit the IT governance principles defined by Weill and Ross in [4] and provide explicit requirements for IT governance to support decentralization. Discussion, conclusions and directions of future work are presented in section 7. 2 From Centralized to Decentralized Organizations The organizational structure defines the rules according to which allocation of responsibilities and resources, coordination and supervision, is made for an organization. In order to differentiate between centralized and decentralized organizations, we consider three organizational properties: geographical dispersion, coordination, and communication patterns [10-12]. On the continuum from centralized to decentralized structures, federated organizations have emerged combining characteristics of centralized organizations, such as centralized authority, planning and regulations, with for example local leadership, as well as competitive local objectives of including business units (decentralized aspects). Table 1. Organizational properties of centralized vs. decentralized organizations. Property Centralized Decentralized Geographical Single location Geographically distributed with a dispersion reliance on IS to work together Coordination: Vertical coordination: decision Lateral coordination: authority and authority, rights are strictly defined and decision making rights decision rights, act down from the top; strict are pushed down to the level and regulations governance and control by the of business units, groups, or even upper management; rigid individuals; individuals can define structuring of accountability, their own roles and responsibilities; roles and responsibilities; heterogeneous goals; individual standardized methods and entities in the organization are procedures; homogeneous goals collaboratively working towards set by high-level authorities some common or complementing goals Communication Communication patterns Informal communication lines; patterns follow the hierarchy; direct flexible, constantly changing interactions and communica- communication lines; fluid, project- tions are not practiced oriented teams. 3 Research Method Having the desire in our research to combine literature and empirical research to develop novel artifacts addressing the problem emphasized in Introduction, we have followed Design Science (DS) research method [8], also presented in [9] In a nutshell, the method is composed of five research activities with input-output relationships: explicate problem, outline artifact and define requirements, design and develop artifact, and evaluate artifact. These activities are commonly carried out in an iterative and incremental manner to enable changes and improvements of intermediate results, as well as of final research artifacts. Adhering to space limitations of this paper, we will in our presentation mostly pay attention to the content of the artifacts and not to the DS process of achieving them; for details of the application of DS to our research, the reader can refer to [23]. The research has aimed to develop several artifacts. The first artifact presents conclusions obtained from the literature study on potential deficiencies of current EA to support decentralized organizations. The artifact has been in details elaborated in [13], while in section 4 we present its summary. The second artifact is the result of the empirical case study presented in section 5, proposing EA principles supportive for decentralized organizations [14]. The third artifact, making the use of the previous one, followed by an additional literature study, proposes a set of IT governance principles for a decentralized organizational context; it is presented in section 6. 4 Deficiencies of EA Frameworks to Support Decentralization EA frameworks include artifacts to specify the current state of a company’s architecture (“as-is”), the target architecture (“to-be”), identify how to best cross the gap between them (architectural roadmap), and to set up the standards and rules to follow during this transformation (EA principles). These elements are often addressed in literature as EA description; the process that an organization has to execute in order to obtain its EA description is called EA method. To assure that the organization will continuously follow the EA principles and achieve the designated goals (architecture “to-be”) a third element has to be defined: EA engine. The presence of this reflects the fact that EA is not static: it makes the organization to change while changing itself over time. In our research effort, the first task was to investigate how existing EA frameworks are supportive for decentralized organizations. The three key organizational properties from section 2 – geographical dispersion, coordination, and communication patterns were used to assess three wide-known frameworks - TOGAF [15], FEA [16], and Zachman Framework [17]. While the analysis [13] revealed some support for decentralization, the main conclusion drawn was that the EA frameworks of TOGAF, Zachman, and FEA are primarily supportive of centralized organizational structures, and therefore fail to address the demands of decentralized. A summarized view is provided in Table 2: Table 2. Existing support of decentralization by EA frameworks. EA Existing support for centralized Existing support for Component organizations decentralized organizations EA Method Approval process is based on hierarchy; Federated architectures; architecture development is coordinated, possibility to adapt ADM for supervised and evaluated by well-defined specific organizations; roles in a company; EA teams coordinate architecture development architectural work and communicate results; process involves multiple results are controlled and evaluated centrally. stakeholders. EA Description Strategic level architectures; hierarchy Architecture partitions; Of architecture principles; a common set of architecture reference models; reference models; hierarchical organization of segment architecture; the EA artifacts with explicitly defined roles and concept of “shared vision”. domains. EA Engine Architecture board; formal governance Integration of various framework; common principles for entire (segment) architectures is organization (global commitment is taken for assured by (centralized) granted); centrally managed architecture management and governance. repository. The important properties of a decentralized business environment that need to be supported by EA are horizontal coordination and lateral communication patterns. However, the three EA frameworks primarily support vertical coordination in their governance styles and top-down/bottom-up formal communication patterns. 5 Case Study We have analyzed a prominent university for higher education in Sweden. Our objective was to investigate the alignment between the organizational structure (including the organization of IT functions), and the EA and IT Governance rules in use. As common, the university includes a number of entities - faculties, faculty departments, and units. Nowadays, the entities are becoming more independent than before, due to several factors: Geographical dislocation. Some faculty departments have been moved out of the • main university campus. An example is the Computer and Systems Sciences department located in Kista, the leading Swedish IT cluster. This proximity enables cooperation between IT companies and students through mentoring programs, internships, graduate work opportunities, guest lectures, etc. Decentralization of management. Decision rights are of the type “push-down” • delegated by the principal to the faculty boards and deans, and some to the faculty departments and their units. Both formal and informal communication patterns. Formal hierarchical • communication from the faculty to its departments, and informal direct communication between and within the departments are present. For example, the administrative tasks (such as registration for graduate courses, or postgraduate research, etc.) are primarily formal, whereas course curriculum can be established between departments cooperatively, using informal communication links. Hence, the university is seen as having high decentralization tendencies. The study was to analyze the aspects of organization’s EA and IT governance in order to assess the decentralization support provided, to reveal the deficiencies and to formulate the guidelines for an EA and IT governance in order to overcome these deficiencies. This case is representative for the Higher Education sector: universities adopt more agile forms of organization including virtual research labs, scientific interest groups in research, joint master programs in education, and so on. As in the studied university, these examples involve geographical dislocation, decentralization of management, virtualization of communication, and use of informal communication patterns. 5.1 Approach Four separate interviews were conducted in one of university’s departments in order to get a holistic view of the way of work across the whole university. The roles of the interviewees were chosen to cover the major business activities of the institution – management, research, education, and IT support: vice division lead, head of postgraduate studies, head of undergraduate studies, and head of IT support. The interviews were conducted face-to-face in a semi-structured manner, starting with a set of open-ended questions that promote the interviewees to elaborate on their views to organization’s processes, decision making, coordination, etc.; for details of the interviews, the reader is referred to [23]. In addition, many official documents on the organizational structure are available, thus making a document study viable. The documents that formed this study are described in Table 3: Table 3. Documents used in the documentation study Document Description Institution’s homepage Contains descriptions of different topic areas of the institution as well its organizational structure Authority delegation Publicly available documents specify authority and delegations of documents said authority of the insinuation’s organizational units Rule book The official rule book of the institution detailing the rules and decisions that must be followed by the institution 5.2 Results During our study, we found that despite an evident decentralization, EA principles used by the studied organization largely rely upon centralized coordination and vertical communication patterns. On the other hand, IT governance mechanisms currently used by this organization often adhere to decentralization and thus represent a mismatch with the existing EA. This problem is a serious constraint for successful evolution of organizational IT. For the purpose of this study, we illustrate our findings on the example of one established EA principle: ─ Integrated IT systems across the university. Owing to a federated organizational structure, and as in more details uncovered during the interviews, some decision rights are pushed down to the operational level, which for the IT-related organizational structure has resulted in highly decentralized governance: Table 4. In-place IT governance mechanisms Name Org. Property / Description Centralization Authority Coordination / The department and the university have separate Decentralized IT and the departmental IT does not report to the structure university. IT adoption Coordination / Department IT does not dictate all IT used in the Decentralized department; research projects and centers; for (department) example, units can develop and use their own IT systems should they desire. Approval Coordination / IT projects are run independently by groups, Mixed though they sometimes need approval from the (department) department if they are expensive. IT collaboration Coordination / Any decision to cooperate with other departments Decentralized or with the university IT is made by the departmental IT itself and is based on cooperation resulting in mutual benefit. Management Coordination / “Essential” systems (e.g. administrative systems Centralized such as HR) for the whole university are controlled of “essential” central by the university board. The department is IT systems required to pay for, and use these systems. Management Coordination / “Non-essential” systems (such as course portals Mixed and schedules) are centrally budgeted, but of “non-essential” departments are not required to use them. central IT systems Use of IT systems Communication / Informal communication patterns are used, i.e. Decentralized when changes are performed on systems, they are (department) informally spread to those who use the systems. The IT governance mechanisms described in the table are in a non-alignment with the established EA principle to integrate IT systems. As a consequence, IT governance initiatives typically fail, and decisions about IT become inefficient. An example of immediate consequence of this is wasted financial resources: we consider a situation outlined in the interview with the head of IT of the department which concerned the acquisition of a software system with the objective of integrated facility management across departments (i.e. “integrated IT systems” principle). Following the principle, a software system has been bought for university-wide use; since the principle holds for the whole organization, the purchase was the decision of the university-board, i.e. the departments were not involved in the decision making process. In contrast, following the decentralized IT governance in-place for the use of “non-essential” software systems (Table 4), a subset of them consequently refused to shut down their local systems and switch to the global one. As a consequence, the principle of integration failed; the departments were able to protect their interests (local, decentralized systems tailored for their needs), but were still charged for the acquired system they never used. To improve the situation, the following problems need to be resolved: EA principles have to be aligned with the evolving organizational structure by − acknowledging novel modes of coordination and communication; As a part of the EA engine, the IT governance has to be transparent and aligned − with the established EA principles; in particular, it has to adequately support decentralization and to ensure efficient coordination and communication between organizational center and its sub-entities. 5.3 Recommendations: Peer-to-Peer Principles Drawing parallels between the domains of peer-to-peer systems used to provide a mechanism and architecture for organizing peers in such a way so that they can cooperate to provide a useful service to a community of users [18] and decentralized organizations, we think that the peer-to-peer concept may be a source of the principles forming a basis for evolving current centralization-focused EA frameworks into ones that are supportive of decentralization. Peer Production: we see organizations as being composed of peers (a peer could be individual, or an organizational unit), For example, TOGAF relies on an Architecture Board responsible for high-level decisions and governance. Instead of a central board responsible for making decisions, a model based on the principle of peer production [19] for creation and evaluation of EA artifacts could be used instead. This would better support decentralization as decision making would then be distributed amongst the peers that make the organization. In the university case, departments’ members could produce strategy, or budget, using peer production (such as for use of information systems). Eventually, faculty or university boards could have control/advisory roles. Peer trust management: TOGAF employs the idea of an approval process grounded on the presence of centralized authority. This is to ensure that the presented architectural material is in fact valid for the enterprise. According to peer trust management [20], whether some content proposed by a peer is of a sufficient quality to be included in the overall architecture, is determined by other peers. In the studied case, this principle could provide a formal mechanism for communication among peers when needed, hence avoid the situations when other peers are not informed about a new proposal (such as a change in IS use). The suggested peer-to-peer principles will seek to maintain the departmental independence becoming prevalent at the university, while addressing the incompatible architecture components this results in. This would be accomplished through a cooperative classification of essential and non-essential software systems by the departments, for example by giving each department a vote. Systems classified as essential are required to be used or integrated by the departments, while departments have the option to choose if they want to utilize systems classified as non-essential. These changes would help at reconciling differences between the architecture principles emphasized in the case without actually changing it. Decision rights are still pushed down, and IT systems are still integrated throughout the organization; this change in IT governance at the university level addresses the conflict that can arise when a decision is made to use a decentralized system that the rest of the organization is integrating (as occurs in the current state). 6 IT Governance Principles in Federated and Decentralized Organizations We have emphasized in the beginning that the notions of Organizational Structure, IT governance, and EA are interrelated: EA specifies architecture principles according to which both business and IT environment of the organization will evolve. Thus, it has to reflect the style of organizational structure. IT governance ensures that these architecture principles are respected by handling the everyday IT operations within the organization. In [21], the authors acknowledge that the organizational structure of a company (centralized, federated, decentralized) and its IT functions in particular affect the IT governance; the IT governance mechanisms hence need to be selected or designed taking this structure in to account. As a result of an extensive study of different organizations, in [4], Weill and Ross define 10 generic principles of IT Governance. Practice-inspired, these principles do not consider the organizational structure in-place. Upon an analysis, we concluded that some of the principles refer to structures and mechanisms adhering only to centralized organizations and require hence adaptation for federated and decentralized organizations. In particular, adaptation is needed for coordination mechanisms and communication patterns on which IT governance relies upon. Our proposed adaptations are mainly based on the concepts defined by peer-to-peer domain, i.e. peer production and peer trust management (section 5): distributed content production, peer production of relevance and accreditation, peer review process and moderation, peer produced rating, peer trust management, decentralized decision making or group decision-making [19], [20] The objective we pursue is twofold: first, we want to formulate requirements for IT governance in order to better support decentralization in organizations and, second, to provide relevant recommendations about tools to use in order to facilitate the coordination and communication (section 2). Principle 1: Actively design governance According to [4], management should actively design IT governance around enterprise's objectives and performance goals. Actively designing governance involves senior executives taking the lead and allocating resources, as well as for support to business processes. Decentralized organizations: Due to management decentralization, senior executives do not play the leading role in the process coordination and resource allocation. Instead, coordination has to be grounded on the principles such as distributed content production and group decision-making. Traditional reporting/approval process used in centralized organizations can be replaced by peer review processes and peer produced rankings. Senior executives can play the role of moderators during the content creation. Lateral communication patterns (e.g. on-line and off-line informal discussions, content sharing) have to be employed replacing formal top-down/bottom-up communication patterns based on a hierarchy. Use of social software for communication and production of relevant content is an important requirement for IT governance in decentralized organizations: traditional meetings or workshops devoted to IT governance design can be highly inefficient as they assume centralized planning and require physical presence of assigned specialists in a given location, and at a given time. In the studied case, IT governance principles supporting peer review of design are well recognized - one example is a by a unit proposed software system for thesis management; reviewed by the other units in iterations of system’s development. Hence, a next step could be to extend good practices of coordination and communication patterns for reviewing to facilitate peer production as well. To summarize: RQ1 With a lack of centralized coordination, governance design process should adhere to principles of distributed content creation and management. Recommendation: group decision-making and peer reviewing can be seen as an alternative to centralized approval process for coordination. RQ2 IT governance should encourage collaborative design, where each entity can easily benefit from and contribute to a common organizational knowledge. Recommendation: adoption and systematic use of IT and non-IT knowledge management tools. RQ3 Mechanisms supporting lateral communication patterns (informal social exchange, semi-formal discussions) have to be encouraged replacing vertical (hierarchy-based) communication patterns. Recommendation: lateral communication can be facilitated using social software platforms. Federated organizations: IT governance has to be designed at multiple levels: at the unit level, to support the autonomy of each unit, and at the corporate level, to maintain the consistency and foster cooperation between units. Successful coordination mechanisms should involve both elements of centralized coordination (e.g. centralized definition of objectives and performance goals, hierarchical assignment of tasks from the corporate level to the unit level), and decentralized elements based on the peer production principles (as defined for decentralized organizations). Both lateral communication patterns (i.e. from a unit to a unit) and top-down/bottom-up communication patterns (from a unit to the corporate level, and vice versa) have to be used. Efficiency in communication and content creation for both decentralized and federated organizations can be gained using commenting tools, on-line discussions, ranking and many other features provided by social software. Possibility to easily and instantaneously evaluate the content, to see evaluation of the others, and to get/receive feedbacks, guarantees a massive user involvement and fosters relevant content creation. RQ4 IT governance needs to support the synergy of units at the corporate level, and units’ autonomy at the unit level, by combining centralized coordination with distributed (peer) production. See also RQ1. RQ5 Mechanisms combining lateral and vertical (top-down/bottom-up) communication patterns have to be adopted (e.g. peer reviewing, moderation) See also RQ3. Principle 2: Know when to redesign According to [4], rethinking the whole governance structure requires that individuals learn new roles and relationships. Learning takes time. Thus, governance redesign should be infrequent. The recommendation is that a change in governance is required with a change in desirable behavior. Decentralized organizations: Compared to centralized organizations, where the governance structure is global and its change impacts the whole organization, entities in decentralized organizations can redesign the IT governance locally. Thus, on the smaller scale, the organizational learning takes less time and the changes can be made more frequently, allowing for more agility and flexibility. The whole organization can benefit from the experience of each of its business units by reusing their best practices. By sharing best practices and lessons learned, units contribute to the common pool of knowledge and foster the organizational learning. In the studied case, a unit specialized for technology-enabled learning (TEL) is capable to propose redesign, such as use of new IT solutions and principles for “flexible learning” (spanning from off- to on-line) to improve organization’s business. However, at the present time, neither a systematic coordination is installed, nor the TEL unit has real communication mechanism in place to share its knowledge for redesign. RQ6 IT governance needs to encourage shorter cycles of organizational learning for more flexibility and agility. RQ7 Systematic sharing of practice and lessons learned has to be an integrated part of any governance redesign. Recommendation: communities of practice, social networks, and document libraries are examples of tools facilitating knowledge sharing. Federated organizations: Organizational learning process consists of both short cycles when business units redesign their governance locally, and long cycles when the corporate IT governance is reorganized. The local redesigns have to be aligned with the corporate governance. IT governance evolution strongly depends on the capacity of units to share and reuse their local practices. Both lateral communication patterns (from unit to unit) and top-down/bottom-up communication patterns (from unit to the corporate level, and vice versa) have to be used. RQ8 IT governance needs to support short cycles of organizational learning at the unit level and long cycles at the corporate level. See also RQ7. Principle 3: Involve senior managers In [4] it is argued that organizations with more effective IT governance have more of senior management involvement. For example, CIOs must be effectively involved in IT governance for success. Other senior managers must participate in the committees, the approval processes, and performance reviews. Decentralized/Federated organizations: It is important to involve both senior management and local (unit) management in IT governance by forming committees, boards, and expert groups. Communities of practice (COP) can be seen as an alternative for “assigned” groups of senior managers to steer the IT governance. A COP refers to a group of people who share a concern or a passion for something they do and has an objective to share and create common skills, knowledge, and expertise. These groups are formed on the volunteer basis and not by a hierarchical assignment; they also gain trust and reputation within the community of by professionals where they exist. Due to the lack of central authority, an approval process has to be grounded on the principles of group decision-making. In communication, an accent has to be made on knowledge sharing and cooperation over authority and hierarchy. Due to a lack of appropriate coordination mechanisms, in the discussed case, there is a problem of non-involving units in management of the IT governance on the corporate level. The example in section 5.2 is an illustration of that. RQ9 Units have to be involved in IT governance management via boards and expert groups. Recommendation: COPs as an alternative to centrally assigned boards/groups. RQ10 Combination of centralized approval process and distributed decision- making has to be adopted for federated organizations. Recommendation: Performance review can be done using peer-reviewing principles. Principle 4: Make choices According to [4], governance can and should highlight conflicting goals for debate. As the number of tradeoffs increases, governance becomes more complex. Top-performing enterprises handle goal conflicts with a few clear business principles.” Some of the most ineffective governance observed in [4] was the result of conflicting goals. The unmanageable number of goals typically arose from not making strategic business choices and had nothing to do with IT. It is observed that good managers trying diligently to meet all these goals became frustrated and ineffective. Decentralized organizations: Having maximum autonomy, units can have different (event conflicting) goals. Peer ranking, peer trust management, peer reviewing and group decision-making are examples of the mechanisms to be adopted for coordinating conflict solving and decision-making. Social software platforms are indispensable instrument to support these mechanisms within the organization. Lateral communication patterns replace the traditional approval process. In the case illustrated in section 5, this principle has not been yet implemented adequately: at the present time choices/goals are either determined centrally, or solely by the units, i.e. without communication to other units (i.e. goals are not shared). RQ11 IT governance needs to support local units’ goals supporting group decision making. Recommendation: peer reviewing, peer ranking, peer trust management are examples of mechanisms that can support “democratic choice” in decentralized organizations. Federated organizations: The goals and priorities are set up at different levels (corporate, and unit). Unit level goals have to be compliant with the corporate level goals. Between the units, the same coordination mechanisms and communication patterns as for decentralized organizations can be used to negotiate and to resolve the local conflicts. RQ12 IT governance needs to support both centralized and decentralized mechanisms for decision making: “democratic choice” (see also RQ11) on the unit level, and compliance with few high level business principles. Principle 6: Provide the right incentives Following [4], a common problem encountered in studying IT governance was a misalignment of incentive and reward systems with the behaviors the IT governance arrangements were designed to encourage. If IT governance is designed to encourage business unit synergy, autonomy, or some combination, the incentives of the executives must also be aligned Decentralized organizations: Decentralized organizations support maximum of units’ autonomy. In a number of situations, however, the benefits from the “whole” produced collectively, by units’ synergy, exceed the benefits from components contributed by individual units. With the lack of central authority, these synergies can hardly be “encouraged” using regular market incentives. Their formation, however, can result from application of peer production principles and creation of production system based on collaboration among business units who cooperate without relying on either market pricing or managerial hierarchies to coordinate their common enterprise [22]. In this case, the incentives can include status, benefits to reputation, value of innovation to themselves [13]. Motivations can be cooperation are characterized by a combination of a will to create and to communicate with others [19]. Federated organizations: The challenge of federated organizations is to encourage units’ synergy at the corporate level and units’ autonomy - locally. To do so, an organization has to promote the culture of collaboration rather than competition between units. Collaborative environments pave the road to peer production systems; here the individual units are much more sensitive to non-market incentives and are willing to form synergies more than in the competitive environments based on “survival of the fittest” principles. Contradictory incentives can represent a problem in Higher Education organizations like the one we studied: encouraging interdisciplinary Master programs on the university level (synergy) in exchange to reputation and recognition will not be efficient until each department is evaluated and financially rewarded based on its individual performance. RQ13 IT governance needs to encourage cooperation instead of competition. Recommendation: use of nonmarket incentives (e.g. status, reputation) Principle 8: Design governance at multiple organizational levels The authors of [4] argue that in large multi-unit organizations, it is necessary to consider IT governance at several levels. The starting point is enterprise-wide IT governance driven by a small number of enterprise-wide strategies and goals. Enterprises with separate IT functions in divisions, business units, or geographies require a separate but connected layer of IT governance. Assembling the governance arrangements matrixes for the multiple levels in an enterprise makes explicit the connections and pressure points. This principle explicitly refers to IT governance with a complex organizational structure, and proposes multi-level governance. Decentralized organizations Governance arrangements for decentralized organizations can vary from a set of autonomous “silos” to a single, distributed IT governance resulted from collaborative efforts of individual units. In both cases, only one governance level is explicitly defined. In the discussed case, IT governance has been defined at multiple levels (department level, faculty level); its design, however, was not systematic as no coordination within level or between levels was provided. RQ14 Distributed IT governance can be encouraged in the organizations with cooperative culture; For highly competitive environments, governance “in silos” needs to be supported. Federated organizations: For federated organizations that support both units’ synergy (on the corporate level) and units’ autonomy (locally, at the unit level), at least two IT governance levels have to be defined. The special attention has to be paid for adoption of collaborative software for facilitating lateral communication between units. RQ15 IT governance needs to be defined at (at least) two levels: corporate and unit. Principle 9: Provide transparency and education According to [4], transparency and education often go together - the more education, the more transparency, and vice versa. The more transparency of the governance processes, the more confidence in the governance. Also, the less transparent the governance processes are, the less people follow them. Communicating and supporting IT governance is the single most important IT role of senior leaders. Decentralized organizations: Communication and knowledge sharing supported by social software is extremely important for providing transparency and education in IT governance. Adopting technique and tools for distributed content production and collaborative content management, an organization can easily and naturally involve its employees into design of the IT governance process, thus guaranteeing its transparency for the users. Lateral communication patterns should be used - facilitated by senior experts, virtual or live, structured, semi-structured (e.g. webinars, workshops) or informal discussions (e.g. forums, chats, knowledge cafes) on the existing IT governance practice contribute to education and foster the organizational learning. In the given case, this principle is enabled through the means of internal social software, however its broad use is typically ensured only in the situations when a higher level has provided the approval of a “knowledge” and has given recommendations for its use (.i.e. lateral communication is not in place). RQ16 IT governance needs to ensure employees involvement into the IT governance design process. Recommendation: distributed content production and management, social software. RQ17 To foster the education and organizational learning, IT governance needs to extensively use lateral communication patterns Federated organizations: Techniques and tools for distributed content production and collaborative content management play equally important role in achieving transparency in the IT governance process as in decentralized organizations. The role of senior leaders is to setup learning objectives, to supervise the education process, and to evaluate its outcomes. 7 Discussion, Conclusion and Future Work In this study we have addressed the challenge of suitable EA and IT governance principles for decentralized organizations arguing that existing frameworks offer a limited support, and that new principles are needed in order to make them to fully support decentralized organizational structures. While technology serves as a catalyst for organizational transformations, it is important to utilize right IT resources, and in a supportive manner. To accomplish this in decentralized organizations, adequate EA processes, principles and concepts need to be employed to both handle the IT resources and to foster business/IT co-evolution. We have used an institution of Higher Education in Sweden as an illustrative case study. This case was chosen as an example of an organization that exhibits many decentralized properties (in particular with respect to IT governance). The focus was on analyzing the state of its EA in order to assess the decentralization support provided, in contrast with what is needed; and proposing features of an EA and in particular IT governance principles, that could provide the needed support. Our proposed recommendations are mainly based on the 2 principles defined by peer-to-peer domain – peer production and peer trust management. These principles were evaluated by a demonstration to the interviewees in the case; and argumentatively seen as applicable / valid to “university” contexts, which are shifting more and more to decentralization; however no validation on this issue was conducted for other organization types. Hence, the current work is based on a single case study that illustrated the argued limitations of current EA and non-alignment with IT governance in-place, and thus gave us a foundation for proposing new principles for EA and IT governance; however the case study did not validate the proposed principles. To generalize and stream-forward our foundations from the case, we have in section 6 revisited the IT governance principles defined by Weill and Ross in [4], and following them defined a set of requirements for IT governance in supporting the specifics of federated and decentralized organizations. We believe that they may be of interest to three groups: the case organization, researchers in the field of EA, and, potentially, other organizations with decentralized structures interested in implementing some form of EA. For the case organization, the proposed mechanism of peer production, reviewing and trust, also embedded into requirements for adequate IT governance, might be important as their application could offer some improvements to their governance structure. For researchers, this study work might be of interest as it highlights some potential issues with traditional EA knowledge, while giving guidelines on how they could be solved. This work may be of interest to organizations that have adopted, or are interested in adopting a decentralized structure and are looking for the insights into how governance can be successfully done in this environment. For the future work, our short term objective is to evaluate our conclusions in the given case context, and then to extend our study and in other organizations. In long terms, we envisage to in more details analyze mechanisms for coordination (decision making) as well as communication patterns, in centralized, decentralized and mixed (federated) organizations, and to see how they can be transformed into IT governance- type patterns, and how to merge them into exiting EA methodologies.",
  "2015-KRDLG": "1 Introduction Crisis management process is an example of a knowledge-intensive process (KiPs) [5]: it strongly depends on the situation (context) and tacit knowledge of human actors plays the central role in this process. On the other hand, crisis manage- ment has to comply with federal regulations such as the Emergency Management Guidelines. These characteristics make specification and implementation of crisis management solutions challenging. In this paper, we consider the case of flood management process specified for floods on Oka River in the Moscow region in Russia. This process is implemented as a part of COS Operation Center (COSOC) - a smart city solution developed 2 Elena Kushnareva, Irina Rychkova, R´ebecca Deneck´ere, B´en´edicte Le Grand by COS&HT [8]. The existing COSOC solution was designed following tradi- tional workflow-based approach: the flood management process is specified as a sequence of activities that have to be executed according to the current condi- tions (i.e., water level and status of the flooded regions) and in compliance with the Emergency Management Guidelines defined by the Ministry of Emergency Situations (MES) in Russia. The experience shows that execution of workflows proposed by COSOC can often be problematic due to unforeseen circumstances (e.g., lack of resources, disrupted tra\u0000c, etc.) Current system provides only limited support for process flexibility and in order to implement an alternative scenario, the process manager often switches to ”o↵-line” mode. To improve the process flexibility and adaptability: 1) Process specification should capture its intentional perspective, or should answer a question ”WHY do we carry out an activity/procedure?”. Understand- ing intentions behind the guidelines will help to define alternative scenarios in the situations when the default scenario cannot be implemented [16][1]. 2) Process specification should focus on WHAT has to be done instead of HOW it must be done [6]. In this case, only the process outcomes need to be fixed at design while concrete procedures or activities leading to these outcomes can be chosen by the process manager at run-time. In this paper, we introduce an approach for modeling crisis management pro- cess from goals to executable scenarios. We use MAP to reason about intentions behind the flood management process (strategic level). MAP is a goal-oriented modeling language introduced by Rolland in [11][12][14]. We use statecharts formalism for representing the flood management pro- cess at the operational level. Statecharts is a state-oriented modeling formalism defined by Harel [7]. To align the strategic and operation process levels we translate the MAP model of flood management process to the statecharts. Statecharts specifications are executable. We simulate the flood management process, showing how the process goals defined on the strategic level can be achieved by various scenarios executed in the operational level.t The remainder of the paper is organized as follows: in Section 2 we introduce the case study of a flood management process and specify this process on the op- erational level using statecharts; in Section 3 we specify a goal model for the flood management process; in Section 4 we translate MAP model to statecharts; in Section 5 we illustrate how the complete model of the flood management process - strategic and operational levels - can be simulated in YAKINDU Statecharts tool; in Section 6 we draw our conclusions. 2 Flood Management Process: Operational level In this section we briefly introduce our case study - the flood management pro- cess; we show how this process can be modeled with Statecharts formalism. Modeling crisis management process from goals to scenarios 3 2.1 Flood Management Process Floods on the Oka River in the Moscow region are seasonal events caused by an increase in the flow of the river, provoked by intensive snow melting during the spring months. Cities built along the Oka River are confronted to the risk of flooding and can expect important damages, a↵ecting thousands of people. Floods on Oka also represent substantial risks for the critical infrastructure facilities situated in the area: a railway bridge, a pontoon road bridge, an electric power plant, etc. Along with other types of crisis, the flood management process has to comply with the Emergency Management Guidelines [15] defined by MES. This docu- ment prescribes the activities that have to be carried before, during and after crisis situations by di↵erent public services and agencies of the city. As specified in Section 1, the flood management process is implemented as a part of COSOC - a process-aware information system used by a government to manage the variety of cross-domain operations within the city. The functions of COSOC can be roughly divided into three groups: (i) data collection and visualization, (ii) analysis of the situation and decision making and (iii) triggering response processes. The COSOC process manager is a member of the city administration who is responsible for monitoring the situation and handling emerging issues. He/she can accept or decline the solution proposed by the system; when a workflow is triggered, he/she monitors its execution and intervenes when decision-making is required. When the problematic situation is resolved, the process manager can pro- vide feedback to the system: request for process improvement, modification of monitored parameters list, etc. In our previous work [10], we examined the BPMN specification of the flood management process designed for COSOC and showed that the capacity of PAIS to support flexibility of the process is inherent to the underlying process modeling paradigm. In order to provide the flexibility of the process, we design a state-oriented model of the flood management process in the YAKINDU Statechart Tool (SCT) using the formalism of statecharts. 2.2 Statecharts model The statecharts formalism specifies hierarchical state machines (HSM) and ex- tends classical finite-state machines (FSM) by providing: depth (the possibility to model states at multiple hierarchical levels); orthogonality (the possibility to model concurrent or independent sub-machines within one state machine); broadcast communication (the possibility to synchronize multiple concurrent sub- machines via events). The statecharts model (Fig. 1) describes the process with a set of states (e.g., Alert, Emergency, Restoring Normal Functioning, etc.) and transitions between them. Process execution starts at an initial state and terminates at a final state. 4 Elena Kushnareva, Irina Rychkova, R´ebecca Deneck´ere, B´en´edicte Le Grand Fig. 1. Statechart model of a flood management process The sequence of states and transitions that leads from the initial state to the final state can be seen as a process scenario. In statecharts (unlike Petri Nets, where transitions are associated with the execution of an activity) each state transition can be triggered by a specific event or combination of events. For example, the event E2: water level h>10cm and keeps rising triggers a transition from Alert to Emergency. The activities producing these events can be selected at run-time. We call this deferred binding. As a result, a process specification is divided into two parts: the state- transition part, defined with a set of states and transitions between states and their triggering events, and the activity part, defined by a list of activities speci- fied by their preconditions and outcomes. The process enactment can be seen as a dynamic selection of activities to produce some outcomes (events) that make the process progress towards its (desired) final state. 3 Flood Management Process: Strategic level In this section we explain our choice of modeling formalism, introduce the con- cept of MAP, illustrated on the flood management process example, and propose a procedure for transforming a MAP to a Statechart model. Modeling crisis management process from goals to scenarios 5 3.1 Choosing the formalism Processes may be formalized in an intentional way in goal-modeling approaches to model the processes according to the purpose of the actors/organizations. We quote among them i* [17][18], KAOS [3] and MAP. We choose the MAP modeling language in our approach because it allows formalizing flexible processes with high level organizational intentions. It supports variability for the goals and o↵ers the possibility to follow di↵erent strategies by focusing on the intentional aspect when enacting methodological processes [4]. i* has an operational semantic for the tasks but not for the goals and it is not used to model strategic goals; it is not designed to be a variable framework, therefore it does not a↵ord a high level of flexibility. As for KAOS, it supports variability and have a well-structured semantic but is less involved in the intentional aspect of IS actors. Furthermore, KAOS has a rigid task-decomposition; modeling complex intentional processes is then di\u0000cult [13]. 3.2 MAP model Fig. 2. MAP model of the flood management process MAP specifies processes in a flexible way by focusing on the intentions and the di↵erent ways to fulfill them, according to the context of each actor. A MAP 6 Elena Kushnareva, Irina Rychkova, R´ebecca Deneck´ere, B´en´edicte Le Grand model is presented as a graph which nodes represent intentions and edges repre- sent strategies. An edge is defined between two nodes where its related strategy can be used to achieve the intended target node. There may be several edges entering a node representing all the strategies to fulfill an intention. A section is a triplet <source intention, strategy, target intention>, which represents a par- ticular process step to achieve an intention (the target intention) from a specific situation (the source intention) following a particular technique (the strategy). The MAP meta-model has been validated in several domains as IS engineering [14], requirement engineering [13], method engineering [9], enterprise knowledge development [2], etc. The MAP corresponding to the flood management process is presented in Fig. 2. There are three intentions specified on this level: Prepare to flood, Secure the city and Handle emergency. Each intention can be fulfilled by several strategies. For example, Handle emergency intention can be reached either from the initial intention (Start) - in case of need for emergency evacuation of the citizens before the preparations finished, or from the Secure the city intention - in case of lack of resources, or getting the ’Electric Power Plant (EPP) is flooded’ alert. The final intention (Stop) corresponds to the process termination and can be attained either By removing barriers, or By restoring normal functioning of objects of the city infrastructure. The flood management MAP model also contains a recursive section <Handle emergency, By rescue operations, Handle emergency>, which represents the maintenance of a need in emergency handling while receiving the rescue operations requests. All goal-oriented models share the same problem concerning the intentions operationalization. The MAP model highlights this problem by proposing several kind of guidelines to guide the user through the navigation in the map and to explain how a specific intention can be realised with a specific strategy (IAG: intention achievement guideline). This guideline can be described in several ways: in natural language, with an algorithm, through a workflow, etc. However, it is always di\u0000cult to o↵er an automatic way of operationalizing this guideline. Statecharts open an essential dimension to this problem by o↵ering an automatic operationalization of the process contained in the IAG. 4 Statecharts semantics for MAP In this section we propose a procedure for transforming a MAP to a Statechart model by using the example of the flood management process, and discuss the advantages that can be gained by this transformation. As introduced in Section 3, a MAP is specified as a set of intentions to be achieved, and strategies for achieving them. The intentions can be interpreted as sets of states a process manager desires to reach. However, reaching a state does not necessarily mean that the intention is achieved. Some goals may require a number of actions to be performed before being achieved. Furthermore, taking an action aimed at attaining the goal, does not necessarily end in achieving this Modeling crisis management process from goals to scenarios 7 goal, but should reach a state which is closer to that goal than the previous one. As a result, intentions in statechart representation have a ”core”, which is a set of states where some actions towards attaining the goal are performed. This is a statechart representation of strategies. The MAP flood management model transformed in statecharts semantics is shown in Fig. 3. Fig. 3. Statechart representation of a MAP flood management model For example, the condition that triggers the Handle emergency state is ei- ther water rises higher than 45 cm (which implies closing roads/bridges), or the Living Area or EPP sub-state becomes Unsecured (Fig. 1). Following the stat- echart model, the Living Area goes to Unsecured state when lack of resources is detected. The EPP sub-state, in its turn, can only reach the Unsecured state when the EPP is flooded and needs to be secured. Hence, lack of resources, need in closing roads/bridges or a flooded EPP force process manager to change his/her intention from Secure the city towards Handle emergency. In order to fulfill the goal and leave the state, the process manager has to request reinforce- 8 Elena Kushnareva, Irina Rychkova, R´ebecca Deneck´ere, B´en´edicte Le Grand ments, close roads/bridges or secure EPP. In other words, he/she has to make a decision, which strategy to use. In our example of crisis, the decision-making process in statecharts can easily be operationalized. Each state of the statechart is associated with the list of mandatory and optional activities that must/can be carried out upon entering, upon exiting and while in this state. With the state-oriented paradigm, the objective of the flood management process is as follows: the process participants (i.e., MES and Police Taskforce) should respond to the events that occur in the environment (e.g., rise of water, flooded EPP, etc.) by executing the operation procedures and producing the outcomes in order to maintain the secure functioning of the city in specified domains. Thus, transforming a MAP representation to a Statechart representation enables operationalization of MAP and, therefore, linking them to the process scenarios for further simulation. 5 Process simulation with Yakindu Statecharts Tools In this section, we consider the executable level of scenarios of the flood man- agement process by simulating statechart model in YAKINDU Statecharts tool. Fig. 4 illustrates the simulation process of the Statechart model (Fig. 1) and Statechart representation of a MAP model (Fig. 3). For clarity, the bottom right model represents a water level detector. Fig. 4. Simulation of the flood management process in YAKINDU SCT The statecharts are executed concurrently. Current situation is described by the six active (red) states: the water level is above 45 cm; the active intention is Handle emergency, which is not yet achieved; extremely high water level triggers Modeling crisis management process from goals to scenarios 9 a set of procedures in High risk state in Living Area; the EPP is flooded and Unsecured at the moment; roads/bridges are closed; resources are under Crisis Control. In order to fulfill the Handle emergency intention and return to Secure the city, a process manager needs to provide EPP with security, so that the Unse- cured state would not be active. The state-oriented paradigm allows for deferred activity planning: an activity can be defined at run time, based on the desired outcome and on the context (i.e., resources, etc.). In response to unforeseen conditions, the process manager can select the next best step from the list of available activities. Thanks to deferred binding, he/she can also define a new activity better adapted for a situation. Thus, the state-oriented paradigm creates a recommendation system where the process manager plays the leading role in scenario definition. 6 Conclusion Crisis management process is safety-critical its failure could result in loss of life, significant property or environment damage. To ensure safety and security, the activities performed during crisis management are highly regulated at federal level. However, crisis handling requires high agility and never follows the same scenario. Our experience with COSOC processes shows that a concrete flood manage- ment process relies a lot on the experience and decisions of the process manager. Assessment of a situation, adaptive scenario planning and handling the unpre- dictable situations represent challenges for the supporting information system. By combining the MAP and Statecharts we arrived at a formalism which defines how human intentions drive a process. The result is an intention-driven approach to process modeling that o↵ers an intention operationalization and provides a process manager with guidelines from goals identification to scenarios execution. In our future work, we intend to test this process modeling approach on other cases and implement it as a part of recommendation system within COSOC.",
  "2015-KRLG": "1 Introduction A natural or technological crisis can occur as a result of an unpredictable se- quence of events, putting lives of people at risk. Crisis management process has to comply with various norms and regulations; at the same time, it needs to constantly deal with uncertainty and adapt the process scenario to a current situation. Modern city administrations seek to automate crisis management, imple- menting it as a part of their process-aware information systems (PAIS). A PAIS is a software system that manages operational processes involving people and applications based on explicit process models [7]. PAIS design is mostly based on the activity-driven paradigm. According to this paradigm, a process is specified as an ordered set of activities that the system has to execute. This paradigm ensures that the crisis management process is compliant with its norms and regulations ”by design”. However, it supports only limited process flexibility in response to unforeseen situation at run-time. 2 Elena Kushnareva, Irina Rychkova, B´en´edicte Le Grand This is what we experience with the COS Operation Center (COSOC) - a smart city solution developed by the COS&HT company in Russia. In this paper, we consider the example of a flood management process im- plemented as a part of COSOC. We examine the existing (BPMN) specification of this process and propose an alternative (state-oriented) specification of this process using the statecharts formalism [12]. The statecharts formalism allows a designer to focus on WHAT must be done (i.e., expected outputs or postconditions) instead of HOW it must be done (i.e, concrete activities and their ordering). As a result, the concrete activities that suit best a given situation can be selected or even invented by a process manager at run-time. We call this deferred binding. Statecharts specifications are executable. In this work, we simulate the stat- echarts specification of the flood management process with YAKINDU SCT (http://statecharts.org/). We show how the instant animation of a process combined with deferred binding of activities improves the process understand- ing, enables interactive (re)design and testing of both mandatory and adaptable process scenarios and paves the road for automated recommendations. Our findings can be summarised as follows: - BPMN focuses on activities, their ordering and thus ensures compliance by design. The statecharts formalism, in contrast, focuses on the expected outcomes and allows for deferred binding of activities at run-time. We propose (and en- visage for the future work) to combine these formalisms for crisis management process specification, ensuring at the same time the required degree of control and flexibility. - YAKINDU SCT provides a simple yet powerful tool for animation of process scenarios. It can be used as a complement to more conventional process spec- ification and analysis techniques. Developing a methodology for state-oriented and simulation-based process design and analysis needs to be addressed in the future. The remainder of this paper is organized as follows. In Section 2, we discuss the related work. In Section 3, we introduce our running example - the flood management process on Oka River in Moscow Region, Russia. In Section 4, we provide a brief overview of the statecharts formalism. In Sections 5 and 6, we show how the flood management process can be specified with statecharts and animated using Yakindu Statecharts Tools. In Section 7, we draw our conclusions and present the perspective of this work. 2 Related Work Crisis management is widely addressed by researchers in management science: in [20][8] leading ideas on crisis management in a business environment are pre- sented; in [16][6] the context, concepts and practice of risk and crisis management in the public sector are discussed; in [15], a multidisciplinary approach to crisis management is defined. These works are mostly targeted towards federal agen- cies, city administration, policy makers, practitioners and researchers in man- Modeling and Animation of Crisis Management Process with Statecharts 3 agement and business administration. Up to our knowledge, only a few works discuss the challenges of crisis management or its supporting information sys- tems. An example is [17], which highlights the importance of context-awareness in crisis management. Crisis management process is an example of Case Management Process (CMP). Davenport [4] defines a case management process as a process that is not pre- defined or repeatable, but depends on evolving circumstances and decisions re- garding a particular situation, i.e., a case. Adaptive Case Management (ACM) is a paradigm developed by a group of practitioners [25]. The body of knowledge on ACM has been extensively de- veloped by practitioners; the best solutions are regularly reported in the book series on WfMC Global Awards for Excellence in Case Management [26]. How- ever, methodologies and formalisms for CMP modeling are rarely discussed. According to ACM [25], CMP must be organized around a collection of data artifacts about the case; the tasks and their ordering shall be adapted at run time, according to the evolution of the case circumstances and case-related data [18]. We agree with the authors of [25] that the conventional, activity-oriented paradigm is very restrictive while specifying case management and crisis man- agement processes in particular. Moreover, we claim that the capacity of PAIS to support process flexibility is inherent to the underlying process modeling paradigm [13]. Up to now, the activity-oriented paradigm remains the mainstream paradigm for PAIS design. Within the activity-oriented paradigm, a process is specified imperatively, as an ordered set of activities that the system has to carry out. Examples of activity-oriented formalisms include BPMN[14], YAWL[1], activity diagrams in UML[23]. To provide better support for process flexibility, activity-oriented formalisms are extended with declarative parts such as constraints or configurable elements [2][22]. Possibilities to add or modify the activities at run-time remain beyond the scope of these approaches. Whereas the activity-oriented paradigm can be very e\u0000cient in specifying highly-regulated crisis management processes, it can hardly support the run- time flexibility and adaptability required while handling the critical situation since it encourages the early binding of activities (at design-time). On the other hand, the product-oriented (or state-oriented) paradigm focuses on scenario adap- tation and supports the deferred binding: at design-time, the process scenario is specified with as a sequence of events; the concrete activities that will produce these events can be selected or even invented at run-time. Examples of product- oriented modeling formalisms include state machines in UML[23], generic state- transition systems or state machines, such as FSM [9] or statecharts by D. Harel [10] created for the specification and analysis of complex discrete-event systems. Several research groups report on approaches to design and specify case man- agement processes based on the product-oriented paradigm: in [19] an approach that combines product- and activity-oriented paradigms is presented. The case 4 Elena Kushnareva, Irina Rychkova, B´en´edicte Le Grand handling paradigm is presented in [3]. Other formalisms extend product-oriented paradigm with the notions of goal and context [21] [24]. These formalisms sup- port automated recommendations and user guidance, providing that for each goal all the situations (states) in which this goal is achievable are known. How- ever, such formalisms focus on high-level system specifications and rarely support process analysis and simulation. To conclude, we consider that combining an activity-oriented formalism with a state-oriented formalism can provide a process designer with a set of tools to 1) ensure the compliance and control over process execution and 2) better support the run-time process adaptability. 3 Activity-Oriented Model and Process Control 3.1 Flood Management Process in COS Operation Center The COS operation center (COSOC) is a cross-domain information system de- veloped by COS&HT in Russia. In this paper, we discuss the example of a flood management process, supported by COSOC. Floods on the Oka River in the Moscow region are seasonal events caused by an increase in the flow of the river, provoked by intensive snow melting during the spring months. Floods on Oka also represent substantial risks for the critical infrastructure facilities situated in the area: a railway bridge, a pontoon road bridge, an electric power plant, industrial storage facilities, etc. The flood emergency is triggered when the water level in the Oka River rises above 10cm. Table 1 provides a brief description of the major phases of the flood. The flood crisis terminates when the water level gets back to normal, the response operations are terminated and the post-crisis reconstructions begin. The goal of the flood management process in COSOC is to dispatch the assignments for operation procedures according to the crisis development and in agreement with the the Emergency Management Guidelines [5] defined by by the Ministry for Emergency Situations (MES). The selected procedures are carried out by MES, police taskforce, fire brigades, etc. The execution of the flood management process is monitored and controlled by the COSOC process manager. The underlying processes in COSOC (including flood management process) are specified with BPMN - an activity-oriented modeling formalism. Fig.1 illus- trates the (simplified) model of the flood management process implemented in COSOC. 3.2 Modeling Flood Management Process with BPMN Following the flood scenario in Table 1, the BPMN model identifies its major phases and specifies the operation procedures accordingly. The list of events processed by COSOC is presented in Fig. 2-a. The decision-making logic is modeled with a complex gateway G4 in the BPMN diagram. Here, various operation procedures can be (inclusively) selected. Modeling and Animation of Crisis Management Process with Statecharts 5 COSOC ni detnemelpmi ssecorp tnemeganam doofl eht fo ledom NMPB ehT .1.giF 6 Elena Kushnareva, Irina Rychkova, B´en´edicte Le Grand Water level Threats / Expected con- Response rise sequences Inform citizens, deploy the equipment and >10cm Flood Alert set up temporary barriers Declare emergency situation, evacuate >10cm and Flood emergency people from the flooded zones; prepare keeps rising temporary accommodation Minor damages in living Emergency water supply; patrol flooded > 25cm areas; risk of disrupted zones, provide boats and reinforce water water supply barriers Risk of severe damage in Rescue operations; secure bridges and or- >40cm living areas ganize deviations Close the pontoon bridge; secure strate- >45cm Disrupted road tra\u0000c gic infrastructure facilities (industrial stor- ages, factories, electric power plant, etc.) Severe damages in liv- ing and industrial ar- Rescuing operations; chemical and biolog- eas; Risk of presence of ical control of water; evacuation of indus- > 60cm toxic substances in the trial storage facilities; temporal accommo- river; Disrupted electric- dation for citizens ity supply Disrupted railway com- > 75cm Close the railway bridge munication Table 1. Flood Scenario Driven by the changing water level in the Oka River The provided model ensures (by design) a full compliance with norms and regulations defined by MES for flood management process. It also supports flex- ible scenario execution: the activities defined by G4 can be selected in various combinations, repeated or skipped. However, the model is bound by the num- ber and kind of activities. When complex (unforeseen) situations unfold and predefined activities cannot be accomplished (e.g., due to disrupted telecommu- nication, lack of resources etc.) new activities cannot be added at run-time. Run-time adaptability of the process scenario can be improved with state- oriented specifications that do not require early (at design) binding of activities to a process scenario. 4 State-Oriented Model and Process Adaptability 4.1 Statecharts The statecharts formalism specifies a hierarchical state machine (HSM) that ex- tends classical finite state machine (FSM)[9] by providing: (i) depth - the possibility to model states at multiple hierarchical levels, with the notion of abstraction/refinement between levels; Modeling and Animation of Crisis Management Process with Statecharts 7 Fig. 2. High-level specification of the flood management process (ii) orthogonality - the possibility to model concurrent or independent subma- chines within one state machine; (iii) broadcast communication - the possibility to synchronize multiple concur- rent submachines via events. A state s S in statechart represents a state of the system at a given time. 2 A state s consists of a (possibly empty) hierarchy of substates, representing (possibly concurrent) state machines. These substates provide the details about their parent state (or superstate). The active configuration of a state s is the set of substates of s that are activated at the current moment. Events that occur in the environment or result from some activity execution trigger state transitions in statecharts. The triggering event e[c] (interpreted as e occurs and c holds ) of a transition t is an event that must occur in order for t to take place. Here e E is the event that triggers the transition; c C is a 2 2 condition that prevents the transition from being taken unless it is true when e occurs; all these parameters are optional. Some state-oriented approaches (e.g., Petri Net) explicitly associate a state transition with the execution of some activity: they consider a triggering event as an outcome of some concrete activity that is defined at design time. With state- charts, we do not specify the activities and focus uniquely on the expected out- comes (triggering events). We consider that the same outcome can be achieved in di↵erent ways and the concrete activity that needs to be executed can be selected or invented in run-time. This is the deferred activity binding that we mentioned in Section 2. Thanks to the deferred binding, at design-time, the process enactment can be seen as a dynamic selection of activities to produce some outcomes (events) that make the process progress towards its (desired) final state. States in statecharts can be explicitly associated with the activities that have to be carried out throughout or within this state. Such activities would represent the mandatory procedures for a crisis management process. 8 Elena Kushnareva, Irina Rychkova, B´en´edicte Le Grand The association between states and activities can also be implicit: an activity can be carried out at any state once its precondition is fulfilled (i.e., if it is ”not forbidden for performance at this state”). Therefore, any state of statechart can be associated with a (possibly empty) set of mandatory activities and a (possibly empty) set of optional activities. In case of unforeseen situations (i.e., when a mandatory activity cannot result in a desired outcome and an expected triggering event does not occur) - the process manager can select an activity from the list of optional activities in order to compensate/resolve the situation and to eventually produce the desired triggering event. Activities with their preconditions and postconditions can be modeled in a separate model called activity chart [11]. The list of optional activities can be maintained and extended dynamically at run time. New activities can be added to the activity chart by the process manager without a↵ecting the statechart. The activity specification is out of scope for statecharts models and will not be further considered in this paper. 5 Modeling Flood Management Process with Statecharts 5.1 High Level Specification We start the statecharts specification defining three main states for the flood management process: S0: Flood Alert, S1: Flood Emergency and S2: Restoring Normal Functioning. S1 is refined in two (exclusive) substates: S1.1.: Preparation and S1.2.: Emergency Control. S1.1 is the state where preparations of the city facing the flood are carried out according to the MES regulations in place. S1.2 is triggered when the water level in Oka River rises above 25cm (E3 in the list of Flood control events). The black circle indicates that S1.1 is entered by default once S1 is entered (Fig. 2-b). With this high-level specification, we provide a correspondence with the original BPMN specification (the states are indicated in Fig. 1). In the statecharts notation states are depicted by rectangular boxes with rounded corners. The substate-superstate relation is depicted by boxes encapsu- lation. 5.2 Introducing Concurrent Areas We model four di↵erent domains of flood management from our example as four concurrent substates of the S1.2 Emergency Control: Living Area, Transport, Electric Power Plant (EPP) and Resources. Concurrent substates are depicted by regions within an AND-superstate separated by dashed lines. When entering S1.2., the process simultaneously enters the (default) state in each corresponding concurrent substate. Black circles with an outgoing arrow indicate default states. Living Area sub-machine defines three states: Elevated Risk, High Risk and Unsecured. The transitions between these states describe how a flood will progress Modeling and Animation of Crisis Management Process with Statecharts 9 Fig. 3. Explicit resource management and will be managed: Elevated Risk is entered when the water level h rises above 25cm (E3). The events received from Police Taskforce (e.g., requests for evacua- tion, rescue operations etc.) or from the environment (further rise of water level) trigger the High Risk state. The events E7a, E9a, E10a trigger the transition back to the safer state Elevated Risk. These events result from the execution of some operation procedures (e.g., evacuation, rescue, pumping the water out of the streets or others). The state Unsecured is triggered when the event E4 indi- cating the lack of resources during execution of an operation procedure occurs. Along those lines Electric Power Plant and Transport concurrent substates define the submachines that show how the corresponding infrastructure objects are managed during the flood. According to the regulations, the power plant must be Shut Down when the water level rises above 40cm (E6). If the water keeps rising there is a risk that this facility will be flooded. Here the Unsecured state is triggered. The Normal Functioning is maintained for the Transport area; when the water h rises above 40cm only Limited Tra\u0000c is supported; when the water level h exceeds 45cm threshold the pontoon bridge has to be closed (Bridge Closed). In our example, each state of the statechart can be associated with the list of mandatory and optional activities that must/can be carried out upon entering, upon exiting and while in this state. With the state-oriented paradigm, the objective of the flood management process can be reformulated as follows: the process participants (i.e., MES and Police Taskforce) should respond to the 10 Elena Kushnareva, Irina Rychkova, B´en´edicte Le Grand events that occur in the environment (e.g., rise of water, weather changes, etc.) by executing the operation procedures and producing the outcomes in order to maintain the secure functioning of the city in specified domains. 6 Simulation with Yakindo Statecharts Tools We have designed the process specification described previously with the YAKINDU SCT modeler. The YAKINDU simulation environment allows us to instantiate the statecharts specification and to simulate the underlying process. Fig. 4 il- Fig. 4. Simulation of the flood management process with YAKINDU SCT lustrates an execution of a flood management process modeled in Fig. 3. The process starts when the water level in Oka River rises above 10cm (see the event E0 in Table 1) and enters the state S0 Flood Alert. The event E2 triggers the state S1 Flood Emergency and enters its default (exclusive) substate S1 1 Preparation (Fig. 3). Modeling and Animation of Crisis Management Process with Statecharts 11 When the water level rises above 25cm - the S1 2 Emergency Control state is entered. Fig.4-a illustrates an active configuration of the statecharts upon the real- isation of a sequence of events: E0 E2 E3 E7. When the event E7 ! ! ! (Request for evacuation) occurs, it triggers a transition to the High Risk state of the Living Area region. In response, the process manager assigns the tasks for evacuation. According to our scenario ”played” in YAKINDU, the water level rises above 40cm (E6) and then above 45cm (E8). These events trigger the corresponding configurations (the latter is shown in Fig.4-b). In response to the threat, specific tasks for securing the power plant (e.g., Pumping water, Evacuating equipment) are assigned by the process manager. One (or several) of such actions produces a desired event E11a (Power plant is secured), which triggers a transition from Unsecured to Shut Down state in our statechart (Fig.4-c). As the process continues, some of the crisis handling activities (e.g., evacua- tion) produce the E4 (lack of resources) event (Fig.4-d). If it occurs repeatedly, E4 triggers the Federal Alert state (Fig.4-e). The triggering event in our model specifies that this state is activated if the E4 event occurs while the Unsecured state of the Living Area or the Power Plant Unsecured state is active. This mod- els ”an interruption” - a situation of high priority that requires the involvement of military forces or other reserves in order to protect citizens. When the required resources are available (E5) - the system returns back to the configuration where this interruption occurred - in statecharts and in YAKINDU it is realised by the ”entering by deep history” mechanism The evac- uation operations that were compromised by the lack of resources can continue once the resources are available. Eventually the E7a (evacuation terminated) Risk state in the Living event is generated. It triggers the return to the Elevated Area (Fig.4-f). The process terminates once the water level comes below critical (E12). 6.1 Mandatory Scenario Our simulation illustrated in Fig.4 shows the development of flood management process for the following sequence of events: E0 E2 E3 E7 E6 E8 E11a E4 E4 E5 E5 E7a E12 ! ! ! ! ! ! ! ! ! ! ! ! Fig. 2 shows the high level view of the statecharts model. Here only the transi- tions triggered by external events (i.e., the water level h) are visible: E0 E2 E3 E12 ! ! ! According to this view, the process can be seen as an execution of predefined operation procedures in response to the water raise. O\u0000cial norms and regula- tions are usually focused on such mandatory scenarios. Compliance with them is essential for crisis management processes. The list of (mandatory) operation procedures can be specified for each state, similarly to the BPMN specification. 12 Elena Kushnareva, Irina Rychkova, B´en´edicte Le Grand These procedures can be carried out on entering, on exiting, throughout or within a state. The resulting mandatory process scenario can be represented as follows: E0 T 1 E2 T 2, T 3, T 4 E3 T 5, T 6 E12 T 14, T 15 \u0000 ! \u0000 ! \u0000 ! \u0000 The detailed statecharts specification involves not only external but also internal events. Combinations of these events result in unforeseen situations. Moreover, the situations where the execution of mandatory operation procedures are com- promised are not uncommon (the lack of resources is one of the most typical situations). These situations are not considered by the o\u0000cial regulations and require adaptation of activities and scenarios at run-time. 6.2 Adaptable Scenario The process can be seen as an execution of mandatory procedures defined by MES and other (optional, adapted) activities justified by a concrete situation. We refer to this process scenario as adaptable scenario. The goal of the process can be seen as ”to maintain a safe state”. This means that at run-time, if the process enters some ”unsafe state”, the process manager will select or propose activities in order to generate an event that shall trigger the transition back to the ”safe state”. In case of ”selection”, the process manager will select among the activities enabled (i.e., with valid precondition) in the Activity chart associated with the process. If the process manager decides to propose a new activity that better suits the situation - she will add the activity into the activity chart by specifying its precondition and its expected outcome. Activity charts remain out of scope for this work. In Fig. 5 we show the alternative activities that can be carried out at a given state in order to ”produce” a desired event that would trigger a transition to a ”safe” state. For example, in High Risk state of a Living Area submachine, we define two alternative ways to execute the evacuation of people from the flooded areas: by land or by air (when the former is not possible). Both activities, in case of successful termination, can produce the event E7a (evacuation successfully terminated) and thus trigger a transition to Elevated Risk state. There is more then a single adaptable process scenario that can be realised within the same sequence of events: a) E7 T 4 E7a ... b) E7 T 4, T 4a E7a ...c) E7 T 4, T 4a, T 4 E7a ... ! \u0000 ! ! ! \u0000 ! ! ! \u0000 ! ! A more interesting case can be seen for the Resource management area: besides a predefined activity Request Reinforcement that consists in contacting the MES, the process manager defines two other activities that (based on her experience) lead to the same outcome (E5). During the process execution, the alternative activities can be carried out in combination or iteratively until the desired e↵ect is obtained and the transition to Crisis Control is triggered. The corresponding process scenarios that can be realised can look as follows: a) E4 T 7 E5 ...b) E4 T 7, T 7, T 7a E5 ...c) E7 T 7b, T 7a E5 ... ! \u0000 ! ! ! \u0000 ! ! ! \u0000 ! ! Note that some of the activities may not even be known at design (e.g., T7b - Call for reinforcements via social media). Modeling and Animation of Crisis Management Process with Statecharts 13 Fig. 5. Adaptive scenarios 7 Conclusion and Perspectives In this paper, we reported on our experience of modeling crisis management process with the statecharts formalism [12]. We also presented the results of the simulations conducted with YAKINDU SCT. Whereas conventional activity- oriented modeling formalisms ensure process control by design, they provide only limited support for run-time adaptability of a process scenario. The formalism of statecharts can overcome this deficiency and, thus, extend the designers toolkit. In particular, it provides capabilities for animated design and paves the road for automated recommendations. In our future work, we are going to further explore these capabilities. Below, we present some of these perspectives. 7.1 Combining Activity-Oriented and State-Oriented Paradigms for Improving Process Flexibility Business Process Model and Notation (BPMN) is a defacto standard for business process modeling and simulation. Various modeling environments (e.g. Bizagi, Aris, Signavio etc.) support modeling, simulation and validation of the result- ing process models.These and similar tools focus on designing the activities and combining these activities into scenarios. While the ordering of activities (control flow) can be configured at run-time, the number and kind of activities have to be predefined at design-time. For knowledge-intensive processes such as crisis man- agement, activities are also a subject of run-time adaptation. Such adaptation is not supported by activity-oriented paradigm and its corresponding formalisms. Following the state-oriented paradigm, a process designer does not need to design activities, but only their desired results. As for BPMN, the numbers of states and state transitions in statecharts are explicitly specified at design time. Activities, however, are not associated with state transitions and do not need to be explicitly defined by the model. They can be linked or even defined on fly. In response to unforeseen situations, the process manager can select from available activities. Thanks to deferred binding, she can also define a new activity better adapted for a situation. 14 Elena Kushnareva, Irina Rychkova, B´en´edicte Le Grand Combining activity-oriented and state-oriented formalisms, we aim to im- prove the process flexibility. With BPMN, we can specify the ”obligatory” part of the process and validate the compliance with norms and regulations. With stat- echarts, we can focus on the adaptive part. The activities prescribed by BPMN can be explicitly linked to statecharts states, whereas the other (optional) list of activities can be maintained on fly by a process manager, providing greater flexibility of a process. 7.2 Exploring Animated Design Statechart formalism and YAKINDU SCT enable an animated design process for crisis management. The simple yet powerful visual formalism of statecharts allows a process designer to focus on the situations (states) and to reason in terms of ”safe” - ”unsafe” states, setting up the objectives of the process (i.e., ”to maintain the safe state”). Desired case handling outcomes (events) are designed independently from the activities that actually produce these outcomes. As a result, a process is simu- lated with a sequence of (desired/undesired/external/internal) events whereas a decision about a concrete activities can be made reflecting a concrete situation (an active configuration, a history of previously triggered active configurations, a history of events occured etc.). Developing a design framework where di↵erent process scenarios (desired or undesired events) can be played and analysed is our objective. Such framework can help the domain experts to improve the process and possibly to find some situations that they have never considered before and be prepared to handle them. 7.3 From Management to Recommendations From the system perspective, the state-oriented paradigm creates a recommen- dation system where the process manager plays the leading role in scenario defi- nition. Unforeseen situations are handled within the system enabling a seamless improvement of the process. We aim at analyzing situations together with the identification of a desired target state in order to generate recommendations.",
  "2015-KRLGa": "I. INTRODUCTION\nCrisis management is the process followed by an\norganization to deal with a major event that threatens to harm\nthe organization itself, its stakeholders, or the general public.\n[1].\nCity administrations are particularly concerned with crisis\nmanagement. Examples of crisis city may have to deal with\ninclude natural disasters (earthquakes, floods, landslides etc.),\ntechnological accidents (e.g., power plant accident, among\nothers.\nCrisis management process is safety-critical – its failure\ncould result in loss of life, significant property or environment\ndamage. To ensure safety and security, the activities performed\nduring crisis management are highly regulated at the city or\nfederal level. These activities and their order of execution are\ndescribed in operation scenarios, procedures, emergency plans,\nwhich are used by the concerned public services (fire fighters,\nrescue, police, etc.) for regular drills and field trainings.\nAccording to the situation, a concrete scenario is selected from\na predefined set: to assess the current situation, a number of\ncritical parameters is taken into consideration (e.g., traffic\ncondition, water level). Unforeseen situations, i.e., the\nsituations that are not or only partly covered by the predefined\nscenarios and procedures, are handled by human actor(s): in\nsimple cases, the process manager is authorized to define and\nlaunch a new workflow based on her experience; in more\ncomplex situations, which require a higher level of expertise, a\ndecision is made by a specially assigned committee (e.g., a\nboard of experts).\nModern city administrations seek to automate crisis\nmanagement, implementing it as a part of their process-aware\ninformation systems (PAIS). In the following, we examine the\ncharacteristics of crisis, in order to define the requirements for\nPAIS that would support automated crisis management\nprocesses.\nCrisis management is widely addressed by researchers in\nmanagement science: in [34][35][36] leading ideas on crisis\nmanagement in a business environment are presented; in\n[37][38] the context, concepts and practice of risk and crisis\nmanagement in the public sector are discussed; in [39], a\nmultidisciplinary approach to crisis management research,\nusing psychological, social-political, and technologicalstructural research perspectives is defined. These works are\nmostly oriented on federal agencies, city administration, policy\nmakers, practitioners and researchers in management and\nbusiness administration. Up to our knowledge, only a few\nworks are discussing the challenges of crisis management or its\nsupporting information systems. An example is [40], where a\nlack of context-awareness (meteorological data and rainfall\nsensors) leads to a failure to adapt water release plans produced\nby a context-aware information system (CAIS), resulting in a\nsevere flood event in Brisbane, Australia.\nIn our work, we consider crisis management as a particular\ncase of knowledge-intensive process and focus on its \"process\"\nperspective.\nCrisis management process is an example of a knowledgeintensive process (KiPs) [41]: it is unstructured and based on\ncollaboration between actors; tacit knowledge of human actors,\nwhich is not embedded in the process model à priori, plays the\ncentral role in this process.\nDavenport evaluates knowledge intensity by the diversity\nand uncertainty of process input and output [3]. A knowledgeintensive process is characterized by activities that may change\non the fly, are driven by the scenario the process is embedded\nin and, most importantly, depend on the completeness of\navailable contextual information. The set of users who should\nbe involved in each step of the process may not be defined in\nadvance and rather needs to be discovered as the process\nscenario unfolds.\nTherefore, the requirements for PAIS supporting crisis\nmanagement can be also applied to PAIS supporting KiPs.\nA threat occurrence, the element of surprise, and a short\ndecision time are common to a crisis [2]. These characteristics\ndefine the unpredictable nature of the management scenario.\nInstead of creating strategies for events that might occur in the\nfuture, crisis management involves reacting to an event once it\nhas occurred.\nDuring the crisis management, its supporting PAIS has to\nprovide control over the process execution in order to\nguarantee the compliance with norms and regulations. Process\nflexibility is another fundamental need for a system supporting\ncrisis management. According to [4], process flexibility can be\nsummarized as three abilities:\n(a) the ability to deal with uncertainty\n(b) the ability to adapt process scenarios at run-time and\n(c) the ability to evolve processes.\nExplicit specifications of underlying processes play an\nimportant role in PAIS: they allow for better communication\nbetween stakeholders, enable process analysis and support\nredesign efforts [5]. In this work we show that the capacity of\nPAIS to support process flexibility is inherent to the underlying\nprocess modeling paradigm. In particular, we consider activityoriented and state-oriented paradigms and discuss their\nadvantages and limitations in supporting the three abilities\nmentioned above.\nWe illustrate our findings on the example of a flood\nmanagement process – a crisis management process,\nimplemented as a part of the COS Operation Center (COSOC).\nCOSOC is a process-aware information system developed by\nCOS&HT [6] in Russia. Since 2013, the COSOC solution has\nbeen used by the city administration of Novgorod. Underlying\nprocesses for COSOC (including flood management process)\nwere designed with BPMN, following a widely accepted\nactivity-driven paradigm.\nTo assess the ability of the system to deal with uncertainty,\nwe need to study how the system collects and processes data in\norder to adequately assess the current situation. In our case\nstudy, we formulate it as our first research question:\n1) How does the flood management process model\nsupport the run-time assessment of a crisis situation?\nTo assess the ability to adapt process scenarios at run-time,\nwe need to study how the system supports the user, i.e., the\nprocess manager, in his/her “on-action” decision-making\n(especially in the situations not covered by the predefined\noperation scenarios). We formulate the second research\nquestion:\n2) How does the flood management process model support\nthe run-time assessment of a crisis situation?\nTo assess the ability of the system to evolve processes, we\nneed to study how the system supports the user, i.e., process\nmanager, in his/her “after-action” analysis and knowledge\nmanagement activities. We formulate our third research\nquestion:\n3) How does the flood management process model\nassimilate the new scenarios proposed by the process\nmanager? What is the cycle of process evolution or redesign?\nWe examine the current specification of the flood\nmanagement process designed using BPMN (our case study)\nfocusing on the defined research questions. Then we propose\nan alternative model based on the state-oriented paradigm and\ndiscuss its advantages with regard to the activity-oriented\nparadigm.\nThe remainder of this paper is organized as follows: in\nSection 2 we discuss process modeling paradigms and study\ntheir support of flexibility in process models. In Section 3we\npresent the COS Operation Center and the example of flood\nmanagement process. In Section 4 we discuss the BPMN\nspecification of the flood management process used in\nCOSOC and examine how the activity-oriented paradigm\nsupports the process flexibility. In section 5 we propose an\nalternative way to specify the flood management process\nbased on the state-oriented paradigm and the statecharts\nformalism. In Section 6 we discuss the advantages and\nlimitations of the two paradigms, present our conclusions and\ndirections for the future work.\nII. BACKGROUND\nIn this section, we present various process modeling\nparadigms and discuss their capacity to support flexibility in\nprocess models.\nWithin the activity-oriented paradigm, a process is\nspecified as an ordered set of activities that the system has to\ncarry out. Examples of activity- oriented formalisms include\nBPMN [7], YAWL [8], activity diagrams in UML [9] and\nother languages based on workflow concepts.\nActivity-oriented process modeling implies that data\nemerges and evolves within a process according to a\npredefined control flow. Events are supposed to occur (or be\nprocessed) at specific moments of the execution predefined by\nthe model. This paradigm suits predictable and highly\nrepeatable processes. Crisis management processes are\nunpredictable [10]: events and process inputs can occur at any\ntime during the executions; the order of activities can therefore\nnot be predefined and depends on the current situation. Such\nbehavior can thus not be captured by the workflow formalism.\nIn order to increase process flexibility and to better address\nunstructured and knowledge-intensive processes, activityoriented formalisms have been extended with declarative\nparts, such as constraints [11], business rules [12] or\nconfigurable elements [13]. These formalisms can handle\nprocess variability within a potentially large number of\nconfigurations or scenarios. However, either such scenarios\nmust be well identified upfront or the set of business rules (or\nconfiguration elements) must be regularly maintained by an\nexpert. This can be seen as a limitation for crisis management\nprocesses.\nAccording to the product-oriented (or state-oriented)\nparadigm, a process is seen as a product life cycle (i.e., a set\nof product states and transitions between these states).\nExamples of product-oriented modeling formalisms include\nstate machines in UML [14], generic state-transition systems\nor state machines, such as FSM [15] or Petri Nets [16], and\nstatecharts by D. Harel [17] created for the specification and\nanalysis of complex discrete-event systems.\nTraditional FSMs and their corresponding state-transition\ndiagrams are efficient for tackling small problems. However,\nthe complexity of a FSM model tends to grow much faster\nthan the complexity of the problem it represents. This \"state\nexplosion problem\" can be overcome by the introduction of\nmultiple hierarchical levels for states and transitions. Indeed,\nthis hierarchy gives a possibility to reuse some common\nbehaviors across many states and, thus, to reduce the model\ncomplexity. This idea is explored in the formalism of\nstatecharts [14][17].\nThe statecharts formalism specifies hierarchical state\nmachines (HSM); it extends classical FSM by providing:\n• depth – the possibility to model states at multiple\nhierarchical levels, with the notion of\nabstraction/refinement between levels;\n• orthogonality – the possibility to model concurrent or\nindependent submachines within one state machine;\n• broadcast communication – the possibility to synchronize\nmultiple concurrent submachines via events.\nWithin the state-oriented formalism, carried out activities\ndepend on the current state of the product and the process\nscenario is adapted at run time, according to the evolution of\nthe product. This paradigm suits well reactive systems\nspecification [18] since the system’s response to an event shall\nbe defined not only by the type of this event but also by the\ncurrent situation of the system i.e., its state.\nSeveral research groups have reported on approaches to\ndesign and specify unstructured, knowledge-intensive\nprocesses based on the product-oriented paradigm:\n• In [19], process instances are represented as moving\nthrough state space, and the process model is represented\nas a set of formal rules describing valid trajectories. This\napproach is grounded on the theory of automated control\nsystems.\n• In [20], a group of researchers from IBM propose an\napproach that incorporates process- and data-centered\nperspectives and is based on the concept of business\nartifacts.\n• In [21], the Product-Based Workflow Design is presented.\nThis approach explores the interaction between a product\ndata model that reflects the product design and the process\nto manufacture this product represented by a workflow.\n• The authors of [22] present case handling as a paradigm\nfor supporting knowledge-intensive business processes.\nThe authors compare case handling to workflow\nmanagement and identify four problems. In particular, they\nrecognize the lack of flexibility of workflow management\nsystems and acknowledge the important role played by the\n“product” - the case - in the case handling. Their view on\nthe case, however, remains activity-oriented: the proposed\ncase definition explicitly includes the list of activities and\ntheir precedence relations assuming that they are known in\nadvance.\nThe decision or goal-oriented paradigm extends the\nproduct-oriented view on the process: the successive\ntransformations of the product are looked upon as\nconsequences of decisions leading to some goal [23].\nGoal-oriented modeling formalisms (examples include\ni*[24], KAOS [25], MAP [26]) support decision making by\nspecifying goal hierarchies and tracing each decision within\nthese hierarchies. Context-driven goal-oriented process\nmodels [27][28][29] support automated recommendations and\nuser guidance, providing that for each goal all the situations\n(states) in which this goal is achievable are known. However,\ndue to unpredictable sequences of events and non-repeatable\nexecution scenarios in knowledge-intensive process, it will be\nhard if at all possible to model relations between various\nprocess situations, goals and activities that must/can be\nexecuted in order to achieve these goals.\nIn this work we discuss crisis management processes. In\nparticular, we examine the example of flood management\nprocess. While being highly regulated, crisis management\nrequires flexibility and reactivity and never follows the same\nscenario. While activity-oriented paradigm remains the main\nchoice for process designers, we claim that the state-oriented\nparadigm has a great potential for specification of knowledgeintensive processes and crisis management processes in\nparticular.\nIII. COS OPERATION CENTER SOLUTION FOR FLOOD\nMANAGEMENT: CASE STUDY\nIn this section we present the COS operation center\n(COSOC) – a cross-domain information system developed by\nCOS &HT in Russia. The system is used by the administration\nof Novgorod city and is planned for installation for the\nadministration of Moscow Region and Krasnodar Region.\nCOSOC supports a large variety of processes within the\ncity, including crisis management processes. We also\nintroduce an example of a crisis management process: a flood\nmanagement process, implemented as a part of COSOC.\nA. COS Operation Center\nThe Operation Center is a process-aware information\nsystem used by a government to manage the variety of\nprocesses and cross-domain operations within the city, ranging\nfrom paper issuing for citizens, garbage collection, public\ntransport management to monitoring and management of large\nscale emergencies.\nFig. 1. A screenshot of the COSOC executive dashboard. The dashboard is\ndivided into three areas: the city map, the list of events and the list of data\nsources for monitoring the situation on the object\nThe Operation Center provides the authorities with relevant\ninformation across the entire city through a common\ninformation space (executive dashboard). This dashboard\ncontains data from various sources; it allows city service\nmanagers to have a full and comprehensive understanding of\nthe issues and to coordinate the operation of multiple agencies\nin real time.\nThe functions of COSOC can be roughly divided into three\ngroups: (i) data collection and visualization, (ii) analysis of the\nsituation and decision making and (iii) triggering response\nprocesses.\nCOSOC collects the data related to different areas of the\ncity life in a real time and visualizes this data on the executive\ndashboard (Fig.1). This dashboard lists the events in the\nsummary table, with an option of sorting on key parameters\n(level of danger, urgency of response, etc.), and shows their\ngeographical location on the map. Stationary and mobile video\ncameras, embedded sensors measuring traffic density,\npollution concentration, temperature, radioactivity, calls and\nemails from citizens reporting on anomalies and accidents are\nexamples of data sources used by COSOC.\nThe collected data is used for calculating key indicators\n(KI) that are used to assess a situation in the different city\nareas. The key indicators are visualized in a Colored KI matrix\n(Fig.2). This matrix provides an integrated and hierarchical\nview on the current situation in the city. Each cell of this matrix\nin Fig. 2 corresponds to an area of the city activity: electricity\nsupply, water supply, healthcare, ecology and meteorology,\ntransport and so on. The color of the cell indicates a situation:\ngreen – normal, yellow – alert, red – reaction is required and\npurple – emergency. The process manager can zoom into a cell\nfor more detailed views, where the sub-areas and the values of\ntheir corresponding key indicators are displayed.\nWhen an indicator exceeds some critical value (e.g., a\ntraffic jam is detected or pollution exceeds a certain threshold),\nthe system modifies the Colored KI matrix respectively and, if\napplicable, automatically triggers a response workflow from\nthe predefined list. If a solution cannot be triggered\nautomatically, COSOC generates a message for the process\nmanager and proposes to choose a workflow from the list of\navailable solutions.\nFig. 2. A screenshot of a high-level view of the Colored KI matrix in COSOC.\nEach cell indicates an area of the city activity; the color code indicates the\nstatus (from normal – green, to emergency – purple). The process manager can\nzoom-in the cell in order to see the details\nThe COSOC process manager is a member of the city\nadministration who is responsible for monitoring the situation\nand handling emerging issues. He/she can accept or decline the\nsolution proposed by the system; when a workflow is triggered,\nhe/she monitors its execution and intervenes when decisionmaking is required.\nWhen the problematic situation is resolved, the process\nmanager can provide feedback to the system: request for\nprocess improvement, modification of KI list, etc. All change\nrequests are treated by the technical support team of COSOC\nand have to undergo a formal approval process before being\nimplemented.\nThe following section presents an example of a flood\nmanagement process on the Oka River in the Moscow region in\nRussia.\nB. Crisis Management in Case of Floods\nA flood is an overflow of water that submerges a land that\nis normally dry. Floods on the Oka River in the Moscow region\nare seasonal events caused by an increase in the flow of the\nriver, provoked by intensive snow melting during the spring\nmonths. Cities built along the Oka River are confronted to the\nrisk of flooding and can expect important damages, affecting\nthousands of people. Floods on Oka also represent substantial\nrisks for the critical infrastructure facilities situated in the area:\na railway bridge, a pontoon road bridge, an electric power\nplant, industrial storage facilities, etc.\nAlong with other types of crisis, the flood management\nprocess is highly regulated by federal authorities, including the\nministry for Emergency Situations (MES), the Ministry of\nInternal Affairs, and the Ministry of Defense. For example, any\ncrisis management process has to comply with the Emergency\nManagement Guidelines [30] defined by MES. This document\nprescribes the activities that have to be carried before, during\nand after crisis situations by different public services and\nagencies of the city.\nContextual parameters such as water level, temperature,\ncharacteristics of flooded areas, status of the ongoing response\noperations etc., are collected by COSOC and displayed on the\nexecutive dashboard and colored KI matrix described in the\nprevious section.\nThe flood alert is triggered when the daily temperature rises\nabove a certain average defined for the season and holds for\nseveral days, provoking intensive snow melting in the area.\nThe flood emergency is triggered when the water level in the\nOka River rises above 10 cm.\nTable I provides a brief description of the major phases of\nthe flood on the Oka River.\nMES and other regulating authorities define a distinctive\nlist of operation procedures (i.e., responses) that have to be\nexecuted for each of the major phase defined in Table I.\nNevertheless, there exist situations where the predefined\noperation procedures are not sufficient: disrupted\ntelecommunication, electricity and water supply, lack of\nequipment or impossibility to deploy/relocate the required\nequipment – are examples of situations that compromise the\npredefined operation procedures and have to be resolved “on a\ncase basis” by a human actor - the process manager. The\nprocess manager monitors the situation using the executive\ndashboard and the colored KI matrix and proposes the\nscenarios based on his/her experience and understanding of the\nsituation.\nTABLE I. FLOOD SCENARIO DRIVEN BY THE CHANGING WATER LEVEL\nIN THE OKA RIVER\nWater Threats / Expected Response\nlevel rise consequences\nInform citizens, deploy the\n>10 cm Flood Alert equipment and set up\ntemporary barriers\n>10cm Declare emergency situation,\nand Flood emergency evacuate people; prepare\nkeeps temporary accommodation\nrising\nMinor damages in living Emergency water supply;\n> 25cm areas; risk of disrupted water patrol boats flooded and reinforce zones, provide water\nsupply barriers\nRisk of severe damage in Rescue operations; secure\n>40cm living areas bridges and organize\ndeviations\nClose the pontoon bridge;\n>45cm Disrupted road traffic secure strategic infrastructure\nfacilities (industrial storages,\nelectric power plant, etc.)\nSevere damages in living and Rescuing operations;\nindustrial areas; Risk of chemical control of water;\n>60cm presence of toxic substances evacuation of industrial\nin the river; Disrupted storage facilities; temporal\nelectricity supply accommodation for citizens\n> 75cm Disrupted communication railway Close the railway bridge\nThe flood crisis terminates when the water level gets back\nto normal, the response operations are terminated and the postcrisis reconstructions begin.\nIV. MODELING THE FLOOD MANAGEMENT PROCESS :\nACTIVITY-ORIENTED PARADIGM\nThe underlying processes in COSOC (including flood\nmanagement process) are specified using the activity-oriented\nmodeling paradigm. In this section, we examine the current\nmodel of the flood management process specified using\nBPMN. We analyze to what extent this specification supports\nflexibility by answering the questions stated in the introduction.\nA. BPMN model of Flood Management Process in COSOC\nThe goal of the flood management process supported by\nCOSOC is to dispatch the assignments for operation\nprocedures according to the crisis development and in\nagreement with the rules defined by MES and other regulating\nauthorities. The selected procedures are carried out by actors or\ngroups of actors involved into crisis handling (MES, police\ntaskforce, fire brigades, etc.).\nFollowing the activity-driven modeling paradigm, the flood\nmanagement process is explicitly modeled as a preordered set\nof tasks with predefined triggering conditions (BPMN events).\nEach process task represents an assignment that will be carried\nout by a designated actor or team.\nFig. 3 illustrates the flood management process as specified\nand implemented in COSOC. This diagram has been made\nwith BizAgi Process Modeler [31]. For the purpose of this\narticle, we simplified the original model while preserving the\nprocess logics, structure and main process elements.\nThere are four main actors defined for the flood\nmanagement process: COSOC, MES, Police Taskforce and\nEnvironment. The Environment actor represents the\ninfrastructure that provides in a real-time the information about\nthe flood (e.g., social networks, wireless sensors, video\ncameras, other measurement equipment). Each process actor is\nmodeled as a separate pool in Fig. 3. We show the process\ndetails only for the COSOC actor; other actors appear as\n\"empty\" boxes.\nFollowing the flood scenario in Table I, the BPMN model\nidentifies the major flood phases (based on the events received\nfrom the environment or from the MES and Police Taskforce)\nand specifies the operation procedures accordingly. The list of\nevents is presented in Table II. The “water level” alerts (e.g.,\nE0, E2, E3, E6, E8, E12 in the model) are generated\nautomatically by the infrastructure, once the corresponding\nthreshold is reached for the first time. Other events are\nextracted from the reports provided by the human actors. They\ntypically indicate the beginning and the end of the operation\nprocedures and report on specific issues during the execution\nof these procedures that require immediate reaction (e.g., E4,\nE7a, E9a, E10a, E11a in Table II).\nThe flood management process is triggered when the water\nlever h in the Oka River rises above 10 cm (E0: Flood Alert).\nIn response to this alert, temporary flood barriers are set up\n(T1). If within the following 12h the water level h goes back to\nnormal – the process terminates by sending E1: End of Flood\nAlert message to MES. Otherwise, the state of emergency is\ndeclared, Police Taskforce starts to patrol the area and the\nevacuation of citizens from the flooded zones is carried out\n(T2-T4).\nThe next phase of the flood management is triggered when\nthe water level h rises above 25 cm. Here, COSOC generates\nassignments for Police Taskforce to provide boats and to start\nFig. 3.The BPMN model of the flood management\nTABLE II. LIST OF EVENTS PROCESSED BY COSOC\nID From To Description\nE0 Environment COSOC Flood Alert: h > 10 cm\nE1 COSOC MES End of Alert\nE2 Environment COSOC Emergency: h >10 cm\nand keeps rising\nE3 Environment COSOC Elevated Risk: h > 25 cm\nE4 Police Taskforce COSOC Request for resources\n(e.g., manpower,\nequipment)\nE5 MES COSOC Report: resources are\nsent\nE6 Environment COSOC High Risk: h > 40 cm\nE7 Police Taskforce COSOC Request for evacuation\nE7a MES COSOC Report: evacuation is\nterminated\nE8 Environment COSOC Alert: h > 45 cm\nE9 Police Taskforce COSOC Request for rescue\noperation\nE9a MES COSOC Report: rescue operation\nis terminated\nE10 Police Taskforce COSOC Alert: streets are flooded\nE10a Police Taskforce COSOC Report: streets are\nthe emergency water provisioning (T5, T6). For simplicity, we\nomit the message flows between the actors in Fig. 3.\nFrom this point, the Police Taskforce regularly reports on\nthe situation sending messages to COSOC. The system uses\nthese messages for a detailed situation analysis and for\nplanning the next activities. The decision-making logic is\nmodeled with a complex gateway G4 in the BPMN diagram.\nprocess implemented in COSOC\ncleaned\nE11 Police Taskforce COSOC Alert: electric power\nplant is flooded\nE11a Police Taskforce COSOC Report: electric power\nplant is secured\nE12 Environment COSOC Below Critical: h< 25cm\nE13 COSOC MES End of Emergency\nHere, various operation procedures can be (inclusively)\nselected based on the activation conditions (i.e., a specific\nevent or combination of events occurred). Once the selected\noperation procedure terminates the control returns to G4 and\na new iteration begins.\nThe same activity can be triggered several times if its\nactivation condition is satisfied.\nFor example, T17: Pump out water from the streets can\nbe triggered several times during the flood. Along those\nlines, some tasks will not be executed, as their activation\ncondition is not met.\nSome operation procedures produce the outcomes\n(events), which, in their turn, can trigger the other operation\nprocedures. For example, during the execution of T8: Rescue\noperation, the lack of resources can be experienced (E4),\ntriggering T7: Request for Reinforcements. Other examples\nof operation procedures and their associated outcomes are\npresented in Table III.\nThe process is terminated when the water level falls\nbelow a 25cm threshold (E12). The flood barriers get\nremoved, reconstructions start (T18-T19). The message\nabout the end of emergency state is sent (E13).\nB. Analysis\nWe claim that the level of COSOC flexibility is inherent\nto its underlying process model and, thus, strongly related to\nthe selected modeling paradigm. In the following, we\nanalyze the BPMN specification of the process by\nanswering the three questions raised in the introduction of\nthis article:\n1) How does the flood management process model\nsupport the run-time assessment of a crisis situation?\nThe crisis situation in COSOC is assessed based on the\ncontextual parameters extracted from the events that the\nsystem continuously receives.\nThe number and type of contextual parameters are\ndefined in the process model at design time. For COSOC\nflood management process, these parameters include the\nwater level in Oka River, the air temperature, the surface and\nthe characteristics of the flooded zones (e.g., presence of\nmedical, childcare facilities, strategic objects etc.), the status\nof infrastructure in the flooded zones (water supply,\nelectricity supply, telecommunications, roads), the amount of\ndamage, incidents, the status of the ongoing response\noperations, etc.\nPolice and MES teams can transmit additional\ninformation about the situation. However, this information is\nnot processed by the system. New events, contextual\nparameters or rules for triggering operation procedures can\nbe introduced on the model level (i.e., by changing the\nBPMN specification). However, these modifications will not\nhave an effect on the running instance of the COSOC flood\nmanagement process. Therefore, no capacity to dynamically\nadd a new contextual parameter is supported.\nOur case study shows that the system (based on fixed\ncontextual parameters) and the process manager (based on\nthe additional information and personal experience)\nfrequently come to different conclusions regarding the crisis\nsituation assessment. For example, in certain conditions,\nemergency water provisioning (T6) might be required even\nbefore the Elevated Risk Alert (E3) is received. In such\nsituations, the process manager has a choice: to “leave the\nsystem decide” (potentially leading to suboptimal or even\nerroneous scenarios) or to make decision himself/herself and\nto adapt the scenario proposed by the system.\nIn the following, we analyze the capacity of the activityoriented process model to support such adaptations.\n2) To what extent does the flood management process\nmodel allow for run-time scenario adaptation according to\nthis assessment?\nAll possible flood management scenarios that the\nCOSOC system supports are explicitly specified in the\nBPMN model at design time.\nOnce the flood management process is triggered – the\nmodel is instantiated. At run time, the appropriate response\n(activity) is defined automatically according to the evaluated\nconditions specified in the model. In certain cases, the\nsystem calculates the list of alternative responses and offers\nthe process manager to make a choice.\nThe complex gateway G4 specifies the flood\nmanagement scenario in a flexible way: the activities can be\nexecuted in various combinations and repeated multiple\ntimes before the process is terminated. However, only\n“predefined by design” activities can be triggered. When\nrequired, new activities can be added or the process logic can\nbe changed on the model level. However, these modifications\nwill not take effect for the running process instance in\nCOSOC. Therefore, no adaptation of the process scenario at\nrun time is supported.\nAutomated scenario definition reduces the risk of human\nerrors related to decision-making in stressful conditions and\ninformation overload. In practice, however, the process\nmanager handles many situations off-line, by communicating\nwith the response team and by determining and adapting the\ncrisis management scenarios according to their experience.\nSuch adaptations and newly discovered scenario are\nextremely valuable for further evolution of the process and\nits implementing system.\nIn the following we analyze the activity-oriented model\nevolution capacity.\n3) How does the flood management process model\nassimilate the new scenarios proposed by the process\nmanager and what is the cycle of process evolution or\nredesign?\nWhenever an unforeseen situation occurs -the process\nmanager needs to decline the execution of a workflow\nrecommended by the system and to define and execute a new\nworkflow that is better adapted for a situation. For example,\ndue to the road conditions, specific vehicles cannot reach the\nflooded areas; in this case, the rescue operations (T8) can be\ncarried out by helicopters only, or immediate evacuation has\nto be triggered. Such situations and the corresponding\n(adapted) scenarios defined at run time by the process\nmanager have a great value for further process evolution and\nimprovement.\nThe business process lifecycle in COSOC follows the\nmain stages defined in BPM and involves (re)design,\nconfiguration, enactment and evaluation [32].\nOur experience shows that the process manager rarely\nhas an expertise to undergo a new workflow definition:\nhe/she often switches to the “off-line” mode and manages the\nprocess manually (e.g., by sending messages, making phone\ncalls and so on). Therefore, the new scenarios are rarely\ngetting integrated into the process model during the process\nenactment. They can be included into the process model only\nduring evaluation and redesign. In this case, the new\nscenarios for crisis management process are designed with\nthe assistance of domain experts (process manager and other\nactors are involved) than modeled by the process designer,\ntested and integrated into the system.\nDue to the long re-design cycle, the expertise of the\nprocess manager cannot be regularly transformed into\nprocess improvements and can therefore be lost.\nC. Summary\nWithin the activity-oriented paradigm, the contextual\nparameters and the decision-making logic are predefined in\ndesign-time. No run-time modifications are possible. The\nmodel explicitly defines activities associated with decisions.\nEven though the invocation order of activities is specified at\nrun-time, according to received events, adding new activities\nor changing the event-activity association at run-time is not\nsupported.\nNew scenarios can be integrated into the process model\nafter process redesign. However, they will not take effect on\nthe process instances that are currently running.\nFrom the system perspective, the automated run-time\nassessment of a crisis situation and scenario definition\nensures a full compliance with norms and regulations and\nreduces the risk of human errors. The unforeseen situations,\nhowever, have to be coordinated “off-line”, with the resulted\nscenarios poorly traced in the system, preventing from\nprocess improvement. The lessons learned are typically\ndiscussed “after action” and can lead to the system update\nonly after a long cycle of process model evaluation and\nredesign.\nV. MODELING THE FLOOD MANAGEMENT PROCESS :\nSTATE-ORIENTED PARADIGM\nIn this section, we apply the state-oriented paradigm for\nmodeling flood management process. We examine the\ninterest of this paradigm for modeling crisis management\nprocesses and show that within this paradigm, some\nshortcomings identified in the previous section can be\nsolved.\nWithin the state-oriented paradigm, a process is\ndescribed by a set of states and transitions between these\nstates. Process execution starts at an initial state and\nterminates at a final state. A state transition is triggered\nwhen some condition is fulfilled. The sequence of states and\ntransitions that leads from the initial state to the final state\ncan be seen as a process scenario.\nSome state-oriented approaches (e.g., Petri Net) associate\na transition with the execution of one concrete activity (or a\ngroup of activities). On the contrary, we associate a state\ntransition with the occurrence of a triggering event (or\ncombinations of events). Compared to activity-oriented\napproaches that encourage the early binding of activities (at\ndesign-time), state-oriented paradigm supports deferred\nbinding: at design-time, the process scenario can be seen as\na sequence of events. The concrete activities that will\nproduce these events can be selected or event “invented” in\nrun-time.\nFollowing this paradigm shift, a process specification is\ndivided into two parts: the state-transition part, defined with\na set of states and transitions between states and their\ntriggering events, and the activity part, defined by a list of\nactivities specified by their preconditions and outcomes. The\nprocess enactment can be seen as a dynamic selection of\nactivities to produce some outcomes (events) that make the\nprocess progress towards its (desired) final state.\nA. Statecharts model of Flood Management Process in\nCOSOC\nWe design a state-oriented model of the flood\nmanagement process preserving the semantics of the\nexisting BPMN process model specified in Fig. 3. Fig. 4\nillustrates the specification of the flood management process\nmade in the YAKINDU Statechart Tool [33] using the\nformalism of statecharts [17].\nStatecharts describe the process with a set of states (e.g.,\nS0: Flood Alert, S1: Flood Emergency, E2: Restoring\nNormal Functioning, etc.) and transitions between them.\nEach state transition can be triggered by a specific event or\ncombination of events (the event E2: h > 10 cm and keeps\nrising triggers a transition from S0 to S1). To maintain the\nconsistency with the BPMN specification, in the statecharts\nspecification we use the same list of events (Table II).\nActivities are not explicitly modeled in the statecharts\ndiagram. The relationships between activities and events\n(i.e., possible activity outcomes) are illustrated in Table III.\nIn the statecharts notation, states are depicted by\nrectangular boxes with rounded corners. Statecharts use the\nnotion of hierarchical state: this economical visual notation\nallows to specify real-size systems avoiding state explosion.\nThe substate–superstate relation is depicted by boxes\nencapsulation. State transitions are represented with arrows\nand labeled with triggering events.\nWe define three main states for the flood management\nprocess: S0: Flood Alert, S1: Flood Emergency and S2:\nRestoring Normal Functioning (we indicate the rough\ncorrespondence between the states defined in the statecharts\nspecification and the BPMN specification in Fig.3). S1 is\nrefined in two substates: S1.1.: Preparation and S1.2.:\nEmergency Control. The former corresponds to the part of\nthe BPMN specification where the preparations of the city\nfacing the flood are carried out according to the MES\nregulations (i.e., the state of emergency is declared, citizens\nare informed, the evacuation of citizens from flooded zones\nis started etc.). S1.2. is triggered when the water level in\nOka River rises above 25 cm (E3 in Table II).\nStatecharts can model concurrency: Living Area,\nTransport, Electric Power Plant and Resources are four\nparallel sub-machines that describe the domains of flood\nmanagement. When entering S1.2., the process\nsimultaneously enters the (default) state in each\ncorresponding sub-machine. Black circles with an outgoing\narrow indicate default states.\nLiving Area sub-machine defines three states: Elevated\nRisk, High Risk and Unsecured. The transitions between\nthese states describe how a flood will progress and will be\nmanaged: Elevated Risk is entered when the water level h\nrises above 25 cm (E3). The events received from Police\nTaskforce (e.g., requests for evacuation, rescue operations\netc.) or from the environment (further rise of water level)\ntrigger the High Risk state. The events E7a, E9a, E10a\ntrigger the transition back to the “safer state” Elevated Risk.\nThese events result from execution of some operation\nprocedures (e.g., evacuation, rescue, pumping the water out\nof the streets or others). The state Unsecured is triggered\nwhen the event E4 indicating the lack of resources during\nexecution of an operation procedure occurs. This event also\ntriggers a transition in the Resources sub-machine from\nCrisis Control to Insufficient Resources. Once the resources\nare obtained (i.e., E5 is broadcasted) – transitions back to\nCrisis Control and High Risk are triggered in the respective\nsub-machines.\nAlong those lines Electric Power Plant sub-machine\nshows how the electric power plant (a strategic\ninfrastructure object) is managed during the flood.\nAccording to the regulations, the power plant must be Shut\nDown when the water level rises above 40 cm (E6). If the\nwater keeps rising – there is a risk that this facility will be\nflooded. Here the Unsecured state is triggered until the\nsuccessful securing of the power plant (E11a) is reported.\nFig.4. Statecharts diagram of COSOC\nTransport sub-machine is described with three states that\nare entered based on the water level: first, the Normal\nFunctioning is maintained; when the water h rises above\n40cm – only Limited Traffic is supported; when the water\nlevel h exceeds 45 cm threshold – the pontoon bridge has to\nbe closed (Bridge Closed).\nIn our example, each state of the statechart can be\nassociated with the list of obligatory and optional activities\nthat must/can be carried out upon entering, upon exiting and\nwhile in this state.\nWith the state-oriented paradigm, the objective of the\nflood management process can be reformulated as follows:\nthe process participants (i.e., MES and Police Taskforce)\nshould respond to the events that occur in the environment\n(e.g., rise of water, weather changes etc.) by executing the\noperation procedures and producing the outcomes in order\nto maintain the secure functioning of the city in specified\ndomains.\nTABLE III. RELATIONS BETWEEN ACTIVITIES AND EVENTS IN COSOC\nFLOOD MANAGEMENT PROCESS\nActivities Events\nT4. Evacuate people E4, E9, E7a\nT7. Request reinforcements E5\nT8. Rescue operations E4, E9a\nT9. Pump out water E4, E9, E10a\nT11. Secure electric power station E4, E9, E7, E11a\nflood management process\nDuring the execution, decisions about specific operation\nprocedures are taken according to a crisis situation that is\ndescribed by a current configuration of the statecharts (i.e.,\nthe set of active states). To model the decision-making logic\n(according to the BPMN specification) we complement the\nstatecharts diagram in Fig. 4 with a list of activities that can\nbe executed according to the outcome they can produce\n(Table III). Contrary to the BPMN specification, activities\nare bound to the process scenario only at run time.\nB. Analysis\nIn this section, we explore the advantages and the limits\nof the statecharts specification. We structure our analysis by\nanswering the questions raised in the introduction.\n(a) Scenario: E0!E2!E3!E6!E4\n(b) Scenario: E0!E2!E3!E6!E4!E5\n(c)Scenario: E0!E2!E3!E6!E4!E5!E7a\nFig. 5. Simulation of flood management process with YAKINDU\nsimulation tool. Current situation is described by the four active (red) states\n1) How does the flood management process model\nsupport the run-time assessment of a crisis situation?\nWe have designed the process specification with the\nYAKINDU statecharts modeler. The YAKINDU simulation\nenvironment allows us to instantiate the statecharts\nspecification and to simulate the process.\nDuring the process simulation, the crisis situation in\nstatecharts specification is represented by a current active\nconfiguration in the statecharts diagram. In Fig. 5 (a), the\nactive configuration describes the crisis situation where, due\nto the water level h > 40 cm (E6 received) the Electric\nPower Plant is Shut Down and the Limited Traffic is\nmaintained; due to the lack of resources (E4 received), the\nLiving Area is Unsecured.\nOnce resources are received from MES (E5), the\nResources sub-machine goes to the Crisis Control state\nwhereas the Living Area returns to the High Risk state where\nsome operation procedures need to be terminated (Fig. 5\n(b)). When the corresponding procedures are terminated\n(E7a or E9a or E10a are received), the Living Area returns\nto the Elevated Risk state.\nAs for BPMN, the numbers of states and state transitions\nin statecharts are explicitly specified at design time.\nAddition or modification of states or state transitions can be\ndone as a part of redesign and does not take effect on the\nrunning process instance (i.e., the current active\nconfiguration cannot be changed).\n2) To what extent does the flood management process model\nallow for run-time scenario adaptation according to this\nassessment?\nThe state-oriented paradigm allows for deferred activity\nplanning:an activity can be defined at run time, based on the\ndesired outcome and on the context (i.e., resources, etc.).\nTABLE IV. ALTERNATIVE ACTIVITIES INTEGRATED AT RUN TIME\nActivities Events\nT4. Evacuate people (by land) E4, E9, E7a\nT4a. Evacuate people (by air) E4, E9, E7a\nT7. Request reinforcements E5\nT7a. Call for volunteers E5\nT8. Rescue operations E4, E9a\nT8a. Rescue operations (by air) E4, E9a\nIn responce to unforceen conditions, the process manager\ncan select from the available activities. Thanks to deferred\nbinding, she can alsodefinea new activity better adapted for\na situation. Table IV shows some examples of alternative\nactivities not previewed by the original flood management\nprocedures (and not specified in BPMN in Fig.3) but\nproposed by the process manager. For example, when the\nlack of resources message (E4) was received while pumping\nout the water, instead of requesting the reinforcements from\nMES, the city managed the situation by calling volunteers\n(T7a). This activity produced the same desired outcome\n(E5) as T7 while being better adapted for the current\nsituation.\nNew activities or sequences of activities can be\nintegrated into the process scenario “on the fly”, without\nredesigning a process model.\n3) How does the flood management process model\nassimilate the new scenarios proposed by the process\nmanager and what is the cycle of process evolution or\nredesign?\nThe integration of new activities and events in the\nprocess specification can be done on fly, without\nredesigning the cycle. Definition of new states, transitions,\ntriggering events, and refinement of states is a subject of\nredesign cycle.\nC. Summary\nWithin the state-oriented paradigm, contextual\nparameters and decision-making logic is predefined at design\ntime and cannot be modified at run time.\nActivities are not associated with state transitions and\nthus do not have to be explicitly defined by the model.\nThanks to the deferred activity planning mechanism, the\nprocess manager can select a concrete activity at run time,\nbased on the desired outcome and on the context (i.e.,\nresources etc.). Adding new activities or changing the\noutcome-activity association at run-time is possible.\nFrom the system perspective, the state-oriented paradigm\ncreates a recommendation system where the process manager\nplays the leading role in scenario definition. Unforeseen\nsituations are handled within the system enabling seamless\nimprovement of the process.\nVI. CONCLUSION\nCrisis handling requires high agility and reactivity and\nnever follows the same scenario.\nFocusing on efficiency, reliability and control of\nscenario executions, a PAIS supporting crisis management\nsubstantially reduces the risk of errors associated with\ninformation overload and human decision making under\nstressful conditions. However, PAIS capacity to support the\nuser (operator or process manager) in recognizing and\nhandling the situations that are not covered by predefined\nscenarios remains limited.\nOur experience with COSOC shows that a concrete flood\nmanagement scenario relies a lot on the experience and\ndecisions of the process manager. Assessment of a situation,\nadaptive scenario planning and handling the unpredictable\nsituations represent challenges for the supporting information\nsystem.\nIn this work, we were motivated by the lessons learned\nfrom working with COSOC:\na) Crisis management (and flood management in particular)\ncannot be fully automated by COSOC: while COSOC\nimplements the regulations defined by MES, the process is\nstill largely based on the tacit knowledge of the process\nmanager. Fully prescriptive process model leaves very little\nroom for this tacit knowledge to be implemented.\nb) In COSOC, we have \"full automation\", when the system\nexecutes the predefined workflows, or \"no automation\" when\nthe case is getting unpredictable and the process manager\ngoes to “off-line” mode. Providing recommendations and\nassisting in decision making are valuable capabilities to\ndevelop.\nc) The re-design cycle for COSOC is long and complex.\n“Best practices” from the process manager are not\nsystematically documented. Embedded knowledge\nmanagement is a useful capability to develop in COSOC.\nIn this paper, we show on the example of COSOC that\nthe capacity of PAIS to support flexibility of the process is\ninherent to the underlying process modeling paradigm. We\nexamined the BPMN (activity-oriented) specification of the\nflood management process designed for COSOC and\nproposed an alternative (state-oriented) specification of the\nsame process made with statecharts.\nWhile ensuring compliance with the norms and\nregulations, the activity-oriented paradigm provides very\nlimited support for process flexibility at run time.\nUnforeseen situations cannot be handled within this\nparadigm since all activities and triggering events them need\nto be identified at design time.\nThe state-oriented paradigm allows us to exclude\nactivities from the process design: we can state that \"any\nactivity is good as soon as it produces a desired outcome\".\nIn particular, it enables deferred activity planning, that gives\nmore freedom to the process manager in choosing an\nactivity that is adapted for a concrete situation.\nThis paradigm also allows for expanding the notion of a\n“management” system, providing the knowledge worker\nwith guidance, decision support and knowledge management\ncapabilities. These capabilities are valuable not only for\nPAIS supporting crisis management but also for PAIS\nsupporting KiPs in general.\nThe process specification with statecharts has a practical\ninterest for the current COSOC system. The Colored KI\nmatrix used in COSOC for visualization of the current\nsituation (Fig.2) can be obtained from the statecharts\nspecification by associating a color to certain states or state\nconfigurations: once such a configuration is visited – the\ncorresponding area of the Colored KI matrix is recolored.\nThe statecharts formalism has originally been developed\nfor the design and simulation of complex discrete-event\nsystems and thus its visual notation can be considered too\nminimalistic compared to BPMN. Extension of statecharts\nfor the specifics of crisis management processes\nspecification is a subject of our future work.\nIn this paper, we used YAKINDU modeling environment\nfor specification and simulation of the flood management\nprocess. The possibility to animate the specification, to play\ndifferent scenarios and to obtain the immediate visual\nfeedback is very appealing and makes the design process\nextremely interactive and pleasant.\nAlthough the extension of the notation and the adaptation\nof the simulation environment for the specifics of crisis\nmanagement processes are desirable, we consider that the\nstate-oriented modeling with statecharts has a great potential\nfor crisis management process modeling.",
  "2016-BRZ": "1 Introduction Cloud computing, mobile computing, social networks, peer production, open in- novation - are examples of paradigms and technologies that enable new business models today. Adhering to these models, modern organization tend to abandon bureaucratic organizational structures based on formal planning and centralized decision-making and to adopt less formal, decentralized structures and manage- ment styles. Since organizations rely increasingly on their information and communication technologies (ICT), any organizational transformation implies (and depends on) a transformation of its ICT. This is often addressed as Business-IT alignment. While many examples of progressive decentralization in the organizations can be seen (e.g., diversification, adopting flat management style, coopetition, virtual and networked organizations), very little is known about how decentralization impacts the organizational IT. In order to continuously support Business-IT alignment in organizations seek- ing decentralization, the latter needs to be explicitly addressed by enterprise models, enterprise architecture, IT governance and other related disciplines. In this work, we are focusing on Enterprise Modeling (EM). Our motivating question is: How the existing methods of Enterprise Modeling (EM) support the decentralization? This paper is grounded on the earlier works presented in [13][17][2]. In [2], we studied how decentralization is captured by enterprise modeling methods, in particular ”For Enterprise Modeling (4EM)” as an example of an Enterprise Modeling framework [12]. We selected this method because it covers all the es- sential aspects of an organization (4EM defines six sub-models focused on goals, rules, processes, actors and resources, concepts, requirements and technical com- ponents respectively). A predecessor of 4EM, the EKD approach [3], is proven to be e\u0000cient by a number of industrial projects. In addition, the documentation for 4EM is publicly available. Decentralization is widely discussed in the literature [2][13][17][4]: it is consid- ered as one of the design parameters that predefine performance and evolution of a firm. No general agreement, however, can be seen about the criteria that define (and can be used to evaluate) the (de)centralization of a firm. In this work, we identify the criteria of organizational decentralization provided in the literature, propose their synthetic summary and illustrate them on two examples inspired by real companies: a traditional (centralized) and a decentralized organization. We synthesize our experience and the evidences presented in the literature and identify several challenges that modeling experts can face while creating an en- terprise model for a decentralized organization. These challenges are typically handled on the ”case basis” and resolved thanks to the experience and often informal communication between practitioners. However, we consider that the integration of the explicit knowledge about decentralization and its specifics into enterprise modeling methods can be very useful. We propose to adapt the EM methods by explicitly decoupling the enterprise models that reflect the corpo- rate (centrally managed) and the local (distributed) elements (e.g., goals, rules, processes, requirements etc.), by providing the relations between these decou- pled models and by specifying the coordination and communication processes that would be needed to create and validate these models across an organiza- tion. Whereas for traditional, centralized organizations, the local elements will have very little or no impact on the overall EM model, for the post-modern or- ganizations with decentralized structure and distributed decision-making, local models will play a leading role. We illustrate our proposal on the example of 4EM method. This paper is organized as follows: In Section 2, we discuss the literature on the organizational structures and enterprise modeling methods. In Section 3, we identify and synthesize the criteria of centralization/decentralization and present two examples that illustrate the typical characteristics.We discuss how the en- terprise modeling in these organizations can be conducted and identify some challenges related to the decentralization in the latter case. In Section 4 we pro- pose some extensions to EM methods in order to address these challenges. We illustrate our proposal on the example of 4EM. Section 5 presents our conclusions and defines various objectives for future work. 2 Related Work 2.1 Organizational Structure and Decentralization The terms centralization and decentralization often refer to the power over the decisions made in the organization. According to Henry Mintzberg [6], when all the power for decision making rests at a single point in the organization - the structure should be called centralized ; to the extent that the power is dispersed among many entities, the structure should be called decentralized. Another type of organization may be defined between these two [17]: the federation, where some decisions are top-down while some are lateral. Military organizations are typically examples of centralized organizations. They have an explicit hierarchy, with responsibilities and decision making power clearly defined and fixed for the positions within this hierarchy. Many public or- ganizations have a hierarchical structure with federated decision making, where decisions are made by a group of individuals (a board or committee) appointed by the authority or government. This also applies to their IT. Centralized and Federated organizations are very stable and robust but they cannot respond eas- ily to change and are slow to act. A number of organizational structures supporting decentralized decision mak- ing have recently emerged and became popular. These structures are often ad- dressed as ”post-modern” organizations. These organizations are grounded on the principles of social P2P [15], implementing peer-production, peer-trust, peer- review, and peer-vote mechanisms for decentralized communication and decision making. The examples of post-modern organizations include Collaborative Net- work (CN), Virtual Organization, Coopetitions, and Sociocratic organizations. They distinguish from both centrally and federally governed organizations and from anarchies. New branches that incorporate some of Endenburg’s principles of sociocracy include holacracy. The examples of holacratic organizations include Sun Hy- draulics, Valve, W. L. Gore, GitHub, Zappos. Among the core principles behind the post-modern organizations are self-organization and peer-to-peer (P2P) that were extensively studied in the literature. While decentralization is widely addressed in the organizational science liter- ature, very few works discuss the impact of organizational structure and decen- tralization on the organizational IT. In [11], the authors specify four operational models (Diversification, Coordination, Replication and Unification) that refer to di↵erent organizational structures of a company. Authors show that an opera- tional model defines the requirements of process standardization and integration and thus has a direct impact on the enterprise architecture of the company. In [16] the same authors propose a framework to help firms design and com- municate IT governance. This framework considers five major areas in the IT where the decisions have to be made and six decision-making archetypes that reflect the organizational structure of a company. 2.2 Organizational Structures and Enterprise Modeling Our current research is focused on Enterprise Modeling (EM), ”a technique that helps to capture the di↵erent elements and structures of an enterprise as well as to visualize the inter-dependencies between the elements” [12]. The type of organizational structure can a↵ect the process of enterprise mod- eling: the answers to the questions ”which types of enterprise models do we need?”, ”who will be involved in modeling/validation?”, ”how/in which order the model(s) need to be created?” can vary depending on the organizational structure. For example, a problem owner and a project leader can be formally identified within a centralized organization whereas in a decentralized organiza- tion this is not always the case. Moreover, the information required for modeling can be formalized and controlled by one designated team/expert/role in the organization or can be non-formalized, distributed within a community. The decision making becomes a collaborative process that can involve conflicts of in- terests (individual vs. team vs. corporation). Without a proper technical support and facilitation, such a process can take more time and e↵orts in a decentralized environment. To reduce the risks of project failure due to lack of expertise, the EM methods need to be enhanced providing better guidance in case of decentralized organi- zations. 2.3 4EM 4EM (For Enterprise Modeling) is an EM method and framework developed in the academia [12][1]. The 4EM method is comprised of three core elements: – A defined procedure for modeling using a defined notation for representing the modeling product (defined modeling process and product) – Performance of EM in the form of a project with predetermined roles (project organization and roles) – A participatory process to involve stakeholders and domain experts (stake- holder participation) ”The 4EM method uses six interrelated sub-models, which complement each other and capture di↵erent views of the enterprise which can also be considered as perspectives” [12][Ch. 7.2 p. 77]. Each sub-model contains its own notations and rules to generate a diagram. – Goal Model shows the goals the organization wants to achieve, or to avoid. – Business Rules Model is used to define and maintain explicitly formulated business rules, consistent with the Goals Model. Business Rules may be seen as operationalization or limits of goals. – Business Process Model is used to define enterprise processes, the way they interact and the way they handle information as well as material. – Concepts Model is used to strictly define the ”things” and ”phenomena” one is talking about in the other models. – Actors & Resources Model is used to describe how di↵erent actors and re- sources are related to each other and how they are related to components of the Goals Model and Business Processes Model. – Technical Components & Requirements Model is focused on the technical system that is needed to support the goals, processes, and actors of the enterprise. Enterprise modeling (EM) projects are conducted in response to some prob- lems identified in a company. The EM process defined by 4EM includes 10 steps [1], and involves various actors (usually Problem owner, Domain expert, EM project leader, EM facilita- tor, and Tool expert). In each step, one of the actors is typically assigned as a Responsible, and one or several other actors as Participants. This is illustrated in Table 1 taken from [9]: Table 1. Actor involvement in the EM process steps (R- responsible, P- participates) Problem Domain EM project EM Process step (1-10) Tool expert owner expert leader facilitator 1) Define scope and objectives of the project R - P - - 2) Plan for project activities and resources R - P P - 3) Plan for modeling session P - R P - 4) Gather and analyse background information - - P R - 5) Interview modeling participants - P - R - 6) Prepare modeling session P - P R - 7) Conduct modeling session - P - R P 8) Write meeting minutes - - P R P 9) Analyse and refine models P - P R P 10) Present the results to stakeholders R P P P - For example, in (1), the Problem Owner - an organizational actor that will benefit from solving the problem - is assigned as a responsible for the step. The EM project leader ensures the communication between the domain experts and the rest of the project team. The problem owner and the EM project leader should discuss the problem boundaries, prospective solutions and outcomes. 4EM does not prescribe a particular list of sub-models to be focused on or the order in which they should be created - this is also a part of the EM project. Steps (3)-(9) can be replicated for all the sub-models defined by 4EM. The original version of 4EM provides a set of generic concepts not particularly tailored for decentralization. In the next section we discuss how 4EM can be applied in the case of a decentralized organization. 3 Applying 4EM on Centralized and Decentralized Organizations In this section, we first explain criteria used to explain centralization and de- centralization, then we illustrate application of 4EM for two fictional companies studios3: inspired by real companies and one with centralized and one with de- centralized organization. 3.1 Criteria of Decentralization Criteria according to Mintzberg: In [6], the five organizational structures are de- fined: simple structure, machine bureaucracy, professional bureaucracy, division- alized form, and adhocracy. These organizational structures are defined based on their coordinating mechanism, the key part of the organization and type/degree of decentralization: MI1 The key part of the organization. One (or combination) of the following six components can be a key part: the strategic apex or top management, the operative core or the workers per- forming actual tasks and providing direct services, the middle line or middle management, the technostructure or analysts who design, plan, change, train the operating core, the support sta↵ or the workers providing indirect ser- vices. MI2 The prime coordinating mechanism. The major method the organization uses to coordinate its activities: Di- rect supervision where one individual is responsible of the work of others, standardization of work process, standardization of skills, standardization of output, mutual adjustment when work is coordinated through informal com- munication. MI3 The type of decentralization used. Mintzberg specifies the extent to which the organization involves subordi- nates in the decision-making process. Three types of decentralization are identified: Vertical decentralization is the distribution of power down the chain of command, or shared authority between superordinates and sub- ordinates in any organization. Horizontal decentralization is the extent to which non administrators (including operators sometimes) make decisions, or share authority between line and sta↵. Horizontal decentralization is re- lated to the coordinating mechanisms, with direct supervision corresponding to more horizontally centralized, and mutual adjustment to more horizon- tally decentralized organizations accordingly. Selective decentralization is the extent to which decision-making power is distributed between di↵erent units within the organization. Criteria according to Morgan: In [7], five types of organizational structure are identified: (1) Hierarchical organizations, based on multiple management layers, top-down communication and rigid rules for decision making; (2) Flatter organi- zations, seeking to reduce the number of hierarchical levels and to support both top-down and bottom-up communication flow; (3) Flat organizations, where all entities are equal and no hierarchy is defined (4) Flatarchies that can be found in 3 Electronic Arts (www.ea.com), Take 2 (www.take2games.com), and many others for centralized case. Valve Corporation [15] for decentralized case (known for its ease in adjustment and creation of internal teams without hierarchy). between hierarchies and flat organizations, with temporarily formed hierarchies out of flat teams; (5) Holacratic organizations focusing on distributed decision making while giving everyone the opportunity to work on what they do best. Morgan distinguishes the organizational structures based on those criteria [7]: M1 A number of hierarchical levels that the organization defines. Centralized organizations tend to have more levels and as a consequence more elaborated chain of commands and coordination protocols; M2 Flexibility of hierarchical levels. Centralized organizations tend to have fixed, not evolving hierarchies; M3 Flexibility of decision making rules. Centralized organizations tend to have decision making power concentrated in the top hierarchical levels, coordination and decision making rules are formalized and fixed. For the decentralized organizations, according to Mor- gan, decision making rules are not fixed and the power is distributed between entities. Criteria according to Luthans: Luthans described 3 criteria [5]. L1 Geographical/territorial concentration or dispersion of operations. The extreme centralization case would be a single huge factory creating the final product from raw materials. In contrary, extreme decentralization case implies one factory per transformation or service. L2 Geographical/territorial concentration or dispersion of functions. The most centralized case would describe a facility containing the whole functions like production, sales, finance, and direction (typically one facil- XXth ity with factory and headquarter inside, like in the beginning of the century, or more recently small industries). L3 Extent of concentration or delegation of decision making powers. Decisions may be taken by a single person for everything (CEO), partially delegated (middle management), or even shared by the whole employees (workers’ cooperative). Criteria according to CIGREF : The CIGREF (Club Informatique des Grandes Entreprises Franc¸aises - large french corporations IT workgroup) defined 4 lev- els of centralization [4]: (1) Concentration that is characterized by grouping of action and control capabilities in one location, under one top position; (2) cen- tralization characterized by grouping of action and control capabilities in one or more locations, under one top position; (3) deconcentration characterized by scattering of action and control capabilities in one or more locations, where local directors make decisions, but they still follow a central top position; (4) decentralization that is similar to deconcentration, but without the central top position. The following characteristics can be identified from those 4 levels: CIG1 Concentration/Dispersion of the actions (IT production, development team) and control (quality management, organization analysts) capabilities. CIG2 Allocation of decision rights (top/middle layer in large corporations context). Criteria according to Pearlson & Saunders: They propose 3 criteria [14][8]. PS1 Allocation of decision rights (one top position, or every employees). Decision making is kept within one person at the top hierarchy or the whole workforce, the two dimensions ”quantity” and ”hierarchical level” are taken into account. PS2 Structure of communication lines (by hierarchy, or direct communication). Communication between teams are either made by a strict process involv- ing hierarchy, or without any formal method and people exchange directly information. PS3 Forms of coordination (vertical coordination, or lateral coordination). Coordination is achieved by an exchange of information about the state of the work, or by standards accepted by all teams. Within vertical coordi- nation, management transfers information and plans objectives with specific rules and standards, besides horizontal coordination, people align themselves by exchanging information directly and create work team if it is necessary. Table 2. Criteria of organizational centralization/decentralization Criteria in the Characteristic Decentralized Centralized literature Dispersed operations in multiple Concentration in the HQ (One facility Concentration of 1 locations (Multiple facilities containing containing the whole production L1, CIG1 operations di↵erent processes) processes) Dispersed functions in di↵erent Concentration of Concentration in the HQ (One facility 2 locations (Multiple facilities containing L2, CIG1 functions with all the functions) one or more functions) Vertical coordination; Direct Forms of Horizontal coordination; Mutual MI2, MI3, CIG1, 3 supervision (hierarchy); Standardization coordination adjustment (peer-to-peer) PS3 of work process, output, and/or skills Distributed in the whole organization Top position / Hierarchy (decision (democratic); Vertical and Horizontal Allocation of making power in the upper hierarchy); MI1, MI3, M3, 4 decentralization; Selective decision rights Vertical centralization or Horizontal L3, CIG2, PS1 decentralization (decision making power centralization shared within the organization) Structure of Direct communication (informal Hierarchy / Formal communication 5 MI3, PS2 communication communication between peers) (strict processes of communication) Multiple levels (Top hierarchy, middle Number of Few levels (Flat organizations, managers 6 line managers, team managers, with M1 hierarchical levels of ad-hoc teams) various ramifications) Regular reorganization (Changing Flexibility of Fixed hierarchies (Fixed roles and/or 7 roles/shape of organization, adaptation to M2 hierarchical levels peoples, slow change management) the environment) Key Part of the Technostructure (Structure and Strategic apex and Operating core 8 MI1 organization Processes of the organization) (Decision making power and Production) 3.2 Case 1 : Application of 4EM on Centralized Corporation Centralized Corporation is an American developer, publisher and distributor of video games (inspired by Electronic Arts, Take 2, and many others). Centralized Corporation is producing and distributing video games through the dematerialized retail store; it also publishes games in a physical form (game- boxes, DVD) for distribution in regular stores, using subcontractors for packag- ing production. The competitive advantage comes from the studios and sub- sidiaries specialized in di↵erent themes and franchises (dedicated studio for sports, adventure, etc). Strategic decisions (financial goal, supporting or dis- continuing games, retail strategies, and so on) are taken by a CEO and a board of studio directors, technical decisions (design, packaging, and so on) are taken within each studio (see MI1-L3-PS1-PS2). Hierarchical levels are limited (Cor- poration level, design studio level, project level) and fixed (M1-M2), allowing each level to report formally to its direct supervisor (L3-PS1-PS2). Standardiza- tion of skills and outputs are used for coordination, and direct communication are used within each studio, forming a limited vertical decentralization (MI3- L3-PS1-PS2). The board of directors and the operating core (designers) are the key parts of the company (MI1). Enterprise Modeling for Centralized Corporation: Consider a project, integration of a new IS supporting design and development of games in Virtual Reality (VR) format. This project requires creation or modification of (some of) the 4EM submodels (see [12]). It will be conducted following the process described in section 2.3. In the Centralized Corporation, a studio director (if the system is requested by one studio only) or a board of directors including CEO (if the system is demanded by a group of studios) can be considered as a Prob- lem Owner. A middle manager will be appointed as a EM project leader. The project leader will create a transverse team of domain experts (from di↵erent studios) if necessary and will supervise the project. An internal or external EM experts will join the team to ensure that the methodology (4EM in our case) is respected and that the models are understood by all the stakeholders. The process defined by 4EM will be implemented with respect to the guidelines: for example, at the step 3)Plan for modeling session the project leader will set up the meeting with the studio director and with the 4EM expert (EM facilitator) in order to plan a specific questions to be addressed during a modeling session and to identify the domain experts (e.g., managers, developers, VR experts, de- signers) to participate in the modeling sessions. At the step 7)Conduct modeling session, the 4EM experts will produce the models together with the appointed domain experts. Creating di↵erent models involves di↵erent domain experts: for example, elaboration of a Goal model will require a participation of studio direc- tors and CEO; business managers in the studios will be involved in definition of Business Rules and Processes; technical experts will be participating in Process and Concept modeling; specification of technical Requirements and preparation of deployment plan for Technical Components will involve technical experts, de- velopers and testers. In the Centralized Corporation, the domain experts will be defined and appointed by a project leader. 3.3 Case 2 : Application of 4EM on Decentralized Corporation Decentralized Corporation is an American digital distributor of video game, and game developer (inspired by Valve Corporation [15]). Decentralized Corporation is distributing video games through its own de- materialized worldwide retail store, it also creates games. It is specialized in the development of an international platform providing games, updates, multiplayer support based on friend lists, worldwide ranking, through cloud technologies. Independent developers are invited to propose their projects on the platform, and even to put their games on it for publication. Concentration of operations and functions: The company is located in one placed4. building (Headquarters) where all the teams are The teams are working together on di↵erent projects; the employees are focusing on game design, devel- oping, testing, marketing and sales. The non-core functions such as infrastructure management (data center, development and testing environment), network and security management are outsourced to the external service providers or used ”in the cloud” (see L1-L2). An employee of the Decentralized Corporation can choose a project or a team to work with; the roles and the competences required for these roles are defined, but the team members can switch the roles freely. If an employee wants to launch a project, he can create a team. No micromanagement exists: for example, the founder of the company works within his own team. Thus, all the employees can be considered at one hierarchical level (see MI2-MI3). Coordination and governance of Decentralized Corporation: Teams coordi- nate themselves by mutual adjustment (MI2), a form of horizontal coordination (MI3-PS3). Decision making power is distributed in the whole operative core (MI1-MI3-L3-PS1), according to Mintzberg [6][Ch. 11 p. 210], this organization corresponds to the type E (Vertical and Horizontal Decentralization). The em- ployees are very skilled and free, this allows the company to produce innovative services. Form of Communication: No formal reporting is defined; the teams use direct communication. This type of communication is linked with the mutual adjust- ment; any di\u0000culty in coordination will be corrected by immediate discussion between teams (see MI2-MI3-PS2). Key part of the organization: The decentralized company mainly lies on its operative core. There is no need of technostructure or analysts for adapting the processes; a team can be modified any time for a better coordination between projects. Enterprise Modeling for Decentralized Corporation: Consider the same project, integration of a new IS supporting design and development of games in Virtual Reality (VR) format. In the Decentralized Corporation, the project is launched by one or multiple teams. The new project team is formed and the roles are distributed by the team members. 4 As in many organizations, some characteristics of the Decentralized Corporation correspond to centralization and some to decentralization according to Table 2. Decentralization implies less formal relations and communication patterns, thus information about how the company is functioning and ”who does what” is scattered. Distributing the roles within an EM project team will be di↵erent compared to a centralized company: The same group will also form the project team and share the role of the project leader. The EM facilitator and tool experts will be external consultants (the company maintains a focused set of skills and EM is not a field of expertise they want to develop in-house). These consultants will join the company for the project and will work on modeling in collaboration with the employees. In the Decentralized Corporation, voluntarily participation in a project is one of the main principles. Thus, the experts cannot be ”assigned” and should nominate themselves for participation in the modeling session. Implementation of the 4EM process (the 10 steps) will also vary compared to a centralized company: First of all, for the Decentralized Corporation, a process cannot be imposed (no manager to do so) but should be agreed upon. Con- ducting modeling sessions (step 5) and refining the models (step 9) will require participation from di↵erent employees and will rely strongly on their interest in the project and willingness to cooperate. Due to the distributed knowledge, gathering the background information (step 4) can take more time and e↵orts. Creating the 4EM sub-models for Decentralized Corporation would also face some challenges: Since there is no executive level in the company, setting up the corporate goals, definition of the business rules, concepts and processes should be done through a collaborative process between the employees. Project-specific or team-specific concepts should be separated from the core concepts accepted by the company as a whole to avoid the information overload. Teams can have their local goals or priorities that are not always related to the corporate goals. Conflicts of interests need to be considered in such cases. Whereas the corporate processes are required to support the smooth collaboration between the teams and to ensure the corporate goals are met, each team can specify its own pro- cess(es) and working style. Since the actors and resources are identified by the project teams based on their current needs and can change all the time, the relations between actors and resources and the elements of a (corporate) goal model become di\u0000cult to capture within a single corporate Actors & Resources model. 4 Discussion During this work, we identified some problems that need to be systematically resolved while applying the 4EM method in decentralized organizations. These problems are related to: – Distributing the roles within an EM project – Implementation of the modeling process defined by 4EM (10 steps) – Creating the 4EM sub-models decentralization5, Depending on degree of the described problems can be more or less apparent. Distribution of roles and implementation of the modeling process defined by the original 4EM are challenging in decentralized organizations. Lack of explicit knowledge and use of informal communication can be overcome with advanced methods of knowledge management and communication around the project. The employees can choose to participate or not in a project (compared to being assigned by a manager) - therefore, some e↵orts should be made to promote the EM project and to explain its importance to all the potential stakeholders. Collaborative nature of work requires specific communication platforms and tools (social networks, groupware, etc.) in order to reduce the time needed to reach an agreement or to inform all interested participants. To overcome the challenges related to the creation of the main sub-models defined by 4EM, we propose the following extensions illustrated in Figure 1: 1 We propose to decouple the local (created by each team) and the corporate (created for the whole organization) models; 2 We propose to create a ”goal model loop” that specifies the dependencies between local and corporate models; 3 We propose to add relations between Actors & Resources and Concepts models 4 We propose to add relations between Business Processes, Actors & Re- sources, and Technical Components & Requirements models The idea behind the separation between local and corporate models has been explored by the FEA with the segments (core mission-area and business- services) [10] [Ch. FEA, The FEA Perspective on EA], where some segments may be local or shared according to their scope. Two types of corporate models are possible : a concrete model containing various local models parts, and an implicit model based on lateral communications between peers (using only local models). In this work, we explore the first one, where a model is built upon local models to maintain a corporate vision about the full organization. This choice is close to a federated model, but it matches a compromise between the two cases pre- sented (the centralized case can use the corporate model, and the decentralized case has more flexibility than before for modeling). Goal model, Business Rule model, Business Process model and other models can be seen on the corporate level and on the business unit (BU) or team level for the decentralized organiza- tions (yellow rectangles in Figure 1). The goal model allows sharing of common values across the company: for example,increasing the number of players can be considered as corporate goals and shared between the game development team, the cloud client team, and the new console development team. The teams can use the local Goal model to define their internal goals. These goals can support the corporate goals, can complement them or can be defined independently. The 5 Evaluation of ”degree of decentralization” for a given company remains beyond the scope of this work. For simplicity, one can consider a company ”more decentralized” if it exhibits more ”Decentralized” characteristics from Table 2. Fig. 1. The extended 4EM for decentralized organizations role of Corporate Business Process model in a decentralized organization is to specify the processes shared by BUs, including core business processes (if any) that can be standard for all BUs, collaborative processes where the activities or sub-processes are performed by di↵erent BUs or teams and supporting pro- cesses ensuring coordination and communication among BUs. Each BU can also have its own (local) processes. For example, di↵erent teams can follow their own process in order to fulfill a sub-process of a corporate process. The Corporate Concept model specifies the concepts shared by the BUs and those required for the communication between the teams. Local model specifies the local concepts, specific for a BU, a team or a project. Similarly, the corporate and local versions of Actors & Resource model and Technical Components & Requirements model can be defined. In order to identify how the local (internal) goals of each team are related to the shared (corporate) goals, we add the cyclic relation (a loop) between lo- cal and corporate goal models. This loop specifies that the local goals can be defined by the corporate goals as well as the corporate goals can be defined by the local goals. Indeed, compared to the centralized organization, where the cor- porate goals define the local goals of each team, in decentralized organizations, we need to consider the situations when the team goals can be leveraged to the corporate level: for example, an innovative solution for a new game console can set up the goals for the game developers. Distributed decision making (L3-PS1) is acknowledged since each team decides locally on its goal model. In a decentralized organization, resources and actors are shared between teams. Compared to traditional (centralized) organizations, it is the resources and actors of all the teams that define the resources and actors of a corpora- tion and not the other way round. This is reflected by a relation between the corresponding models in Figure 1. We also add a relationship between Actors & Resources and Concepts Models since the concepts should also be attached to their owners (or creators) in a company. This relationship reflects the concept of direct communication defined in MI3 and PS2. To create each local model, internal meetings has to be made regularly to ask questions and change their processes : what are the new objectives ? where could the work be optimized ? Thereby, each team decides internally its way of working. In the decentralized context, representatives of each team meet oc- casionally together to establish corporate goals and acknowledge the change in their services interfaces (services they o↵er to other teams, like an API for devel- opment, a version control system, etc.). Thus, if a team needs to use or create a service, it may check if another is already serving it (no duplication). To extend this way of working, we may assume that a subscription to external services implies subscription about their updates. Then, at least one component must be shared in the whole organization : the services catalog (supported by centralized or decentralized technologies). This process assumes teams are collaborating and have the capacities to make meetings (physically or not), as collaboration and communication are important in decentralized organizations. This discussion is limited to the presented cases; more empirical work and experimentation is required. 5 Conclusion and Perspectives In this paper, we study how decentralization impacts the enterprise modeling methods in the organizations. We gather centralization and decentralization cri- teria in Table 2, then we propose four extensions to the EM methods in order to better capture the specifics of decentralized organizations. We illustrate our proposal on the example of 4EM in Figure 1. We described a corporate model updated by a meeting of representatives of each team composing the organiza- tion, but other ways may be explored where peers review their productions and decisions continuously. This work is a step in a larger research project study- ing decentralization and its impact on the organizational IT. More studies on post-modern organizations and their business models will be done to better un- derstand the possibilities o↵ered by modern technologies (P2P, mobile, cloud, virtualization, etc.).",
  "2016-Dall": "1 Introduction This article describes some recent advances in the Information Systems research do- main to support users of mobiles devices such as tablets or smartphones. Our goal is to provide these users with a better guidance through their daily use of these interfac- es. The objective is to generate recommendations that are adapted to a given user’s context and that take into account his/her objectives. These recommendations systems also take into account the experience of the other system users. In order to make this possible, we show how information about the evolving context of users can be organized and analyzed through context management. We also de- scribe an original approach called intention mining, which analyzes traces of users’ actions on the system and extracts their goals (intentions). Finally, we explain how an adequate process modeling formalism allows for personalized recommendations (and 1 therefore adaptive scenarios) in order to optimize users experience with the infor- mation system. So far, these approaches have mostly been applied to business process models, and the second objective of this work is to study their interest for video games, in particu- lar urban games which combine physical and real worlds. 2 Context Management 2.1 Context management in information systems: state of the art The “one size fits all” way of using Information Systems (ISs) does not work any- more. Users now expect these systems to be context-aware, i.e., to adapt to users’ specific contexts [Baldauf et al., 2007]. The context represents any information that can be used to characterize the situation of an entity (a person, place, or object) that is considered relevant to the interaction between a user and an application [Dey, 2001]. More generally, we can consider the context as everything that surrounds the center of interest of the individual and that provides additional information that can help under- stand his/her focus [Mostefaoui et al., 2004]. Gamers playing to an urban game are very sensitive to their contexts. When playing their game, they evolve outdoors in the real word and, as a result, can modify their behavior to match this context. For instance, if the gamer is heading to a particular place but that rain starts pouring, then he will certainly delay his walk to do some- thing else until the rain stops (exchange with another gamer, try to solve a clue, or any other activity that can be done inside instead of outside a building). The management of context information has become strategic in the context of perva- sive information systems [Kourouthanassis et al., 2008] [Najar et al., 2009] [Villalon- ga et al., 2010]. Data generated from these systems by observing user interactions and the environment in which they take place is extremely valuable for systems designers and service providers. It serves as a basis to enhance their systems and services ac- cording to user’s context (e.g., location, date, time, past and current activities). In the case of urban games, context can be composed of the geolocation of the gamer, the time of day (or night), the proximity of other gamers, the weather forecast, etc.). Analyzing the impact of context information on the user’s actions becomes a key aspect for successful pervasive information systems. Data mining techniques can be used to this end, in order to extract relevant information to support decision and pre- diction [Fayyad et al., 1996] [ Chen et al., 1996]. Several works such as [Ramakrish- nan et al., 2014] are considering data mining techniques for analyzing context data. These techniques can play an important role in understanding and discovering the intrinsic relationships between data. Among them, Formal Concept Analysis (FCA) 2 [Priss, 2006] [Wille, 2005] represents an interesting method of conceptual clustering. It helps extracting implicit knowledge and finding a natural data structure, while asso- ciating user actions to observed context elements. We describe in the following sec- tion how FCA can be used to manage context information. 2.2 Use of Formal Concept Analysis for Context Management The ways mobile interfaces are used depend a lot on user context: geographical loca- tion, time of the day, day of the weeks, meteorological conditions, quality of network connection, etc. understanding the influence of these context elements makes it possi- ble to propose better personalization and recommendation features and to improve adaptation mechanisms. However, identifying relevant context elements (for a given user) from the highly heterogeneous collected context data is a challenge not only because of data heterogeneity, but also because this relevance is personal and may vary significantly from user to user. Context analysis methods are then necessary in order to establish context relevance for a given user. In earlier work [Jaffal et al., 2014], we have used Formal Concept Analysis (FCA) to this end and shown its inter- est to cluster context elements with regard to associated user activities (and vice ver- sa) into overlapping classes. Table 1. Example Formal Context (the values in parentheses correspond to the number of times the applica- tion was used in each context). noitatropsnarT emoh kaerb snoitacilppA kaerb tnaruatseR ytisrevinU noonretfA gninroM gninevE ’stneraP eeffoC hcnuL emoH G3 Gmail 1 (2) 0 0 0 1 1 (2) 1 0 0 1 0 SMS 1 (80) 1 1 (5) 1 (30) 50 1 (170) 1 (40) 1 (20) 1 (40) 1 (60) 1 (10) Tele- 0 0 0 1 (3) 1 (2) 1 (5) 1 0 0 1 (2) 1 phone VDM 0 0 0 1 (4) 1 (2) 1 (5) 1 (4) 0 0 0 1 (2) Flappy 1 0 0 1 1 1 1 1 0 1 1 Bird Youtube 0 0 0 1 0 1 1 0 0 0 0 The input of the FCA algorithms is a binary relation - called formal context - between a set of objects and a set of attributes that describes these objects. Table 1 shows such a formal context in which objects are the applications executed by a user on a tablet, 3 used1. and attributes describe the context in which these applications have been For example, Youtube has been used at home, but also on a 3G network and in the morn- ing. Note that the only possible values in the formal context are 0 and 1, which does not reflect the real numbers of time various applications have been used (indicated in parentheses): SMS are associated to university and restaurant in a similar way in the formal context, although the number of SMS sent in each location is very different. Fig. 1. Galois lattice generated from the formal context of Table 1 Figure 1 represents the Galois lattice generated from the formal context of Table 1. Each cluster of the lattice is called a (formal) concept which groups objects that have attributes in common. For example, this lattice shows that Flappy Bird, SMS and tele- phone applications have all been used in the morning, in the afternoon, in the evening, during transport, from a 3G network and at home. These context elements are com- mon to these applications. One interest in terms of recommendation is that if the user happens to be in a similar context in the future, the information system can automati- cally personalize the interface according to the user preferences, e.g., display a game in the foreground of the screen when the user enters the metro station. Note that the concepts become more and more specific when going from the top to the bottom of the lattice, from concepts with many applications sharing few context ele- ments to concepts with few applications sharing many context elements. Indeed, the concept containing Flappy Bird, SMS and telephone applications can be specialized into a concept that only contains Flappy Bird and SMS applications, with two addi- tional context elements: coffee break and university. The lattice provides a clustering of applications according to the context elements they have been associated to. Conversely, context elements are described with regard 1 This data comes from a dataset built from a questionnaire filled by 28 master students of our university. The goal of this survey was to know in which context they had been using appli- cations (such as social networks, games, emails, etc.) on their tablets during a week. 4 to the applications associated to them. We can notice from the lattice that 3G connec- tion and morning context elements appear in all concepts, which means that they are not significant here and should not be taken into account for recommendations to this specific user. Conversely, the SMS application has been used with all context ele- ments, and is therefore context-independent. We have proposed a methodology to reflect in FCA results the relative importance of context elements on user activities, by refining the input data using a semantic fre- quency measure. In order to build a formal context from collected data, Formal Concept Analysis re- quires to replace the numeric values (Table 1) by binary values in order to represent the relationships between the objects and the formal context attributes in concept lattice. In table 1, we have followed a simplistic approach, by directly replacing null values indicated by users by a zero and any positive value by one. More elaborate methods exist: Pensa et al. [Pensa et al., 2004] use a cut-off threshold to replace val- ues greater than or equal to the threshold values by 1 and the rest by 0. The method in [Ma et al., 2006] is to set a threshold δ, and then turn the context into a binary one by replacing values less than δ with 0 and 1 the others. However, these thresholds are calculated based on numerical values in the formal context regardless of the semantic link between the attributes. The originality of our frequency measure described in [Jaffal et al., 2015] is that it is a semantic measure that relies on a context ontology, illustrated in Figure 2. Fig. 2. Context Ontology 5 The lattice represented on Figure 3 is obtained after the refinement process we pro- pose. It is more interesting than the original lattice of Figure 1 for recommendation and personalization purposes, as more clusters of applications and context elements are generated, leading to a finer-grained classification. In Figure 1 for example, Flap- py bird, SMS, telephone, VDM and Youtube applications have in common the context elements 3G, home and morning, whereas they are separated into 2 concepts in Figure 3: Gmail, VDM and Youtube are associated to 3G and morning, and Flappy bird, tele- phone, VDM and Youtube are associated to 3G and home. It means that after the re- finement strategy, Flappy bird and telephone are no longer associated to the morning (and will therefore not be considered as very likely to be used in the morning). We also notice that some context elements have disappeared from the lattice, as they have no sufficient impact on the user applications: coffee break, restaurant, parents’ home and lunch break. It is interesting to remark that the number of concepts in the lattice increases despite the disappearance of 4 context elements. This shows that the preci- sion of the obtained results has indeed increased significantly with regard to the initial lattice. Fig. 3. Galois lattice obtained after the semantic refinement strategy of the formal context 2.3 Interest for Video Games We have shown previously how the semantic refinement strategy we proposed leads to a better understanding of context impact. Such understanding represents an im- portant step towards personalization and recommendation features. Urban games have many common features with pervasive information systems: users interact with their system in a transparent and mobile way. Examples of context elements that could be relevant in urban games are shown on Figure 4. 6 Fig. 4. Example of context elements in urban games The application of the approach we have proposed in the previous section is illustrat- ed on Figure 5. Fig. 5. Context management applied to urban games This methodology could be could be interesting in order to make the following ac- tions possible: 1) Analysing gamers’ behaviour in similar contexts (Figure 6) For instance, we can imagine that when gamers are located in a specific place and use a specific device, they systematically choose to perform a precise activity, whereas if 7 their context changes (for instance same place but other device), they choose to do something else. Information about gamers’ behavior according to their context can be very useful for game designers in order to propose different kinds of services accord- ingly. We can imagine that the expected behavior is the one obtained with the use of a smartphone, and propose a new clue to tablet users to help them perform the expected activity. Fig. 6. Analysing gamers’ behaviour in similar contexts 2) Analysing the contexts generating a similar behavior (Figure 7) In a similar way, game designers could be very interested in knowing the contexts leading to the same behavior. Let us consider a key activity in a game, which a set of players has a lot of difficulty to perform. The analysis of the players’ contexts may highlight combinations of context elements that confuse these gamers and prevent them from completing the task correctly. As with the previous example, the game designer could then decide to offer a clue to the players who are within these contexts to help them go further in the game. Fig. 7. Analyzing the contexts generating a similar behavior 3) Recommending specific actions according to the gamer’s context (Figure 8) 8 Context analysis, performed at run time, can also be a way to recommend some ac- tivities to the gamers, according to their current context. For instance, taking into account weather modifications, the gamer can be proposed in-door or out-door quests, to keep his interest in the game by playing in comfortable conditions. Fig. 8. Recommending specific actions according to the gamer context 3 Digital Traces Analysis and Intention Mining Although context management is essential to understand the interactions of users with an information system (and, similarly, the actions performed by players of video games), we believe that it is also necessary to capture the intentions of the users when they perform actions. Indeed, we claim that the actions executed by information sys- tem users follow a strategy that aims at satisfying an intention within a specific con- text. Context management has been addressed in the previous section; we know focus on the identification of users intention through a process we call intention mining, described in Section 3.1. 3.1 Process and intention mining: state of the art Analyzing activities in the Information Technologies (IT) context is a very lucrative business. While surfing on websites, all our activities—products and services search- es, on-line game activities, comments and posts on blogs or social networks, among others—are traced and analyzed to propose us the most adapted advertisements. Games, as other kind of software, leave digital traces of the users that can be ob- served, stored and analyzed, as illustrated on Figure 9. Video and computer gaming have become increasingly popular branches of the entertainment industry and lots of traces are generated. Due to the large variety of available games, players have a plethora of choices. User selection can be influenced by the content of games, the quality of the services, the popularity of games, the choices of in-game friends, etc. Retaining players with longer player lifetime can yield more revenue for game com- 9 panies. The ways a player interacts with in-game friends, as well as his/her in-game play strategy are also some interesting facts that can be analyzed in the traces. Tracing the gaming activities of a user can then become the source of a lot of knowledge. Analyzing game traces allows understanding the characteristics of games such as player behaviour [Suznjevix et al, 2011][(Ducheneaut et al, 2007], traffic analysis [Chen et al, 2006][Kinicki et al 2008], resource management [Chambers et al, 2010][Denault et al, 2011], etc. Traces can be stored in a specific repository as the Archive2 Game Trace which allows analyzing large and public game traces datasets [Guo et al, 2012]. Fig. 9. Digital traces capture The player behavior, as any user behavior, can also be described as a process, and process can be defined with process models. The approach of tracing and analyzing activities has been used with great success in the field of process mining, in particular for enhancing software development. [Weijters and van der Aalst, 2003] have pro- posed methods and tools based on Petri nets and α type algorithm to analyze events logs (traces) in order to discover process models. Process mining approaches aim at modelling users' behaviors in terms of sequences of tasks and branching in an auto- matic way [Van der Aalst et al., 2004] [Agrawal et al., 1998] [Cook and Wolf, 1998] [Datta, 1998] [van Dongen, 2004] [Herbst, 2000]. The resulting (i.e., mined) process flexibility3. models are activity-oriented. However, activity-oriented models lack Processes can also be seen as teleological [Ralph and Wand 2008]. A teleological process is a process that takes into account the teleological behaviors of process en- actment (behavior attached to the notion of goal). It describes the intentions (goals, objectives) associated with a result that an individual wants to obtain. Many research works in intentional process modelling demonstrate that the fundamental nature of processes is mostly intentional and the processes should be modelled from an inten- tional point of view [Davis et al., 1989] [Plihon, 1996] [Rolland et al., 1999]. The notion of context also plays a key role for the intention, since a given intention 2 http://gta.st.ewi.tudelft.nl 3 We will further discuss process models in Section 4. 10 emerges in a given context, which not only promotes its appearance, but also influ- ences the realization of this intention [Rolland, 2005]. Process mining applied to in- tentional process models is called intention mining, which is at the heart of our solu- tion. This original concept of “Intention Mining” aims at automatically discovering intentions from traces of user activities [Hug et al., 2012] using pattern mining [Zaki, 2001]. Hidden Markov models technique [Baum et al., 1966], a specific type of Dy- namic Bayesian Networks, was used in the Map Miner Method [Khodabandelou et al., 2014] [Khodabandelou, 2014]. Moreover, [Epure et al., 2014] proposed an ap- proach using an unsupervised technique and a knowledge tree to build intentional process models at a low level of abstraction. Information about user intentions within a specific context, together with knowledge of other user behaviors with the same intentions in similar contexts is the key to provide them with adapted recommenda- tions. 3.2 Potential interest of intention mining in the area of video games In this section, we describe how the intention mining techniques presented previously could be applied in the context of video games. Player behaviors traces, analyzed with intention mining techniques, could in particular make the following activities possi- ble: 1) Discovery of the underlying player’s intentions and strategies hidden in the trac- es. Intention mining allows identifying whether there exists a generic behavior of the players, through the identification of players’ usual activities and the usual ordering of these activities, as shown in Figure 10. For instance, if gamers usually change their direction after finding a clue, this information could be useful for the game designer in order to offer new services adapted to this generic behavior (for instance to auto- matically display a map when the user enters the clue in its device). Moreover, from a marketing point of view, knowing the gamer’s general behavior can add some high- lighting information when designing ads for the game. Fig. 10. Extraction of a general player behavior 2) Conformance checking: comparison between the observed behaviors of players and what is expected from them by the game designers. 11 Once the actual behavior of players is identified, it is possible to compare it with the one expected by the game designer, as shown on Figure 11. Some unexpected issues may appear, such as a gamer intention never achieved, a strategy never used, a new intention that was not expected by the game designer, etc. Fig. 11. Comparison between actual and expected players behaviors 3) Enhancement of the gaming process model If the observed player’s behavior is not consistent with the game designer’s goal, e.g., if some paths are never followed, the game designer can add features in the game to guide players towards these paths (see Figure 12). Conversely, if some strategies used by the players are more efficient than the ones expected by the game designer, the expected model can be enriched by adding the identified new behaviors in the model. Fig. 12. Refinement of the player process model 4) Recommendation of specific steps to the gamer, at run time, based on their sup- posed reasoning while performing activities and based on their current context. It is possible to recommend the next activity to a gamer, following the percentage of players that executed this activity when exhibiting the same behavior earlier in the game (see Figure 13). For instance, if the generic behavior consists in exchanging information with other gamers just after a specific step, the device used by the gamer 12 can propose this action to him automatically as the next step option. The game can then become more adaptable to the gamer behavior, at run time. Fig. 13. Recommendation to players based on context and intention 4 State-Oriented Adaptive Process Modeling Sections 2 and 3 of this paper are dedicated to the analysis of user context and inten- tions. These elements allow for personalizing user interaction with their mobile inter- face and for supporting them with relevant recommendations based on their context intentions and previous experience. We now focus on the adequate way to model the underlying scenarios followed by users during their interaction with their mobile interfaces. We first discuss the chal- lenges of game scenario modeling, then we briefly overview the modeling formalisms used in the domain of software engineering and Business Process Management. Final- ly, we discuss the interest of statecharts and life sequence charts (LSC) modeling formalisms for the domain of urban games. 4.1 Challenges of game scenario modeling The objective of a multi-player game is to create a virtual or semi-virtual (in the case of urban games which mix virtual and real worlds) environment where players are placed. During the game, players follow some scenarios in order to achieve one or multiple goals through communicating and collaborating with each other. A game scenario needs to specify the objects, their positions in a virtual world (e.g., a maze) and some puzzle to be solved or some task to be carried out. It induces a par- tially ordered set of actions that the player must perform [Gal et al., 2002]. In general, the game scenario is a description of the main game phases and the navigation be- tween them. The main challenge in game scenario is to remain “unpredictable” for the player while driving this player towards some (probably implicit) goal. Unpredictability is im- portant: first of all, if similar situations occur too often and if the reaction of the game 13 environment and its objects can be predicted, the player may get bored very rapidly. The scenario of the game needs to evolve together with the player. Unpredictability is extremely important in urban games which mix real and virtual worlds, as the artificial life needs to bear a resemblance to real life. As real life is rich, unpredictable and evolving, the player needs to feel it in the simulated game envi- ronment too. Not only the situations encountered by the player should look “unique” but also the opportunities (what the player can and cannot do) in each particular situa- tion should be defined dynamically and not embedded into the scenario. The game scenario therefore needs to unfold progressively, at run-time, and all players can be also considered as scenario designers. The game scenario thus can be compared to a process, for which the requirements for flexibility and “unpredictability” support are also relevant. Game designers can there- fore reuse process modeling languages to model the game scenarios. In the following section, we review existing modeling paradigms used for business process manage- ment. 4.2 Process modeling: state of the art Conventional activity-oriented formalisms for process modeling represent a process as a sequence of activities. These formalisms have been mentioned in Section 3. The most popular examples of these formalisms include BPMN and UML activity dia- grams. These formalisms are mostly prescriptive and provide only limited support for process adaptability at run-time. Goal-oriented is another group of modeling formalisms that, compared to activity- oriented formalisms, represent a process as a sequence of goals. The MAP modeling notation mentioned earlier in this paper is an example of goal-oriented languages. Context-driven goal-oriented process models such as [Rolland et al., 1995], [Soffer and Yehezkel, 2011] and [Pohl and Weidenhaupt, 1997] support automated recom- mendations and user guidance, providing that for each goal all the situations (states) in which this goal is achievable are known. In urban games, we might face unpredictable sequences of events and non-repeatable execution scenarios; it would therefore be hard, if at all possible to model relations between various process situations, goals and activities that must/can be executed in order to achieve these goals. In the literature, several major perspectives of the process models are specified [Ja- blonski and Bussler, 1996]: the control flow perspective that captures the temporal ordering of process tasks, events, and decision points; the data perspective that cap- tures the lifecycle of data objects (creation, usage, modification, deletion) within the process; the resource perspective that describes how the process is carried out within 14 the organization and deals with roles and resource assignments; the operational per- spective that addresses the technical aspects of process execution and specifies the elementary process tasks and their assignment to concrete applications or application components of the organizations; the context perspective that describes the attributes related to the process execution context; the performance perspective, addressing the process cost effectiveness. Flexibility along control flow, data and resource perspectives is widely addressed: modeling languages (e.g., BPMN, EPS, C-iEPC, Declare) enable the specification of processes where the activities can be skipped, repeated, or their ordering can be cho- sen depending on the context (at run time). Some languages (e.g., configurable mod- els in C-iEPC [La Rosa et al., 2009]) allow for dynamic resource and data manage- ment. Flexibility along the context and the operational perspectives, however, are still challenging. Flexibility along the operational perspective would allow the player, for example, to not only select an activity to execute from the predefined list, but also to propose an alternative that was probably not even thought of at design. To search for a formalism that would support such flexibility, we appeal to the do- main of complex reactive systems. According to [Harel et al., 2008], the design of games and complex reactive systems share a common set of challenges: 1) The need to construct a system from a set of many interacting small units; 2) The importance of emergent properties, unique char- acteristics of the global system behavior that emerge from the interaction between the small units but are not always evident from the rules of behavior of each of the com- ponents by its own; 3) The crucial role of visualization and graphics; and 4) the need to provide strong tool support that frees the mind and encourages creativity of the designers, allowing them to focus on the big picture and the important parts rather than on implementation details. In the games where multiple players are placed in virtual reality (i.e., an “artificial life”), each \"object\" or “actor” of the game is specified as a random automaton send- ing and receiving stimuli from other objects. The global behavior is the results of the interactions between local ones [Gal et al., 2002]. This allows for simulating complex behaviors and avoiding repetitive scenarios. For the specification of unpredictable and flexible game scenarios, the state-oriented modeling paradigm and the formalisms of statecharts [Harrel, 1987] can be beneficial. 15 4.3 Applying the statecharts modeling formalism to game processes Within the state-oriented paradigm, a process (a game in our case) can be described by a set of states and transitions between these states. A game starts at an initial state and terminates at a final state. A state transition is triggered when some condition is fulfilled. The sequence of states and transitions that leads from the initial state to the final state can be seen as a game scenario. Activities carried out by a player depend on the current state of the system (the game environment in our case) and the game scenario is adapted at run time, according to the evolution of the game. Examples of state-oriented modeling formalisms include state machines in UML [Rumbaugh et al., 2004], generic state-transition systems or state machines, such as FSM [Plotkin, 1981] or Petri Nets [Murata, 1989], and statecharts by D. Harel [Harel, 1987] created for the specification and analysis of complex discrete-event systems. The core difference between state-oriented (and goal-oriented) formalism is the way they specify the transition between states (or goals) within a process: whereas some formalisms (e.g. Petri Net) state that a transition is a result of an execution of an activ- ity, the others (e.g., statecharts) state that a transition is a result of a triggering event. In the first case, all activities need to be explicitly modeled and thus these formalisms do not provide much flexibility compared to activity-oriented formalisms mentioned above. Whereas in the second case, only the (desired or undesired) triggering events have to be specified. The activities that “produce” these events may remain unknown and can be defined at run time. This provides a supplementary degree of flexibility along the operational perspective required by urban games. Following this paradigm, a game scenario specification can be divided into two parts: the state-transition part, defined with a set of states and transitions between states and their triggering events, and the activity part, defined by a list of activities specified by their preconditions and outcomes. The process enactment can be seen as a dynamic selection of activities to produce some outcomes (events) that make the process pro- gress towards its (desired) final state. In [Rychkova et al., 2015], we proposed the basis for implementing the state-oriented formalism of statecharts for modeling case management processes, where the case management scenario, similarly to a game scenario, depends a lot on the context and requires flexibility. In [Kushnareva et al., 2015], we elaborated on this basis, propos- ing an approach for interactive modeling and simulation-based analysis of process specifications for crisis management. Again, crisis handling can be easily considered as a subject of a game. This approach is based on statecharts formalism and tools4. YAKINDU statecharts 4 Yakindu Statechart Tools version 2.4. for Eclipse Luna, 2014, http://statecharts.org/ 16 The state-oriented paradigm allows us to exclude activities from the process design: we can state that \"any activity is good as soon as it produces a desired outcome\". In particular, it enables deferred activity planning, that gives more freedom to the pro- cess participant in choosing an activity that is adapted for a concrete situation. The system based on statecharts model cannot be considered as a “management” sys- tem that automatically defines and executes the scenarios, but rather a “recommenda- tion” system that only advises the process participant on scenario planning but not drives him/her through a sequence of predefined steps. That is why this formalism is also beneficial for game design. In our vision, the ”a priory” statecharts specification of a game can be analyzed using graph theory algorithms. The objective of the analysis is to search and optimize a path from some current state of the statecharts model to its target state, representing the goal of the game. As a result, the ”best next state to visit”, ”best next transition to fire” and, consequently, ”possible activities to execute” are recommended to the play- er. The state-oriented model will address the internal behavior of objects involved in the multi-player game in a simulated virtual world. The internal behavior of an object describes how this object can react on any external stimuli in any context (i.e. situa- tion). This model can be complemented by a model of inter-object behavior [Harel et al., 2008] that describes how this object interacts with other objects in particular situa- tions. In [Harel and Marelly, 2003], the inter-object behavior is modeled with scenari- os that specify the interactions between objects. In [Harel et al., 2008], the authors state that “the approach makes it possible to de- scribe behaviors of different parts of the system using different reactive system design languages and tools. Among the advantages of this approach is the ability to use exist- ing analysis tools to understand the game behavior at design time and run time, the ability to easily modify the behavior, and the use of visual languages to allow various stakeholders to be involved in early stages of building the game.” Let us illustrate our approach on a simple example inspired from urban games. Figure 14 represents a statechart that specifies a (gamer’s view on a) game with a state ma- chine defined at three hierarchical levels. On the higher level, only two states are defined: Disconnected and Connected. The transitions between these states are trig- gered by the events produced by a gamer: e1 – when the gamer connects to the game, and e2 – when he/she disconnects. 17 Fig. 14. Example statechart inspired by urban games When connected (the second hierarchical level), the gamer can be in one of the three (exclusive) states: Navigation, when the player moves in a physical environment (e.g., walking on the street or visiting buildings, as shown on the third hierarchical level) with an objective to pursue another player, to reach some specific place defined by a mission or to search for a clue; Communication, when the player meets an actor or another player with whom he/she can communicate in order to search for a clue; Ex- ploration, when the gamer explores the environment around him/her (e.g., a building) in order to search for a clue. The transitions between these states are defined by logical expressions over events (e1, e2, …) . The list of events is presented in Fig. 15. Events can occur as a result of a player’s actions (these events are prefixed with Player in the diagram) or can be produced by the game environment (these events have the Game prefix). A black circle in the diagram identifies a substate that will be entered “by default” when its parent state is visited: for example, when the Navigation state is entered for the first time, its substate “On_the_street” is activated. A black circle with a H* symbol means “entering the state by history”: for example, once the gamer leaves the Communication or the Exploration state, he/she returns to his/her previous position in the Navigation state. 18 Fig. 15. Events list The main idea is that the activities that the gamer can execute are not specified at all by this model. It only specifies the events that can trigger state transitions. The player is free to perform any activities, assuming that they will lead to the generation of a desired event. For example, in the Navigation state, the gamer can walk, run, stop, turn, take a metro, ride a bicycle etc. Along these lines, while in the Exploration state, the gamer can use any means at his/her disposal in order to search for a clue. This means that concrete activities can be defined by the player based on particular situa- tion. This is especially important for urban games, where the game is taking place in a real environment. Obviously, this example is very simple. When the scenario becomes more complex, more specific events associated with the game mission will be defined and the player behavior will be specified with more details using new hierarchical levels. The statechart represented on Figure 14 has modeled with the YAKINDU open source environment for modeling and simulating statecharts. This model can be simu- lated and refined, providing more details about the game: new hierarchical levels, states, events, and transitions can be specified. 5 Conclusion and Perspectives We have presented how context management, intention mining and state-oriented adaptive process modeling could be beneficial to game process analysis and model- ing, in order to better understand the behavior of players during their gaming experi- ence and provide them with relevant recommendations. This section first summarizes 19 the contributions presented in this article. Finally, as the recent advances presented in this article have been developed independently, we present one of our perspectives for future work, which consists in integrating them into a single solution. 5.1 Conclusion: overall approach The overall objective of the various contributions presented in this article is to provide users of an information system with relevant recommendations, which take into ac- count their context and their intentions, as illustrated on Figure 16. We propose to build a recommendation system based on the model of user behavior using a state- oriented modeling formalism that integrates context information and intentions of users. User intentions are extracted from traces with intention mining algorithms, whereas context management relies on Formal Concept Analysis. The obtained statecharts can then be used in order to identify the next best state for the process according to the current situation of the user and recommend the most efficient path towards this next best state. Fig. 16. Overall approach 5.2 Perspectives: towards an integrated solution To achieve autonomic computing, IBM has suggested a reference model for autonom- ic control loops [IBM, 2003], which is usually called the MAPE-K (Monitor, Ana- lyze, Plan, Execute, Knowledge) loop, illustrated on Figure 17. The MAPE-K loop is similar to the generic agent model proposed by [Russel and Norvig, 2003], in which an intelligent agent perceives its environment through sensors, and analyzes the ob- tained information to determine actions to execute on the environment. In the MAPE-K loop, the adaptive system represents any software or hardware re- source that can be adapted to better responds to the changing context. Thus, the adap- 20 tive system can for example be an operating system, a specific software component in an application (e.g., the query optimizer in a database), a cluster of machines in a grid environment, a wired or wireless network, a middleware, but also an adaptive game. Fig. 17. MAPE-K loop architecture The elements of the MAPE-K loop and their interactions are presented as follows: - The monitor function collects the details from the internal and external managed resources. For instance, the managed resources in an urban game can be metrics (e.g., offered capacities or powers, throughput of the network for distributed games, and score), configuration property settings and so on. In addition, the monitor function periodically aggregates, correlates, filters this information and sends it to the analyzer. With regard to the contributions presented in this article, the monitor is responsible for context management and the monitored resources are those that appear in the context ontology. - In our case, the analyzer function performs the intention mining. Moreover, it evaluates whether changes are required in the adaptive system (i.e., whether rec- ommendations are needed in the game) according to gamer intentions and to the information provided by the monitor function. Usually, changes are required when the current system does not satisfy the constraints provided by the monitor or when a better solution exists that satisfies these new constraints. In our case, the analyzer is in charge of finding the next best state in the statecharts process model, relying on context information and on gamer intentions (which are part of the knowledge). If changes are required in the system (i.e., in the game), a change request is sent to the planner. - The planner function structures the actions needed to achieve goals and objec- tives. The planner creates or selects a procedure to enact a desired alteration in the managed resources. The planner’s role is to identify the path that should be taken in order to reach the next best state. Thereafter, the plan is passed to the ex- ecutor. - The executor function changes the structure or the behavior of the adaptive sys- tem (the game), and sometimes also of the adaptive context [Sawyer et al., 2012], 21 according to the information provided by the planner. In our case, it consists in actually providing users with recommendations. - The knowledge base contains the data shared by the monitor, analyzer, planner and executor functions. This shared knowledge, is created/updated by the moni- tor, analyzer and executor. In our case it gathers in particular the context ontolo- gy, and the behavior model.",
  "2016-KRLG": "I. INTRODUCTION\nCase management process (CMP) [1] can be characterized\nby the following: it is driven by emergent knowledge about\nthe case subject or the environment; largely based on human\nexpertise; highly unpredictable; difficult to replicate; hard to\nanalyze and improve as no HOWTOs available. CMPs have\nmultiple applications: crisis situations handling, patient care,\ninsurance application, computer games development, etc.\nThe features of a case management support system often\ninclude case artifact organizer/repository, task scheduler, report generator, document sharing, business calculator based\non business rules, etc. Definition, assessment of alternative\nscenarios and decision-making remains a responsibility of a\nhuman expert - a case manager.\nWhen a scenario is not prescribed for a given situation\nor when the prescribed one cannot be implemented for one\nreason or another, the case manager has to (i) identify relevant\nparameters that can affect the scenario (e.g., events, resources,\nlessons learned from the previous situations, guidelines, manuals etc.); (ii) assess the situation based on the observed\nparameters, evaluate/compare scenarios; (iii) make a decision\nabout the course of action.\nWhen a number of influential parameters is high, a number\nand complexity of possible scenarios grows exponentially. The\ncapacity to process the received information and to compare\nmultiple scenarios can be extended by using the IS supporting\nsystems (e.g., data analytics). Whereas the capacity to identify\nthe relevant parameters and to make a decision (i) and (iii)\nrelies strongly on the experience of a person. Automated\nassistance in scenario evaluation and activity planning (ii) can\nbe provided by an IS.\nInformation systems for business process management\n(BPM) are widely developed today. These systems mainly\nexploit an activity-centric paradigm, where a process is described as a flow of activities. Process flexibility is one of the\npreoccupations for a modern BPM system: existing solutions\nsupport coordination between human actors and adaptation\nof a scenario at run-time. For example, ordering of activities\n(scheduling / skipping / repeating) can be configured at runtime. An activity can be also parametrized so that a particular\nvariant can be selected at run-time based on the situation.\nTo ensure this flexibility, various mechanisms are proposed,\nincluding ad-hoc subprocesses defined in BPMN 2.0 [2] or\nprocess variability and variation points [3].\nHowever, the main challenge of an activity-centric\nparadigm is that all the process variants (variation points and\nactivities that follow) have to be defined in advance (at design\ntime). For CMP, it is not always the case: not only a case\nmanager needs to decide on the order of activities to execute\nat run-time, she may also have to propose new activities that\nfit the situation and are not currently in the model. Semantics\nbased on activities cannot specify the latter.\nIn our previous work [4], we discussed a state-oriented\nparadigm and the operational semantics of CMP based on\nstatecharts [5][6][7][8]. A statechart diagram depicts a set of\nstates and transitions between states that can be triggered by\nan event (or a combination of events). In a statechart model,\na process is defined in terms of results or ”milestones” one\nneeds to achieve (modeled as states) and conditions or events\nrequired to achieve them. Whereas the events are resulted from\nand conditions are met thanks to execution of activities - the\nlatter are not a part of the model. There are several important\ndifferences with the activity-centric models:\n- in a state-oriented model, transitions can happen and the\ngoals can be achieved as a result of certain external events\nhappening in the environment. For example, in a case of flood\nconsidered in our previous work [4], the crisis can be terminated once the water level subsides to normal, independently\nof the activities executed by the case manager.\n- state-oriented model requires the results (process goals) to\nbe known in advance (states of the model). The activities that\nhave to be executed in order to achieve them can be defined\nat run-time.\n- in a state-oriented model, several paths from a current\nstate to a target state (the goal of the process) usually exist.\nThis would be equivalent to an activity-centric model with\nvariation points defined after each task.\nIn [4][9], we demonstrated that the state-oriented approach\nand statecharts models can be successfully used to describe and\nvisualise case management processes. We created a model of\nFlood management process in Yakindu statecharts tool [10]\nand illustrated how multiple scenarios can be created during\nthe execution of this process.\nIn this work, we formalise the state-oriented process models with higraphs [11]. A higraph is a mathematical graph\nextended to include notions of depth (defined hierarchy) and\northogonality (a Cartesian product or partitioning), which\nmakes them a combination and extension of Euler graphs,\nhypergraphs and Euler Cirles (or Venn diagrams).\nHigraphs semantics provides mathematical foundation for\nstatecharts and eventually enables a wide panoply of (graph)\nalgorithms for process analysis. In particular, we explore how\nthe algorithms of path search and optimization on graphs can\nbe used in order to support run-time decision making and\nadaptive scenario planning for unstructured, non-workflow\nprocesses.\nHigraph semantics and statechart/higraph transformation\nare a central part of the research illustrated in Fig. 1: whereas\nthe statecharts are used for visualisation and simulation of a\nnon-workflow process, underlying formalism of higraph allows\nfor (run-time) process analysis and optimisation.\nIn this paper, we (i) report on the transformation of\nstatecharts diagrams in higraphs, then we (ii) discuss how the\ngraph algorithms (e.g., the shortest path) can be used in order\nto identify the best scenario and (iii) how the run-time analysis\nof higraph can help the case manager to choose/propose an\nactivity that would lead to realisation of this scenario.\nOur research paves the road to creation of an automated\nrecommendation support for the case manager: the latter can\nuse a state-oriented model to fix the milestones and the main\ngoals of a process (states) and to define the conditions under\nwhich these milestones and goals can be reached (transitions).\nAt run time, the current state of the model reflects the current situation of a process. At each current state, the proposed\nalgorithm can identify the next milestone that contributes best\nto the main process goal. It can also suggest the list of activities\nto execute. The case manager may choose one of the suggested\nactivities or may define her own activity. If the selected activity\nfails or another external event occurs, the target milestone may\nnot be achieved. In this case, the new alternative milestone will\nbe defined.\nThe reminder of this paper is organised as follows: In\nSection II, we explain our motivation and introduce higraphs,\nstatecharts and related theories. In Section III, we describe\nour state-oriented approach for modeling and analysis of nonworkflow processes. In particular, we discuss the importance\nof the formal semantics of higraphs within this approach.\nIn Section IV, we present the transformation of statecharts\nto higraphs. Section V covers higraph analysis methods and\nautomated recommendations enabled by statecharts/higraph\nsemantics. Section VI presents our conclusions.\nII. BACKGROUND\nA. Motivation\nUp to now, BPM approaches based on the activity-centered\nparadigm remain the mainstream paradigm for process-support\nsystems design. Within this paradigm, a process is specified\nwith an (ordered) set of activities that the system carries\nout during the execution. To provide better support for process flexibility, activity-oriented formalisms are extended with\ndeclarative parts such as constraints or configurable elements\n[12][3]. Ad-hock subprocesses in BPMN 2.0 allow for specification of activities without giving a particular order of\nexecution.\nThe main challenge of an activity-centric paradigm is that\nall the activities available for execution have to be predefined\nand included in a process model. But it does not always work\nfor an unstructured process, such as CMP.\nDavenport [1] defines a case management process (CMP)\nas a process that is not predefined or repeatable, but instead,\ndepends on its evolving circumstances and on decisions regarding a particular situation, i.e., a case.\nAs reported in [13], numerous efforts to create an efficient\nIS for CMP support in the industry are compromised by the\nattempt to deal with CMP the same way as with regular\nbusiness process - i.e., representing case management by a\nworkflow and focusing on the (predefined) sequence of tasks.\nSeveral research groups report on approaches to design\nand specify case management processes: in [14] and [15] an\napproach that combines state- and activity-oriented paradigms\nis presented. The case handling paradigm is presented in [16].\nIn [17], the authors outline the present solutions for case\nmanagement. They position an intelligent assistance and guidance to knowledge workers as one of the main challenges: ”the\nnext generation tools should be intelligent enough to bring the\nright and needed information to the knowledge worker in the\nright time in a proactive manner (e.g., recommend course of\naction, identify new relevant sources of information that may\nimpact the decisions made in the course of a case, etc.)”.\nProviding an intelligent support and guidance for scenario\nplanning is the main purpose of our work.\nIn this work, we represent state-oriented models with\nhigraphs and, in order to guide the case manager suggesting\nher the best process scenario, we exploit some algorithms for\ngraph analysis.\nB. Higraphs\nHigraphs are a combination and extension of Euler graphs,\nhypergraphs and Euler/Venn diagrams. A graph in the simplest\ncase can be defined with a set of nodes and a set of edges,\nwhere an edge is connecting a pair of nodes (a binary relation).\nA hypergraph is a graph in which an edge (a hyperedge)\nconnects not a pair, but a subset of nodes.\nA higraph is a quadruple:\nH = (B, \u0000, ⇡, E)\nwhere B is a finite set of elements, called blobs, and E, the\nset of edges, is a binary relation on B:\nE B B.\n✓ ⇥\nThe subblob function u is defined as\n\u0000 : B 2B\n!\nIt assigns to each blob x B its set \u0000(x) of subblobs and is\nrestricted to being cycle free. 2\nThe partitioning function ⇡ is defined as:\n⇡ : B 2B ⇥ B\n!\nassociating with each blob x B some equivalence relation\n⇡(x) on the set of subblobs, 2 \u0000(x). This is really just a\nrigorous way of specifying the breakup of x into its orthogonal\ncomponents, which are now defined to be the equivalence\nclasses induced by the relation ⇡(x).\nAn Euler/Venn diagram consists of simple closed curves\ndrawn in a plane. The inside region of a curve depicts a set. The\noutside region of this curve depicts the elements that do not\nbelong to this set respectively. Curves whose interior zones do\nnot intersect represent disjoint sets; two curves whose interior\nzones intersect represent sets that have common elements and\nso on. Therefore, a diagram can illustrate logical relations\nbetween sets, such as being a subset of, being disjoint from,\nand having a nonempty intersection with.\nA node of a higraph, called a ’blob’, represents a curve\nfrom Euler/Venn diagrams and allow for reasoning about its\n’inside’ and ’outside’ regions using visual, topological representation. The blobs of higraphs can be connected by edges or\nhyperedges. Therefore, compared to regular graphs, higraphs\nvisually distinguish between structural (set-theoretical) and\nother (domain-specific) relations that can be defined between\nthe nodes: the former relations are depicted by the blobs and\ntheir topological configuration, the latter are depicted by the\n(hyper-)edges between blobs. Furthermore, higraphs extend the\nEuler/Venn diagrams with a possibility to represent a Cartesian\nproduct - another structural relation between nodes of a graph.\nCompared to a regular graph, higraph provides a superpolynomial saving in size of the description.\nFormal syntax and semantics of higraphs defined by Harel\nin [11] makes accessible a wide spectrum of techniques\nand algorithms for qualitative, quantitative analysis as well\nas optimisation, validation and verification. Algorithms for\nanalysis of higraphs are proposed in [18] (shortest path and\nminimum cover problems, calculating distance, Hamiltonian\ncycle, bipartition test). Other algorithms from graph theory\ncan be applied to higraphs as well.\nC. Statecharts\nA number of applications of higraphs is discussed in\n[18][19]. In particular, higraphs are suitable for knowledgerepresentation, where both structural and domain-specific relations must be defined. The language of statecharts [5] is\nan application of higraphs for modeling behavior of complex\nevent-driven systems. Here the Cartesian products specify the\northogonal or concurrent states of a system.\nStatecharts are a higraph-based extension of standard statetransition diagrams, where the blobs represent states and\narrows represent transitions.\nStatecharts = state diagrams + depth + orthogonality +\nbroadcast communication.\nDepth is represented by the insideness of blobs.\nOrthogonality is the dual of the XOR decomposition of\nstates, in essence an AND decomposition, and is captured by\nthe partitioning feature of higraphs, that is, by the unordered\nCartesian product.\nIII. HIGRAPHS AS A TOOL FOR CASE MANAGEMENT\nPROCESSES (CMP)\nFig. 1 illustrates our state-oriented approach for modeling\nand analysis of non-workflow processes. It consists of four\ninterconnected parts:\nApplication domains\n•\nTheories\n•\nModels\n•\nTools\n•\nApplication domains depict different cases where the stateoriented approach can be used. For example, statecharts and\nhigraphs were originally used for modeling the industrial\ncontrol systems. Other domains can be also considered.\nModels depict the formalisms we have chosen to represent\nthe problems from the application domains. Statechart operational semantics is used to visualise and simulate a process\n(e.g., with Yakindu SCT). Higraph denotational semantics\nprovides an underlying formalism for further process analysis\nwith the Tools.\nTheories (e.g., graph theory, etc.) are used to create the\nalgorithms and build these tools.\nThe selection of tools and algorithms depends on a given\nproblem and on the concrete application domain. Once the\nanalysis is performed, it needs to be interpreted in the Application domain. The overall objective of this approach is to\ncreate a toolbox for various types of analysis originated from\ngraph theory and other related theories.\nIn our approach, we propose to model a CMP with a\nstatechart paradigm (this is illustrated by the arrow from the\nApplication domains to the Models in Fig. 1). The stateoriented paradigm does not specify concrete activities that\nshould be executed during the process. The process scenarios\ncan be discussed in more business-oriented terms such as\nFig. 1. State-oriented approach for modeling and analysis of non-workflow\nprocesses\ngoals and milestones (modeled as states) and constraints and\nrequirements (modeled as events and conditions).\nStatecharts provide a simple notation for visualizing the\nprocess, focusing uniquely on WHAT will be done instead\nof HOW it will be implemented. One can also simulate a\nstatecharts model, playing different process scenarios, testing\n”improbable” cases and improving the understanding of the\nprocess [20].\nThe problem of planning the course for action by the case\nmanager can be reformulated in terms of statecharts as follows:\ngiven a current state S c in the process and a target state T (a\ndesired objective)\n1) what would be the best1 sequence of states\nS 1 , S 2 , ...T - the best process scenario - that would\nlead us from S c to T ?\n2) what are the activities that should be executed at S c\nin order to lead to realisation of this best scenario?\nTo solve this problem, one should be able to reason\nabout the process with mathematical rigour. The formalism\nof higraphs discussed below provides required mathematical\nfoundation and enables the set of tools for process analysis\n(this is illustrated by the arrow from the Models to Tools in\nFig. 1). In particular, we are interested in graph algorithms that\ncan be executed iteratively, (re)defining the new current state\nS c and the best scenario at this state.\nThe defined state-oriented model does not contain the\nactivities; it only describes the effects (postconditions) that\nare required in order to trigger a transition from one state to\nanother - the definition of activities should be supported by\nthe Application Domain (this is illustrated by the arrow from\nthe Tools to Application Domains in Fig. 1).\nPotentially, the tools can be also used in order to reason\nabout model (higraph or statecharts) quality (this is illustrated\nby the arrow from the Tools to Models in Fig. 1).\n1We do not specify the criteria for the ”best” scenario as they can vary\ndepending on the situation (e.g., the cheapest, the fastest, the scenario where\nthe states S x , S y will be avoided etc.).\nIn the next sections we explain how a higraph can be\ncreated from a statechart diagram and how this higraph can\nbe further explored using graph algorithms. On the example\nof the classical algorithm for the shortest path, we illustrate\nhow the best process scenario can be identified. This will\nanswer the first question formulated above. Then we show\nhow from the ”best sequence of states” we can identify the\n”best activity” that can be recommended to the case manager.\nThis will answer the second question formulated above.\nIV. FROM STATECHARTS TO HIGRAPHS\nWe use Yakindu SCT for modeling statecharts where the\nmodels are stored in XMI files. The tree structure of a statechart model includes regions, vertices, outgoing and incoming\ntransitions with specific attributes (e.g., id, name, type, label,\netc.). The resulting models define a complex structure that may\ninclude hierarchy (i.e., regions and vertices can be placed one\ninside the other as many times as required) and orthogonality\n(i.e., regions and vertices can be placed in parallel as many\ntimes as required).\nIn order to find and recommend a case manager the next\nbest step while running the process, we have transformed a\nstatechart model into a higraph. We used Python language [21]\nfor programming.\nIn order to get the data from the Yakindu SCT output\nfiles (.sct) and to transform it into higraph, we created a\nparser. While parcing a Yakindu statechart model, we extract\nprocess states (ids, names, types, parent-children relations),\ntransitions (ids, names, outgoing states, incoming states, labels)\nand relations between regions.\nWe developed a higraph constructor for Statechart-Higraph\ntransformation.\nTo create a higraph representation in Python, a NetworkX\nsoftware package [22] was used. In particular, we constructed a\ndirected graph with self loops and multiple edges between two\nnodes, which, in terms of NetworkX, is called a MultiDiGraph.\nWe started with constructing a list of vertices (VerticeList)\nby going deeper into states nesting levels in each region of the\nStatecharts model, specifying the current region, depth level\n(Depth) and direct parent state (ParentVertice) for inner states.\nAfter that, we constructed a list of transitions (TransitionList) via specifying outgoing/incoming states.\nStatecharts formalism allows adding entry pseudo states in\neach region, so that the start of the process (or a subprocess)\nscenario would be indicated. A MultiDiGraph, however, does\nnot use anything similar, as entry pseudo states are not explicitly connected to a super-state there are situated in. As a result,\nthe VerticeList consists only of actual states of the Statecharts\nmodel.\nTransitions, connecting entry pseudo states with some\nother states in the Statecharts model, are now replaced with\ntransitions, connecting their direct ParentVertice with the same\nstate.\nTransformation algorithm is shown below:\n1. create MultiDiGraph()\n2. for each Vertice in VerticeList:\n3. if VerticeDepth == 0:\n4. add Vertice to BlobList\n5. else:\n6. if VerticeName != ’ ’:\n7. find ParentVertice\n8. VerticeName = ParentVerticeName + VerticeName\n9. add Vertice to BlobList and InternalBlobList\n10. add Nodes from BlobList to MultiDiGraph()\n11. add attributes Depth and ParentBlob to Nodes in\nMultiDiGraph()\n12. for each Transition in TransitionList:\n13. if SourceVertice in BlobList and TargetVertice in\nBlobList:\n14. add Transition(SourceVertice, TargetVertice) to EdgeList\n15. else:\n16. if SourceVerticeDepth != 0:\n17. find ParentVertice\n18. if ParentVertice in BlobList:\n19. add Transition(ParentVertice, TargetVertice) to EdgeList\n20. add Edges from EdgeList to MultiDiGraph()\nThe graph internal data structures in NetworkX are based\non an adjacency list representation and implemented using\nPython dictionary datastructures. The graph adjaceny structure\nis implemented as a Python dictionary of dictionaries; the outer\ndictionary is keyed by nodes to values that are themselves\ndictionaries keyed by neighboring node to the edge attributes\nassociated with that edge. This dict-of-dicts structure allows\nfast addition, deletion, and lookup of nodes and neighbors in\nlarge graphs. The underlying datastructure is accessed directly\nby methods (the programming interface API) in the class\ndefinitions. All functions, on the other hand, manipulate graphlike objects solely via those API methods and not by acting\ndirectly on the datastructure. This design allows for possible\nreplacement of the dicts-of-dicts-based datastructure with an\nalternative datastructure that implements the same methods.\nWe illustrate our transformation results on several abstract\nexamples in order to verify that datastructure and relation\nbetween the elements are modified in a correct manner.\nThe first example (Fig. 2) shows a simple combination of 8\nstates with hierarchy (Entry, C and D states lie in B superstate).\nAs proposed within the algorithm, we get rid of an Entry\npseudo-state and rename C and D states into B C and B D, as\nB state is their direct parent. In the end, higraph | nodes | and\nedges are:\nEx1: Hierarchy higraph model example\nNodes: [(’A’, {’depth’: 0}), (’B’, {’depth’:\n0}), (’E’, {’depth’: 0}), (’B_|_C’,\n{’depth’: 1, ’parent’: ’B’}), (’Stop’,\n{’depth’: 0}), (’B_|_D’, {’depth’: 1,\n’parent’: ’B’}), (’Start’, {’depth’: 0})]\nEdges: [(’A’, ’B’), (’B’, ’B_|_C’), (’B’,\n’E’), (’E’, ’B’), (’E’, ’Stop’), (’B_|_C’,\nFig. 2. Hierarchy statechart model example (Ex1)\nFig. 3. Concurrency statechart model example (Ex2)\n’E’), (’B_|_C’, ’B_|_D’), (’B_|_D’, ’E’),\n(’Start’, ’A’)]\nThe second example (Fig. 3) focuses on concurrency\nproperty of a Statecharts model: super-state B contains regions\nR1 and R2, that can be reached at the same time, starting their\nsub-processes in parallel.\nOnce again, we cut Entry pseudo-states (in R1 and R2) and\nconstruct higraph nodes and edges, according to the algorithm:\nEx2: Concurrency higraph model example\nNodes: [(’A’, {’depth’: 0}), (’B’, {’depth’:\n0}), (’E’, {’depth’: 0}), (’B_|_C’,\n{’depth’: 1, ’parent’: ’B’}), (’Stop’,\n{’depth’: 0}), (’B_|_F’, {’depth’: 1,\n’parent’: ’B’}), (’B_|_D’, {’depth’: 1,\n’parent’: ’B’}), (’Start’, {’depth’: 0})]\nEdges: [(’A’, ’B’), (’B’, ’B_|_C’), (’B’,\n’E’), (’B’, ’B_|_F’), (’E’, ’B’), (’E’,\n’Stop’), (’B_|_C’, ’E’), (’B_|_C’,\n’B_|_D’), (’B_|_F’, ’E’), (’B_|_D’, ’E’),\n(’Start’, ’A’)]\nFig. 4. Deep hierarchy statechart model example (Ex3)\nThe third example (Fig. 4) is a more complicated version\nof an hierarchy example, as there are 3 nesting levels.\nNote that there are no triple named blobs among those who\nare on the deepest level, as only direct parent blobs are taken\nin account while renaming.\nEx3: Deep hierarchy higraph model example\nNodes: [(’A’, {’depth’: 0}), (’B_|_C’,\n{’depth’: 2, ’parent’: ’B’}), (’A_|_Stop’,\n{’depth’: 1, ’parent’: ’A’}), (’Start’,\n{’depth’: 0}), (’A_|_B’, {’depth’: 1,\n’parent’: ’A’}), (’A_|_E’, {’depth’: 1,\n’parent’: ’A’}), (’B_|_D’, {’depth’: 2,\n’parent’: ’B’})]\nEdges: [(’A’, ’A_|_B’), (’B_|_C’, ’A_|_E’),\n(’B_|_C’, ’B_|_D’), (’Start’, ’A’),\n(’A_|_B’, ’B_|_C’), (’A_|_E’, ’A_|_Stop’),\n(’B_|_D’, ’B_|_C’)]\nThus, the transformation algorithm captures the main\nproperties of statecharts and reflects them in a form of the\nhigraph representation, therefore enabling the usage of graph\nalgorithms for model analysis.\nV. EXPLORING HIGRAPHS FOR PROCESS ANALYSIS\nNow that we have transformed statecharts into higraphs,\nwe will show how the higraph formalism can be applied to\nthe CMP analysis and decision-making support.\nA. Defining the best process scenario\nIn order to identify the best process scenario and figure\nout how the run-time analysis of higraphs can help the case\nmanager to choose/propose an activity that would lead to a\nrealisation of this scenario, first, we need to understand, how\nto apply graph algorithms to higraphs.\nDespite the applications discussed in [11][18], very little\nwork has been carried out on the algorithmic properties of\nhigraphs.\nAs, by definition, higraphs are an extension of ordinary\ngraphs by AND/OR decomposition of vertices, several blobs\ncan be reached at the same time. Such sets of blobs are called\nconfigurations.\nHaving defined the configurations, we can talk about\nsemantics of higraphs in terms of an induced graph, where\nvertices are higraph configurations and edges between these\nvertices exist only if there is an ’appropriate’ higraph edge.\nThis approach to semantics allows us to work with higraphs\nby means of ordinary graphs.\nThus, the shortest path between two blobs in a higraph is\ndefined as the shortest path in the induced graph between any\nconfiguration containing the source blob and any configuration\ncontaining the target blob [18].\nFor example, consider Ex1 (Fig. 2) higraph. Here is the\nlist of every possible path from Start to Stop blobs:\n1. Start -> A -> B -> E -> Stop\n2. Start -> A -> B -> B_|_C -> E -> Stop\n3. Start -> A -> B -> B_|_C -> B_|_D -> E ->\nStop\n4. Start -> A -> B -> E -> B -> ... -> E ->\nStop\n5. Start -> A -> B -> B_|_C -> E -> B -> ...\n-> E -> Stop\n6. Start -> A -> B -> B_|_C -> B_|_D -> E ->\nB -> ... -> E -> Stop\nSince there is a cycle in this higraph, for the path list only\nits first iteration is considered.\nThe shortest path in this case is the first one, because\nit has the minimal number of transitions (if not specified,\nthe default weight of any transition equals 1, except for the\nparent-child blob transition, which equals 0). But since the\nhigraph represents a non-workflow process, the shortest path\ncalculation might not be that simple.\nStatecharts models consist of states and transitions between\nthem, representing a combination of events. While the process\nis running, these events might or might not happen, therefore,\nmaking its corresponding transitions (un)available. As a result,\nif, by chance, the transition B -> E is not available at some\npoint, path No.1 from the list is not the shortest path to reach\nthe target Stop blob.\nFor the case manager, the best process scenario corresponds\nto a sequence of milestones that contribute best to the main\nprocess goal. The manager can select one of such scenarios (if\nseveral are available) and decide upon the activities to execute\nduring this scenario. Alternatively, these activities can be also\nrecommended by the algorithm.\nB. From ”best scenario” to ”best course of actions”\nConsidering that the best scenario is defined as a shortest\npath on the higraph, now we need to go back to the application\ndomain and to answer the question: what are the activities that\nshould be executed in order to lead to realisation of this best\nscenario?\nThe link between the activities that has to be scheduled\nby the case manager and the state-oriented model (statecharts)\ncan be expressed as follows:\n1) From the manager’s perspective, a process execution\nscenario consists of activities;\n2) An activity can be characterised by its results (or\nevents) produced upon the execution (often referred\nto as postconditions).\n3) Once an event occurs, it can trigger a state transition\nin our state-oriented model.\nIn other terms, the state-oriented model describes what\nshould be produced (event or postcondition) in order to trigger\na transition required by the best scenario. An activity from\nthe application domain that matches this description can be\nconsidered as the best activity and recommended for the case\nmanager2. Alternatively, the case manager can define her own\nactivity that also matches this description.\nThe listing below illustrates how a list of available activities\ncan be specified in the application domain. We define an\nactivity through their names, preconditions (i.e., a set of of\nstates where an activity can be executed) and postconditions\n(a combination of triggered events):\nactivities_names = {activity_1_pre :\n’Activity_1’, activity_2_pre :\n’Activity_2’}\nactivities_list = {activity_1_pre :\nactivity_1_post, activity_2_pre :\nactivity_2_post}\nactivity_1_pre = (’A’, ’A_|_B’)\nactivity_1_post = (’event_1’)\nactivity_2_pre = (’A_|_B’, ’B_|_D’)\nactivity_2_post = (’event_2’)\nactivities_pre = [activity_1_pre,\nactivity_2_pre]\nHere Activity 1 is enabled in A, A B states and triggers\nevent 1 when executed, while Activity | 2 is available in A B,\nB D states and triggers event 2. |\n|\nFor a given state (or configuration of states) a list of enabled\nactivities can be automatically defined: in our example, when\nthe process scenario reaches the state A, Activity 1 becomes\nenabled and can be executed. Upon its execution, event 1 will\nbe produced.\nThe fact that Activity 1 is enabled for execution at some\nstate(s) does not mean that this activity will/should be executed. Only if its postcondition (event 1 in our example)\nmatches the best scenario identified by the analysis of the\nhigraph, this activity will be suggested for execution to the\ncase manager as (one of) the best activity.\nIf the case manager decides to use her own activity instead\nof the recommended one - she can add this activity to the list\nat run-time and reuse it later.\nVI. CONCLUSION AND PERSPECTIVES\nIn our previous work, we demonstrated how a case management process can be modeled and simulated with statecharts a state-oriented formalism invented by David Harel and used\nfor modeling real-time event-driven systems. Compared to\n2Here we consider that some list of activities associated with the process\nis defined in the application domain.\nBPM approaches, a statechart process model does not specify\nconcrete activities but only the objectives and constraints to\nbe met. Thus, our approach does not prescribe but describe\nan activity to be executed next. The manager can define an\nactivity that fit the description ”on the fly”, based on her\nexperience and intuition, or can select such activity from a list\n(e.g., recommended activities or activities previously executed\nin a similar situations) in order to ensure a desirable process\nscenario.\nWhen the process complexity grows, definition of desirable\nscenarios, comparison of alternatives and selection of the best\nscenario at run time becomes complex. Automated guidance\nfor the case manager is a big advantage.\nIn this paper, we propose an approach for analysis of nonworkflow processes (including case management processes)\nbased on higraph semantics. On the example of classical\nshortest path algorithm, we show how the graph algorithms\ncan be used to provide automated guidance about best scenario\ndefinition.\nHigraph analysis algorithms are not limited by the shortest path computations and include much more: connectivity\nchecking, clustering, cycle finding, communicability and other\nalgorithms. These algorithms will be explored in our future\nwork.\nAmong the main challenges of the presented approach we\nconsider the relation between Application, Model and Tool\ndomains (Fig. 1): Translation of a real life process to a statechart model, identification of relevant tools and algorithms for\nmodel analysis, interpretation of the obtained results in order\nto provide meaningful recommendations, expressed in terms of\napplication domain are some of the important questions that\nwe are going to consider next.\nOur proposed approach overcomes the shortcomings of the\nactivity-centric models: the activities can be defined at runtime, the process can be (re)configured at each state depending\non the situation (compared to a predefined number of variation\npoints in conventional process models), the process can evolve\nas a result of an external event (making certain scenarios\nirrelevant). The proposed algorithms help the case manager\nto make better decisions while not restraining her creativity.",
  "2017-RLGS": "· · 3.1 Introduction A Process-Aware Information System (PAIS) is a software system that manages and executes operational processes involving people, applications, and/or information sources on the basis of process models [16]. Workflow management systems and BPM systems are classic examples of PAIS. B · · I. Rychkova ( ) B. Le Grand C. Souveyet Université Paris 1 Panthéon-Sorbonne, 12, Place du Panthéon, 75005 Paris, France e-mail: irina.rychkova@univ-paris1.fr URL: http://www.univ-paris1.fr/centres-de-recherche/cri/ B. Le Grand e-mail: benedicte.le-grand@univ-paris1.fr C. Souveyet e-mail: carine.souveyet@univ-paris1.fr © Springer International Publishing AG 2017 49 Advances in Intelligent Process-Aware G. Grambow et al. (eds.), Information Systems, Intelligent Systems Reference Library 123, DOI 10.1007/978-3-319-52181-7_3 50 I. Rychkova et al. Started by F. Taylor and H. Ford, a pursuit of process optimization and automation resulted in the creation of workflow concepts, where a process is specified with a (predefined) flow of tasks [55]. Workflows provide a powerful formalism for the structured, design, simulation, analysis as well as management and execution of activity-oriented processes. Today, practitioners express the increasing need for information systems sup- unstructured, data-oriented processes case management processes porting such as (CMP). The Object Management Group (OMG) defines case management as “a coordinative and goal-oriented discipline, to handle cases from opening to closure, interactively between persons involved with the subject of the case and a case man- ager or case team” [34]. Davenport [12] defines a case management process as a process that is not predefined or repeatable, but instead, depends on its evolving circumstances and on decisions regarding a particular situation, i.e., a case. Claim processing, residence permit issuing, crisis management, and organization of events are examples of CMP. PAIS supporting case management are gaining momentum nowadays. Among Manager,1 Papyrus,2 Computas3 ISIS successful solutions the IBM Advanced Case Center4 or IBM Intelligent Operations can be cited. Many solutions supporting case management are now being developed and reported by the community of practitioners promoting Adaptive Case Management (ACM) [49]. Explicit process specifications play an important role in PAIS: they allow for better communication between stakeholders, enable process analysis and support redesign efforts [2]. Methodologies, specification languages and environments for workflow modeling and analysis are widely presented in the literature and recognised by practitioners. In contrast, current CMP supporting solutions are mostly focused on process configuration and execution. Very little support for CMP modeling and analysis is provided. In this chapter, we define a state-oriented formalism for the incremental and inter- active modeling and simulation of CMP. Our formalism is grounded on statecharts developed by D. Harel in 1987 [19]. In particular, we explain (a) why statecharts is a suitable formalism for CMP, (b) how statecharts can be adOpted and adApted for specifying CMP; we also show (c) how executable statecharts specifications can be used for CMP simulation and (d) how they can enable predictive analysis and recommendation support for a case manager. Statecharts were originally created as a visual, fully executable formalism for the specification, design and analysis of complex discrete-event systems. Case man- agement processes share a number of characteristics with complex discrete-event systems [19, 20, 23]: they continuously interact with their environment, respond to 1http://www-03.ibm.com/software/products/en/category/advanced-case-management. 2http://www.isis-papyrus.com/. 3http://www.computas.com/. 4http://www-03.ibm.com/software/products/en/intelligent-operations-center. 3 Towards Executable Specifications for Case Management Processes 51 unexpected events (interrupts) and have many possible operation scenarios. In par- ticular, a CMP can be compared to a reactive system, for which the main challenge is to identify the appropriate activity or group of activities to perform in reaction to a given internal or external stimulus in a given situation (context). However, contrary to goal conventional reactive systems, CMP has a that can be reached by several alter- native scenarios. Moreover, decisions about these scenarios in CMP are typically made by a human actor (the case manager). Therefore, a CMP supporting system can seldom automatically execute the activities but it can enable or recommend them for execution. The statecharts formalism combines an intuitive and concise visual notation with Rhapsody5) and the precise semantics [21, 31]. Rhapsody [20] (now IBM Rational (SCT)6 are examples of statecharts model- open source YAKINDU Statechart Tools ing environments, where visual statecharts specifications can be created and executed. we adopt the main concepts of statecharts, Following the points stated above, such as states and state hierarchies, transitions, triggering events, concurrency and broadcast communication for CMP specification. In order to address CMP specific we extend the statecharts formalism features, with the notions of goal and path; we also revisit the semantics behind triggering events and introduce the concept of event duration. The advantages of statecharts specifications can be perceived both during the design of CMP and during their execution. As we will explain in this chapter: • Statecharts specifications allow for incremental CMP design; • Executable statecharts specification can be used for the simulation-based testing of CMP scenarios; • Executable statechart specifications pave the road for automated recommendations for CMP. We apply the proposed formalism to specify an example of CMP: a crisis (flood) management process defined for Hauts-de-Seine department of France. The remainder of this chapter is organized as follows. In Sect. 3.2, we intro- duce our example and provide the terminology that will be used in this chapter. This terminology spans across two domains: complex systems and case manage- ment. In Sect. 3.3, we present and discuss the related work in CMP management and modeling. In Sect. 3.4, we introduce the statecharts formalism and draw the parallels between complex discrete-event systems and CMP. In Sect. 3.5, we demonstrate how the statecharts formalism can be adopted and extended in order to provide fully exe- cutable specifications of CMP. In Sect. 3.6, we discuss the prospective added value of executable CMP specifications, trace a roadmap for future research and draw our conclusions. 5http://www-03.ibm.com/software/products/en/ratirhapfami. 6http://statecharts.org/index.html. 52 I. Rychkova et al. Fig. 3.1 The scope the flood management process 3.2 Case Management Process Example and Terminology In this section we provide an example of CMP—a crisis management process designed to handle floods (we will call it flood management process) in a French department Hauts-de-Seine. We also briefly introduce the terminology used in this chapter and illustrate it on our example. Figure 3.1 shows the scope of the flood management process. 3.2.1 Crisis Management in Cases of Flood flood A is an overflow of water that submerges a land. It happens, for example, because of an increase in the flow of a river provoked by significant rainfalls. The risk of a “major flood” is the main natural risk in the Ile-de-France region, particularly Paris7 are confronted to during the winter period from November to March. Cities like this risk: if a flood occurs, important damages can be expected, affecting thousands of people. Floods are considered harmful when the water level of the Seine river 7See http://cartorisque.prim.net/dpt/75/75_ip.html. 3 Towards Executable Specifications for Case Management Processes 53 exceeds 5.50 m according to the scale on the Austerlitz bridge in Paris. In the Hauts- de-Seine department, the risk of flood is considered as particularly important since 1910.8 The goal of the flood management process is to maintain the proper operation of city infrastructure (water supply, electricity, telecommunication, road networks, pub- lic transport and so on) and to protect people and facilities from flood consequences. This process is a typical example of CMP: • it demands interaction between multiple actors (government, public services, vol- unteers, etc.). • it is driven by the dynamic context of the case (i.e., flood development, current status of vulnerable areas and of rescuing operations) rather than by a predefined sequence of activities. Flood Emergency begins when the water level rises above 5.5 m at the Austerlitz Bridge and is supposed to keep rising (according to weather forecasts). At this stage, the centers for crisis management are set up and the Emergency Plan Specialized on Floods (EPSF) is triggered. The city services (rescue, fire fighters, police, etc.) therefore carry out specific activities accordingly. The regional authorities monitor the crisis situation and coordinate the operation evacuation of population and facilities, procedures in the following major areas: temporary accommodation, public transport, road traffic, water supply, electricity supply and telecommunications. According to the flood severity, the EPSF identifies different phases of flood emergency for each of these seven areas and specifies the procedures to control the situation and to protect the population and facilities. For example, when the water level exceeds 6.25 m, the drinking water supply is reduced for the towns of Saint-Cloud, Garches, Vaucresson, Marnes la Coquette and Ville d’Avray. When the water level reaches 6.7 m, the drinking water supply for these towns is completely disrupted. Therefore, the provisioning and distribution of bottled drinking water should start as soon as the water level at Austerlitz Bridge reaches 6.25 m. In case of limited supply, prioritized water provisioning has to be organized. Along those lines, depending on the water level, various procedures are launched: a partial or complete interruption of public transport (SNCF Paris Rive Gauche, RER C, RATP), deviation and blocking of main highways (A86, A14, N14, etc.), evacuation of people, health care and childcare facilities. Resources available for crisis management also need to be constantly monitored. In case of deficiencies in equipment, manpower or other problems that can compro- mise the crisis handling in one or several areas, specific measures such as mobiliza- tion of volunteers or federal alert raising can be taken. We model the resources as a specific area of the EPSF. 8Source: Préfecture des Hauts-de-Seine: Plan de secours spécialisé sur les inondations Hauts-de- Seine, SIDPC 21/11/2005, (2005), Available at: http://www.ville-neuillysurseine.fr/files/neuilly/ mairie/services_techniques/plan-secours-inondation.pdf. 54 I. Rychkova et al. 3.2.2 Terminology Used in This Chapter case A is a situation (e.g., a flood crisis), which requires resolution. It is described by a set of elements that are relevant to or involved in a CMP. Within the case, we define the system boundary and distinguish between so-called system elements and context elements (that belong to the environment): Case = System under description + Environment System Under Description (SUD) The is described by the set of elements that can be controlled during the case management: public services, equipment, infrastructure, administration etc. It also includes a Case Management Supporting System (CMSS) and a case manager. The SUD reacts to various stimuli (events) produced by the environment (e.g., change in temperature, water level, incidents) and performs activities in order to maintain the functioning of city infrastructure and to protect people and facilities from flood consequences. The SUD produces internal events such as messages, reports and alerts sent by the agents via radio or mobile network. They can indicate the success or failure of a mission, resource deficiencies, emergency situations and so on. environment The is described by the set of elements that interact with the SUD. It cannot be controlled but only monitored using specific equipment (e.g., meteo stations for monitoring weather, embedded sensors for measuring water level, video cameras for measuring traffic, social networks for collecting information about areas affected by the flood). The environment’s behavior is unpredictable and brings uncer- tainty in the CMP. The environment produces external events such as accidents, traffic jams, electric outages, malfunctioning of telecommunication. Case Management Process (CMP) The describes the behavior of the SUD and defines what it has to do in order to achieve some objectives, i.e., to ensure safety and security for people and goods during the flood, until the emergency is over. case management The element in Fig. 3.1 depicts a subsystem of the SUD which is responsible for the coordination of SUDs activities. It includes the case manager and the case management supporting system (CMSS): Case Management Supporting System (CMSS) The is a PAIS for case manage- case manager ment. The is a human actor who uses the CMSS in order to monitor the case, to take decisions regarding the case handling scenario and to coordinate the activities of the SUD. 3.3 Related Work In this section, we discuss Adaptive Case Management—for now, the most prominent paradigm for CMP support. We also review the existing modeling paradigms and formalisms for process specification and their capacity to model CMP. 3 Towards Executable Specifications for Case Management Processes 55 3.3.1 Adaptive Case Management The concept of Adaptive Case Management (ACM) has been defined as an “infor- mation technology that exposes structured and unstructured business information (business data and content) and allows structured (business) and unstructured (social) organizations to execute work (routine and emergent processes) in a secure but trans- manner”.9 parent One of the major challenges identified by the ACM community, is the attempt to deal with CMP in the industry the same way as with regular business process—i.e., representing a case management by a workflow and focusing on the (predefined) sequence of tasks. This view implies that the data emerges and evolves within a process according to a predefined control flow similarly to a product evolving on a conveyor belt. a collection of data According to ACM [52], CMP must be organized around artifacts about the case; the tasks and their ordering shall be adapted at run time, according to the evolution of the case circumstances and case-related data [41]. The body of knowledge on ACM has been extensively developed by practition- ers; the best solutions are regularly reported in the book series on WfMC Global Awards for Excellence in Case Management [53, 54]. However, methodologies and formalisms for CMP modeling are rarely discussed. 3.3.2 Modeling Paradigms for CMP Specification The important role of modeling in PAIS is discussed in [2]. The following general process modeling paradigms are identified in the literature [10, 11, 15]: activity- oriented, product (or state)-oriented and decision (or goal)-oriented. The choice of a modeling paradigm depends on the conceptual properties of the process (e.g., flexibility vs. control). According to the literature, case management processes (CMP) have the following conceptual properties: 1. CMP are unstructured, with non-repeatable execution scenarios [9, 52]; 2. CMP are data-centered and are organized around a collection of data artifacts about the case [6, 52]; 3. CMP are reactive and event-driven: activities should be carried out in reaction to a given internal or external event; 4. CMP must be considered within their context and the boundary between the system and its environment and the scope of the process must be clearly specified [6, 27, 48]; 5. CMP are goal-oriented and flexible: goals are set and can be modified, added or removed during the execution [48]; 9http://www.xpdl.org/nugen/p/adaptive-case-management/public.htm. 56 I. Rychkova et al. 6. CMP are knowledge-intensive: decisions about the process scenario are made by a human actor—a knowledge worker—and are based on her knowledge, experience and intuition [25, 41]; 7. CMP are unpredictable—they have to deal with events and handle the situations that were not planned or even imagined before [52]. In this section, we discuss the capacity of activity, product (state) and goal-oriented paradigms to express these conceptual properties of CMP. activity-oriented paradigm, Within the the process is specified as an ordered set of activities that the system has to carry out. Examples of activity-oriented formalisms include BPMN [35], YAWL [3], activity diagrams in UML [46] and other languages based on workflow concepts. Activity-oriented process modeling implies that data emerges and evolves within a process according to a predefined control flow. Events are supposed to occur (or be processed) at specific moments of the execution predefined by the model. This paradigm suits predictable and highly repeatable processes. CMP are unpredictable processes [52]: events and process inputs can occur at any time during execution; the order of activities cannot be predefined and depends on the current situation. Such behavior can therefore not be captured by the workflow formalism. In order to increase process flexibility and to better address unstructured and knowledge-intensive processes like CMP, activity-oriented formalisms are extended with declarative parts, such as constraints [5], business rules [7] or configurable elements [45]. These formalisms can handle process variability within a potentially large number of configurations or scenarios. However, either such scenarios must be well identified upfront or the set of business rules (or configuration elements) must be regularly maintained by an expert. This can be seen as a limitation for CMP. Techniques and frameworks for the analysis of activity-oriented process models are widely presented in the literature [57]. To provide automated process analysis, activity-oriented modeling languages are often annotated with or translated to some formal specification languages. The Declare framework [37] is a constraint-based system that uses a declarative language grounded on temporal logics. In [1], the state- oriented formalism of Petri Nets is used for workflow specification and analysis. In [14], the Petri Nets semantics for BPMN is presented. In [28], a business process model is mapped into a nondeterministic state machine for further analysis. product-oriented (or state-oriented) paradigm, According to the a process is seen as a product life cycle (a set of product states and transitions between these states). Examples of product-oriented modeling formalisms include statemachines in UML [20], generic state-transition systems or state machines, such as FSM [38] or Petri Nets [32], and statecharts by D. Harel [19] created for the specification and analysis of complex discrete-event systems. Within this paradigm, carried out activities depend on the current state of the product and the process scenario is adapted at run time, according to the evolution of the product. This paradigm suits well reactive systems specification [23] since the system’s response to an event shall be defined not only by the type of this event but also by the current situation of the system i.e., its state. 3 Towards Executable Specifications for Case Management Processes 57 Several research groups are reporting on approaches to design and specification of unstructured, knowledge-intensive processes (including CMP) based on the product- oriented paradigm. In [9], process instances are represented as moving through a state space, and the process model is represented as a set of formal rules describing valid trajectories. Compared to our proposal based on statecharts, this approach is grounded on the theory of automated control systems. In [24], a group of researchers from IBM incorporates process- and data-centered perspectives; their approach is based on the concept of business artifacts. The Case Management Model and Notation is presented in [36]. This specification “is intended to capture the common elements that Case management products use, while also taking into account current research contributions on Case management.” In [42], the Product-Based Workflow Design is presented. This approach explores the interaction between a product data model that reflects the product design and the process to manufacture this product represented by a workflow. The authors of [6] present case handling as a paradigm for supporting knowledge-intensive business processes. They recognise the lack of flexibility of workflow management systems and acknowledge the important role played by the “product”—the case—in the case handling. Their view on the case, however, remains activity-oriented: the proposed case definition explicitly includes the list of activities and their precedence relation assuming that they are known in advance. Formalisms based on state machines are suitable for automated analysis including simulation, formal validation and model checking. Algorithms from graph theory can be used in order to analyse states reachability, “dead” states, path search and optimisation (where the path represents a process execution scenario). In [24], the operational semantics of Guard-Stage-Milestone is presented. This semantics explains the interactions between business artifacts which are formalized following declarative principles. In our earlier work [47], we define formal semantics for CMP using the Alloy specification language. The Alloy Analyzer tool allows us to simulate and validate a CMP model; it also provides visual diagrams. Compared to statecharts, however, Alloy model is difficult to construct. The product-oriented paradigm seems to be a good choice for specifying CMP. However, it does not support decision making since it does not define a notion of objective or goal. decision or goal-oriented paradigm The extends the product-oriented view on the process: the successive transformations of the product are looked upon as con- sequences of decisions leading to some goal [33]. Goal-oriented modeling formalisms support decision making by specifying goal hierarchies and tracing each decision within these hierarchies. The examples include i * [58], KAOS [30], MAP [43]. Goal-oriented formalisms extended with the notion of context are presented in [40, 44, 51]. These formalisms link a decision (expressed as a goal) to the situation in which this decision is taken (product state): for each state, a set of achievable and non-achievable goals can be identified and vice versa, each goal can be expressed in terms of states that the product has to reach. These formalisms can also connect the goals and the activities that must/can be carried out in order to achieve these goals. 58 I. Rychkova et al. The Generic Process Model (GPM) [51] is an example of context-driven goal- oriented formalism. It captures the process context and allows for reasoning about process goals. It is also suitable for automated process analysis. Context-driven goal-oriented process models support automated recommenda- tions and user guidance, providing that for each goal all the situations (states) in which this goal is achievable are known. Due to unpredictable sequences of events and non-repeatable execution scenarios in CMP, however, it will be hard if at all possible to model relations between various process situations, goals and activities that must/can be executed in order to achieve these goals. Such relations can be, though, discovered using process mining techniques (this is an interesting subject that lies behind the scope of this work). Our analysis of existing modeling paradigms and their corresponding formalisms shows that the activity-oriented paradigm can hardly provide the flexibility required by CMP as expressed by their conceptual properties 1–3 and 5–7 listed above. Con- figurable specifications and business rules can be used to overcome the rigidity of traditional workflow-based formalisms, addressing properties 5 and 1–3 respectively. Nevertheless, they support the variability of process scenarios only within some boundaries defined by a number of business rules or configurable elements. Thus, they fail to address properties 6 and 7 of CMP. The goal-oriented paradigm offers flexibility and supports knowledge workers. However, goal-modeling formalisms are typically suitable for an early phase of system modeling (abstract system design); formal analysis, simulation and testing are not their priorities. Addressing properties 1 and 7 of CMP would lead to an extremely complex model. The product-oriented (or state-oriented) paradigm addresses all conceptual prop- erties of CMP except the 5th one—goal orientation—as this paradigm does not define the notion of goal. On the other hand, compared to goal-oriented formalisms, state- oriented modeling formalisms typically focus on concrete system design followed by validation and testing. They are supported by a plethora of techniques and tools for model simulation and formal analysis. Therefore, for modeling CMP, we adhere to the product-oriented paradigm. 3.4 Finite State Machines, Hierarchical State Machines and Statecharts As explained above, we have chosen the product-oriented paradigm for modeling CMP. According to this paradigm, a state transition system (or state machine) rep- resents our knowledge about the case and its evolution. The choice of a concrete modeling formalism within the selected paradigm is related to the purpose of modeling (e.g., communication support, high-level design, simulation, formal validation and verification [18], diagnostics and improvement, recommendation and optimisation of process behaviour [8, 13]). 3 Towards Executable Specifications for Case Management Processes 59 In this section we discuss a selection of existing formalisms based on state machines and focus on statecharts for CMP specification. 3.4.1 CMP Versus Complex Discrete-Event Systems A CMP shares the following characteristics of complex reactive systems behavior defined in [19, 22]: 1. It continuously interacts with its environment. Its inputs and outputs are often asynchronous: they may occur or evolve unpredictably, at any time; 2. It must be able to respond to high-priority events (interrupts); 3. It has to operate and to react to inputs with respect to strict time regulations; 4. It has many possible operation scenarios, depending on its current mode of oper- ation, current values of data as well as its past behavior; 5. It is very often based on interacting processes that operate in parallel. As in a reactive system, the main challenge for the case manager is to identify the appropriate activities to perform in reaction to a given internal or external stimulus in a given situation (context). State machines are a popular choice for specifying the behavior of reactive soft- ware systems. We will therefore consider them further. 3.4.2 Finite State Machines A finite state machine (FSM) [38] specifies a machine that can be at one state at a time and can perform a state transition as a result of a triggering event (or a group of events guarded by a condition). It is defined by a (finite) set of states and a set of triggering events for each transition. To trigger a state transition, the execution of some activities and/or the observation of some contextual events can be required. Traditional FSMs and their corresponding state-transition diagrams are very effi- cient for tackling small problems. However, the complexity of a FSM model tends to grow much faster than the complexity of the problem it represents. This makes the simulation or automated reasoning about the model extremely difficult. This the state explosion problem phenomenon is called [56]. 3.4.3 Hierarchical State Machines and Statecharts The state explosion problem can be overcome by the introduction of multiple hier- archical levels for states and transitions. Indeed, this hierarchy gives a possibility to reuse some common behaviors across many states and, thus, to reduce the model 60 I. Rychkova et al. complexity. This idea is explored in the formalism of statecharts, invented by David Harel in the 1980s [19]. The statecharts formalism specifies a hierarchical state machine (HSM); it extends classical FSM by providing: (i) depth—the possibility to model states at multiple hierarchical levels, with the notion of abstraction/refinement between levels; (ii) orthogonality—the possibility to model concurrent or independent subma- chines within one state machine; (iii) broadcast communication—the possibility to synchronize multiple concurrent submachines via events. Each internal (produced by the system) of external (produced by the environment) event is instantaneously broadcasted. statecharts = FSM + Abstraction + Orthogonality + Broadcast- communication Some state-oriented approaches (e.g., Petri Nets) associate a transition with the execution of one concrete activity (or a group of activities). On the contrary, with statecharts we associate a state transition with the occurrence of a triggering event deferred activity binding. (or combinations of events) allowing for a Thanks to the deferred binding, at design-time, the process scenario can be seen as a sequence of events; the concrete activities that will produce these events can be selected or invented in run-time. The process enactment can be seen as a dynamic selection of activities to produce some outcomes (events) that make the process progress towards its (desired) final state. Visual notation. In the statechart notation, states are depicted with rectangular boxes with rounded corners. Figure 3.2 illustrates a high level diagram for our flood man- agement process example. The substate–superstate relation is depicted by boxes Activation of Crisis Centers EPSF encapsulation. and are exclusive substates of the Flood Emergency Flood Emergency state: when in the state, the case can be either in Flood Emergency one or in the other of these substates. While entering the state for Activation of Crisis Centers the first time, the substate is entered “by default”—this is depicted by the arrow with a black circle pointing at this substate. EPSF Figure 3.3 shows a detailed diagram of the state from Fig. 3.2. The areas EPSF separated by the dashed lines represent the concurrent substates of their super- EPSF state: when in the state, the case is simultaneously in eight concurrent substates. Each of them can be seen as a separate statechart with its own state hierarchy. Thus, Fig. 3.2 High-level view of the Flood management process 3 Towards Executable Specifications for Case Management Processes 61 detavitca si FSPE eht ecno ssecorp tnemeganam sisirc eht gniyficeps margaid trahcetatS 3.3 .giF 62 I. Rychkova et al. the introduction of concurrent substates is a convenient mechanism to specify logi- cally different areas of the case management (Public Transport management, Water Supply management, Road Traffic management etc.). active configuration The set of active states of all concurrent substates is called the of a statechart. It replaces the term of “current state” in conventional (flat) FSM. The transition that terminates with a circle with “H” stands for “entering the state PT1 PT2 by history”. The transition from to in Fig. 3.3, for example, specifies that PT2: Public Transport is Not insured once the case recovers from the state and PT1:Emergent Functioning re-enters the state—the last active configuration of the latter is selected (and not the default one). The transitions between states in statecharts are depicted by arrows labeled with expressions that specify the triggering events and (optionally) the actions that are carried out while the transition is triggered. In our example, the triggering events mostly represent external and internal events. More details on the statecharts notation can be found in [23]. The semantics of statecharts for CMP will be presented in more details in Sect. 3.5. Execution of statecharts specifications. The operational semantics of statecharts was originally implemented in the STATEMATE system and described in [21, 31]. The statecharts formalism was also adopted by the UML community in the form of UML statemachine diagrams [46]. Rhapsody [20] (now IBM Rational Rhapsody) and open source YAKINDU Stat- echart Tools (SCT) are examples of statecharts modeling environments, where the statecharts specifications can be created and executed in an intuitive and interactive way. 3.5 Statecharts Semantics for Case Management Processes As explained above, we adopt the formalism of statecharts for the specification of case management processes (Sect. 3.5.1). We also propose some extensions of statecharts in Sect. 3.5.2. We create the statecharts specification for the flood management process based on the description provided by the Emergency Plan Specialized on Floods (EPSF) and on some practical knowledge about resource management during floods. The resulting diagrams are shown in Figs. 3.2 and 3.3. states—Flood We start with a high-level view of the process described by two Emergency Stabilization—and Flood and transitions between them (Fig. 3.2). The Emergency state is entered when the water level at Austerlitz Bridge raises above Activation of Crisis Centers EPSF. 5.5 m. It contains two substates: and The transition Stabilization “end of crisis” to state is triggered once specific conditions identified as are met. The diagram in Fig. 3.3 specifies the main areas of crisis management as concur- EPSF rent substates of the state. For the purpose of this work, we show only a few of 3 Towards Executable Specifications for Case Management Processes 63 Water supply, Public transport, Road traffic Resources. these substates in detail: and This model can be refined providing further details on the crisis management sce- narios and operation procedures. 3.5.1 Statecharts Semantics for CMP Specification Below, we explain how the following concepts defined by the statecharts formalism [23] can be adopted for the specification of CMP: • State, state hierarchy and state decomposition; • Abstraction and refinement; • AND, OR and basic states; • Entering a state by default and by history; • State configuration; • Internal, external and triggering events; • Activity; • Broadcast communication; • Inter-level transition. State, state hierarchy and state decomposition. A CMP state can be seen as a specific situation in the case management process that requires reaction. On the abstract level, states can be compared to business milestones. The defi- nition of the right set of states for the process is subjective: it reflects our current understanding of the process and evolves over time. In this work, the states of the h. flood crisis management process are characterized by the level of water These states represent the critical points for different management areas defined by EPSF (Sect. 3.2). being in a given state, While some work has to be done in order to maintain this state or in order to leave this state and enter another state. Note that statecharts do not specify how exactly this work will be performed or which activities will be executed and in which order. Another means for modeling activities is needed: statecharts, for example, can be complemented with activity charts [23]. In this paper, we do not discuss activity modeling in detail. RT0, RT1 RT2 In Fig. 3.3, three states and specify the main phases of the road traffic control after the emergency plan (EPSF) is triggered. • RT0: Normal functioning is the default state upon triggering the EPSF. The water level of 5.5 m does not disrupt the road infrastructure of the region and normal functioning is maintained. • RT1: Emergency traffic control—this state is attained at 6.1 m; here the flood is affecting the road traffic. Specific measures must be continuously taken in this state in order to maintain road safety. 64 I. Rychkova et al. • RT2: Heavy Traffic!—this state is reached when the road traffic degrades (due to accidents, traffic jams) to the point where the crisis management itself becomes compromised (e.g., the rescue teams cannot arrive to the endangered areas, etc.). s hierarchy A state consists of a (possibly empty) of substates, representing (possibly concurrent) state machines. These substates provide details about their parent state (or superstate). RT1.1 RT1.4) In Fig. 3.3, four different substates (from to are defined based on the flood severity: upon entering each of these states, the city executes some scenario: deploying equipment, marking deviations, blocking roads, informing drivers, etc. Each substate belongs to one superstate (its surrounding state) that is also its nearest ancestor in the state hierarchy. We call the relations between the superstate and its substates abstraction/refinement relations. Abstraction and refinement. State abstraction consists in clustering states into a superstate according to some similarity criteria. This mechanism allows one to describe the problem at multiple abstraction levels, hiding or introducing details when necessary. Refinement is the opposite of abstraction, it consists in decompos- ing a state into substates according to some discrimination criteria. refinement More formally, is a XOR decomposition of a state, where being in a superstate means being in exactly one of its (exclusive) component substates. One substate can be marked as default so that this state is visited each time its parent state is entered. Public Transport Emergent Functioning state (PT1) in Fig. 3.3 is specified with three exclusive substates corresponding to three different management scenarios that h. PT1.1 are activated based on the water level is the default scenario. From a visualization standpoint, clustering states allows for a very economical rep- resentation. It avoids duplicating transitions and the model logical structure appears clearly. The AND decomposition results in the specification of orthogonal (or concurrent) components of the parent state. The AND decomposition models the situation when being in the state means being in one of the combinations of its components. All possible combinations make an orthogonal product. AND, OR and basic states. The statecharts formalism defines three types of states: AND, OR and basic states. The AND-state is a state that contains two or more orthogonal substates; the OR-state is a state that contains one or more exclusive substates. A state is basic if it does not have any substates. Water Supply WS1:Emergency water supply Consider the in Fig. 3.3: is an AND- Damage Reaction. state that contains two concurrent substates and These substates model the damage due to the flood and the reaction to it, i.e., emergency water WS1 provisioning. Once is entered both of its concurrent substates are activated. Damage The state is an OR-state; it contains two exclusive substates that specify WS1.1, WS1.2, its details: where the water supply of some towns is reduced; where the water supply is totally disrupted. Reaction WS2: Water The state is an OR-state; it contains two exclusive substates: provisioning WS3: Suspended Water supply! and 3 Towards Executable Specifications for Case Management Processes 65 WS1 entered,WS1.1, WS2 WS2.1 Once is and its substate are entered by default. Emergent water provisioning (WS2) defines specific measures to provide areas with drinking water: normal provisioning (WS2.1) and prioritized provisioning E1). WS3: (WS2.2) in case of limited stock of drinking water (event The state Suspended Water Supply! has no substates—it is a basic state. It refers to the sit- uation when the emergent water provisioning can no longer be guaranteed. This, for instance, can result from severe road conditions or insufficient stock of bottled WS1.2 drinking water while no other supply is available (i.e., when the case is in E2orE3orE4orE1[inWS1.2]]. state). This is indicated by the transition label: Entering a state by default and by history. The default indicator is used to identify which substate will be visited when its parent state is entered. Alternatively, in many cases it can be useful to enter the superstate by history, i.e., to enter its most recently visited substates (or configuration of substates). The examples include the transition WS3 WS2 from back to in Fig. 3.3: once the problem is solved, the emergency water WS2.1). supply is restored at its latest visited substate (which is not necessarily State configuration. Compared to conventional (flat) FSM, in hierarchical state machines depicted by statecharts, multiple states can be activated at the same time. configuration Statecharts define the term (of a state or of a system): active configuration s s The of a state is the set of basic substates of that are activated at the current moment. Intuitively, active configuration replaces the con- ventional term of current state defined in FSM. = h For the state WS1 in Fig. 3.3, consider that 7 m and the water provisioning functions normally; this would correspond to the following active configuration of ( ) = . , . WS1: c f W S1 W S1 W S2 2 1 The sequence of active configurations resulting from the execution of the statechart trace specification represents a of the CMP. Internal, external and triggering events. Internal events are produced by the system (Fig. 3.1); they are the results of carried out activities. E1 insufficient stock of drinking water. In Fig. 3.3, event specifies an It is an internal event that can result from the water distribution activity or can be generated by some other activity like stock verification. External events are produced by the environment (context) of the case. The case context consists of various objects that influence the case and affect its handling (Fig. 3.1). Water level, weather forecast, current situation on the roads, incident sensed reports are examples of contextual parameters by a system during a flood. WS0–WS1 WS1.1–WS1.2 In Fig. 3.3, or state transitions are taken if a certain value h of (water level) is reached or exceeded. We consider that the change in the water level is an external (contextual) event. triggering event e[c] e occurs and c holds) t is The (interpreted as of a transition ∈ t e E an event that must occur in order for to take place. Here is the event (or a ∈ c C logical combination of events) that triggers the transition; is a condition that e needs to be true for the transition to be taken when occurs. WS2 WS3 In Fig. 3.3, the triggering event for the transition from to is described by E2 or E4 or E1[in(WS1.2)]. an expression: This transition is taken if no more tracks 66 I. Rychkova et al. for transporting bottled water are available or if the road to the concerned area is blocked or if the stock of drinking water on place is insufficient while some towns no longer have regular water supply. The operational information about the resources or in(WS1.2) traffic conditions corresponds to contextual or internal events. Condition WS1.2 specifies that substate is active. To specify some work to be done, statecharts use the concept of activity. Activity. state-dependent activities The statecharts formalism defines that are linked s throughout within s. directly to a state and can be carried out or In the first case, s an activity starts when entering the state and terminates when leaving it. In the s; s, second case, an activity starts when entering when exiting if the activity it is not terminated yet, it is stopped by the system. This is a valid interpretation for a case management process too: in our example, WS2.1 upon entering the state (in Fig. 3.3) an activity for water provisioning must be PT1, started and must continue within this state. Upon entering activities for closing throughout the concerned stations must be carried out this state. Relations between activities and states defined by statecharts can be characterized A s throughout within as mandatory: if activity is linked to state by a or a relation s it must be carried out at this state. Therefore, each state can be associated with a (possibly empty) set of mandatory activities. Broadcast communication. Broadcasting allows for communication and synchro- nization between concurrent sub-machines. According to statecharts, both internal broadcasted, and external events are meaning that one single event can trigger tran- sitions at multiple orthogonal substates. Broadcast communication allows for coordination between different management Road Traffic Resources EPSF areas in CMP. For example, and substates of can both react to an (external) event reporting on the hard traffic in a particular road section. Along those lines, blocked roads or insufficient water supply (internal events) may trigger evacuation of people and facilities from the concerned area. Inter-level transitions. The transitions that cross state boundaries are called inter- level transitions in statecharts. Their purpose is to model the interruptions or the situations when the process has to react, no matter its state or the activity it performs. End of crisis For example, if the event occurs (Fig. 3.3), no matter what the configu- EPSF ration of substates is and what activities are executed in all its areas, they will Stabilization) be terminated and the new state (presumably will be entered by the process. 3 Towards Executable Specifications for Case Management Processes 67 3.5.2 Adaptation and Extension of the Statecharts Formalism for CMP Specification Below, we discuss the specific features of CMP that cannot be captured by the original statecharts concepts and we therefore propose adaptations and extensions to the statecharts formalism. What kinds of extensions are needed? Why? Despite the similarities identified in Sect. 3.4, there exists a number of characteristics that makes a CMP significantly different from a conventional reactive system: 1. A CMP has a goal. In reaction to given stimuli in a given situation, the case manager searches for scenarios that could steer the case towards its goal. Thus, compared to a reactive system where the next state (or active configuration) is defined by its current state and a given situation, the next state in CMP is also defined by the process goal. 2. CMP is a knowledge-intensive process where decisions (e.g., scenario planning, task assignment) are typically made by the case manager. As a consequence: 3. CMSS can be compared to business-intelligence systems (BI) rather than auto- mated control systems: for the latter, once the preconditions are satisfied for an a, a automatically executed. a activity is For CMSS, once the preconditions of are a enabled for execution mandatory) satisfied, is and (unless explicitly stated as the case manager decides weather it will be carried out or not. 4. Whereas some events are relevant only immediately after they occur (e.g., button pressed), other events, once they occur, remain relevant or valid for some period of time or for the whole execution of a CMP (e.g., document received; permission granted). In order to faithfully represent the complexity of a CMP, we propose to extend the statecharts formalism with the following concepts: 1. Final configuration; 2. Path, path selection and path reinforcement; 3. Relevance and validity interval for events; 4. Mandatory versus optional activities. We briefly describe these concepts in the remainder of this section. Final configuration. goal Similarly to [51], we express the process in terms of “target state” (or configuration) that the state transition system has to reach or to maintain; strategies (possible scenarios) for achieving this goal can be seen as sequences of states to visit (or state transitions to fire) between the “current” and the “target” states. By analogy with the active configuration defined by statecharts, we define a final configuration for CMP: final configuration s s The of a state is the set of basic substates of that we want to enter and/or maintain upon the CMP termination. In our example, the goal of the process is to support the areas affected by the flood and to protect the population from the flood consequences. For the statechart 68 I. Rychkova et al. WS3, PT2, RT2, CC2, CC3 in Fig. 3.3, any configuration where the critical states are not active can be considered as a final configuration (i.e., it should be maintained until the crisis is over). Path, path selection and path reinforcement. path We define a in statecharts as any sequence of active configurations that terminates with the final configuration. In our example, if one of the concurrent submachines has entered a critical state, the path is the sequence of configurations that would lead this submachine back to CC2: Insufficient resources one of its non-critical states. For example, if the state of Resources the substate is active, there are two paths for this submachine to bring it −→ −→ −→ CC2 CC1 CC2 CC3 CC1. back to CC1: or In any given active configuration, a path towards the final configuration can be calculated. The optimal path can be selected using some criterion (e.g., the cheapest path, the shortest path, the path with the highest probability to be realized). p Consider the optimal path from the current active configuration to the final Enforcing path p configuration. means executing some activities in order to enable t and then to take some state transition that will lead to the next active configuration p. in CC2: Insufficient resources Resources Consider that the state of the submachine −→ CC2 CC1 is active and that the path is the optimal path. Enforcing this path CC2 CC1. means here enabling the transition from to This transition will be taken if E11 E12 E13 at least one of the events (added manpower), (added supplies) or (added equipment) occurs. We can enforce the path by mobilizing volunteers or relocating supplies, manpower or equipment, considering that these activities can generate E11, E12 or E13 as a result. Relevance and validity interval for events. t, To take some transition the execution of process activities and/or observation of contextual events can be required. Most process formalisms including statecharts define a triggering event as a single event, or a group of events that occur simultaneously and instantly trigger a state events are only available in the step transition. In particular, statecharts specify that directly succeeding their generation instantaneous [19]. We call such events events continuous and distinguish them from events that, once observed, remain valid and can be reacted upon asynchronously, during multiple steps. validity interval tv e We define the for event as a period of time between the moment when this event is first observed and the moment when it becomes irrelevant for the process. e, To define the validity interval for an event we associate it with the time the ê e: system resides in some state or with the occurrence of another event that cancels tv e s: tv(e) = s, If the validity interval of event is some state this means that, after s e being sensed for the first time in (or one of its substates), will be valid until the s. system leaves The higher the state in the state hierarchy, the longer the validity interval. tv e ê: tv(e) = ê, If the validity interval of event is some event this means that, e ê after being sensed, will be valid until occurs. 3 Towards Executable Specifications for Case Management Processes 69 approval received For example, the event for starting the evacuation procedure Flood Emergency (not shown in the statechart in Fig. 3.3) is valid as long as the is E6 Ê6 active; (traffic jam) event is valid until (fluid traffic) event is received. instantaneous If no validity interval is specified for an event, this event is an event. the triggering event e[c] We extend the definition of triggering event as follows: ∈ t t e E of a transition is an event that must occur for to take place. Here is a combination of events and/or absence of events observed during some period of time ∈ c C (validity interval) that triggers the transition; is a condition that needs to be e true in order the transition to be taken when occurs. This definition allows us to take into account not only immediate events but also relevant events observed in the past (email received, approval obtained, etc.). Mandatory versus optional activities. In the statecharts formalism, activities can state-dependent be considered as [56] (i.e., each state s is associated with a list of mandatory: activities to be carried out in this state). These activities are also they are automatically executed once their preconditions are met. a situation a reaction To relax the coupling between and and to allow for more state-independent flexibility in process execution, we propose to define activities for statecharts—activities, that can be executed in any configuration if their preconditions are met (unless explicitly stated otherwise). s Thus, each state can be associated with two (possibly empty) sets of activities: mandatory s the set of activities that must be carried out in and are state-dependent enabled and the set of optional or activities that are defined dynamically, based on within throughout the statechart status. Optional activities can be executed or the state in order to ensure the right progression of the case towards its goal. The specification of activities is beyond the scope of this chapter. 3.6 Perspectives and Roadmap for Future Research The benefits of the proposed formalism are numerous for CMP: executable statecharts specifications allow for interactive design, simulation-based testing and simulation- based recommendations. In the future, these features could be integrated as a part of CMP-supporting PAIS in order to provide intelligent decision-support functionalities for case managers. To conclude this chapter we discuss these perspectives and outline the directions for future work. 3.6.1 Design and Simulation-Based Testing Interactive design of CMP. A statecharts specification can be created based on some a priori knowledge about the CMP (e.g., norms, regulations, best practices, etc.) (Fig. 3.4a). Thanks to the concept of hierarchical state, this model can be extended 70 I. Rychkova et al. Fig. 3.4 Incremental design of CMP and refined by integrating the experience of the case manager: new states and state transitions can be specified reflecting new situations and the way to deal with them (Fig. 3.4b); concurrent substates can be added in order to increase the scope of the Rhapsody10) and open source YAKINDU State- process (Fig. 3.4c). IBM Rational (SCT)11 are examples of statecharts modeling environments, where the chart Tools statecharts specifications can be created and executed in an intuitive and interactive way. However, creating a detailed statecharts specification for a real-life CMP is a challenging task, as explained below. Using clustering techniques for statecharts improvement. Although statecharts have been designed to represent states in a hierarchical way, this formalism does not specify how states should be organized into abstraction levels. In most cases this clustering of states is performed manually by a process designer. Clustering algorithms gather entities (e.g., the states of a state machine) into clus- ters according to some similarity criteria that can include complex sets of parameters. Formal Concept Analysis (FCA) [17] is a well-known clustering technique that is successfully used in many areas including knowledge discovery, representation and sharing [39]. In FCA, the obtained clusters are organised into a lattice using gen- eralisation and specialisation relationships, which could be used to identify state hierarchy in statecharts. A significant advantage of FCA is that the resulting clusters may overlap, whereas many traditional clustering techniques build partitions. FCA can therefore be used for clustering states and helping to define the hierar- chical structure within a statecharts specification. Various attributes may be chosen to describe states: pre-conditions, post-conditions, contextual parameters, and any combination of them. As a result, each FCA concept (cluster) is explicitly labelled by the set of attributes that characterize the objects of the cluster. This can be considered as a starting point for further model analysis and improvement: detection of “missing” states or state transitions, identification of “similar” states or activities etc. Simulation of CMP specifications. Statecharts combine an intuitive and concise visual notation with precise semantics. Thanks to these semantics, the statecharts 10http://www-03.ibm.com/software/products/en/ratirhapfami. 11http://statecharts.org/index.html. 3 Towards Executable Specifications for Case Management Processes 71 Fig. 3.5 Simulation-based testing of CMP scenarios specifications can already be simulated at the early design stages, providing an instant visual feedback. At the later design stages, they can serve as a basis for simulation- based testing as explained below. A statecharts specification can be executed with a test event log (i.e., a pre-recorded sequence of events defining some flood development scenario) allowing for the sim- ulation and testing of various handling scenarios. Two simulation modes can be defined: • A fully automated mode (Fig. 3.5a), where the statecharts specification is executed with a test event log that includes both contextual events (e.g., raise of water level, traffic jam) and system events (e.g., successful deployment of equipment, empty stock of drinking water). The events from the event log are processed by a stat- environment12 triggering the state transitions. The simulation echarts simulation result is a sequence of visited states. • In an interactive mode (Fig. 3.5b), the test event log contains only contextual (external) events and emulates the environment. A case manager reacts to external events by executing enabled activities (e.g., deploying equipment, making task assignments)—these activities represent the steps of case handling scenarios. Similarly to computer simulator games, the interactive statecharts simulation is an iterative process, where the case response is simulated after each step taken by the case manager: pre-recorded external events and internal events resulting from the case manager’s decisions trigger state transitions in statecharts specification and Sx once the (new) current state is entered a new step starts. The simulation result is the sequence of visited states, executed activities and received events. In the case of a crisis management process, multiple scenarios can be “played” automatically or interactively and used as a basis for trainings, drills and improvement of formal operation procedures (e.g., procedures described by EPSF). 12Development of modeling and simulation environment for CMP will be addressed in our future work. 72 I. Rychkova et al. Conversely, possible case development scenarios can be calculated as sequences of events acceptable by the state machine representing the CMP. This could help to analyse the process and reveal scenarios that were not considered before. 3.6.2 Simulation-Based Recommendations Gartner’s Hype Cycle for Emerging Technologies report provides a cross-industry perspective on technologies and trends, with an assessment of their maturity, adoption 2014,13 Predictive and business benefit. According to the reports from 2013 and Analytics technologies have already reached their plateau of productivity and are currently becoming the mainstream technology, whereas Complex-Event Processing (CEP), Big Data and Content Analytics are currently rolling down from their peak of inflated expectations and will reach their maturity (the plateau) in 5 to 10 years. This makes run-time situation analysis and recommendations for case managers the next challenge for the CMP-supporting PAIS. Some recommendation systems supporting process modeling and process man- agement are presented in the literature [29, 50]. Process mining is a widely recognised technique for predicting a best process scenario based on the analysis of past exe- cution logs [4]. The approach reported in [8] uses constraint-based programming to generate recommendations on process execution strategies. An example of CMP solution integrating intelligent support for the case manager is reported in [26]. Here the authors introduce the concept of User-Trained Agent (UTA), which recommends the best next actions based on the actions taken by the case managers in previous similar situations. The proposed recommendation technique is based on pattern recognition and is integrated as a part of ISIS Papyrus platform. Whereas all the approaches for recommendations mentioned above are based on “past experience” or process logs, we propose an alternative technique that is based the execution of a CMP specification in the simulated process environment: on In our vision, the “a priory” statecharts specification of a CMP can be analysed using graph theory algorithms. The objective of the analysis is to search and optimize a path from some current state of the statecharts model to its target state, representing the goal of the process. As a result, the “best next state to visit”, “best next transition to fire” and, consequently, “possible activities to execute” are recommended to the case manager. The main advantage of this analysis is that recommendations can be provided based on: • our current knowledge about the process represented by its executable statecharts specification and • our current knowledge about the process environment, represented by a real-time event buffer (or event log). 13Gartner, http://www.gartner.com/newsroom/id/2575515, http://www.gartner.com/newsroom/id/ 2819918. 3 Towards Executable Specifications for Case Management Processes 73 Fig. 3.6 Run-time recommendations on the CMP activity planning No “past experience” represented by a log of the past process executions is required. This makes “cold starts” possible. real-time A statecharts specification could be initialized and then executed using a event log (i.e., where both contextual (external) and system (internal) events occur in Sx St, real time). Given a current state of the statecharts and the desired (target) state paths Sx possible case management scenarios can be calculated as alternative from St to on the statecharts diagram. Each scenario can be seen as a sequence of “correct” state transitions resulting from the execution of corresponding activities (Fig. 3.6). The alternative scenarios and activities that need to be executed in order to rein- force these scenarios could then be recommended to the case manager. The integration of CMP executable specifications and analysis tools as a part of CMP-supporting PAIS could provide an intelligent support for case managers, as explained below. 3.6.3 Enhancing the CMP-supporting PAIS with Recommendations for Agile Activity Planning vision Figure 3.7 illustrates our of the intelligent CMSS introduced in our earlier works [48, 49]. We describe below the main components of this system: Dynamic context manager (DCM), Navigation manager (NM), Activity/Resource repository, Log and History. the Dynamic context manager The role of is to select, measure and monitor rele- vant contextual variables of the CMP. Internal and external events are collected and Event log. Activity/Resource repository stored in the The stores the definitions of activities that can be performed during the case handling and resources that can be Log and History used. The component registers the ongoing process scenario (i.e., the sequence of executed activities, received events and visited states). Navigation Manager The is the “heart” of the system, it provides intelligent support for the case manager by recommending the best scenario(s) for handling Executable statecharts specification the case. It contains the of the CMP and the Recommendation component that uses graph theory, process mining and clustering algorithms in order to provide recommendations for the case manager. 74 I. Rychkova et al. Fig. 3.7 Intelligent CMSS. The Navigation Manager provides recommendations about the best scenario based on the current state of the process and the list of valid events Executable statecharts specification behavior The of CMP models the of the SUD and its Environment. It can be executed with the collected real-time events. Possible case management scenarios are described by the sequences of states of the statecharts model that lead from the current state to some target state that represents the CMP objective. Recommendation The component can provide the case manager with an insight about how the situation might develop and about the possible strategies (paths in statecharts, activities, groups of activities to carry out) to bring the situation under control. The recommendation mechanism uses the Activity/Resource repository to define the list of activities enabled at a given situation identified with the current configuration (state) of the statecharts model. Since activities are independent from states, new activities can be added to the Activity/Resource repository at run time and further used by the Recommendation component without needing to change the model. The intelligent CMSS sketched above will be grounded on the statecharts spec- ifications enabling incremental interactive process design, simulation-based testing and recommendations. According to the statecharts formalism, a case management process is represented by a hierarchical state machine, where the process scenario a dynamic choice of activities with an objective to trigger a “good” can be seen as state transition that would move the case from its “current state” towards its “target state” representing the case management goal. The development of a prototype of the intelligent CMSS is our future work.",
  "2017-RZ": "Towards Decentralized IT Governance in the Public Sector: a Capability-oriented Approach Irina Rychkova Centre de Recherche en Informatique University Paris 1, Pantheon-Sorbonne 90, rue Tolbiac, 75013, Paris, France irina.rychkova@univ-paris1.fr Jelena Zdravkovic Department of Computer and System Sciences Stockholm University Borgarfjordsgatan 12, 164055 Kista, Sweden jelenaz@dsv.su.se Modern public organizations undergo an important transformation becoming a part of dynamic “innovative ecosystem” where they co-create value with citizens, government, policy makers and other institutions. Information Technology plays a central role in this transformation. Getting more value from IT becomes an intrinsic part of organizational mission. Information Technology Governance (ITG) is an important instrument that ensures that the organization will succeed in its mission. Efficient yet adaptive ITG is indispensable. To respond to the increasing service demand, public organizations require resources and capacities that lay outside the organization. Co-production, engagement of citizens and partner organizations, open innovation – are some of the major challenges. Meeting these challenges, public organizations need to master new governance styles to overcome the shortcomings of hierarchical structures and centralized decision making. In this work, we define a model where we adopt the theory of public value in order to reason about different contexts where ITG mechanisms are proposed as capabilities. We distinguish between three ITG styles: centralized, federated and decentralized and thereby provide a rationale allowing public organizations to identify an ITG style that fits best to their value-creation context and corresponding capability patterns as reusable ways for implementing governance of IT. 1 Introduction Today, technologies are emerging and evolving at an ever-increasing rate - e- Government, distance/hybrid education (e.g., MOOC), e-Health, e-Commerce are just few examples of influential applications of Information Technologies (IT) which shape strategies in both private and public sector. Getting more value from IT is an increasingly important organizational competency (Weill and Ross 2004); in this view, Information Technology Governance (ITG) is an instrument that can ensure that the organization will meet its strategic goals. (Weill and Ross 2004) defines IT governance as a part of corporate governance, focused on specifying the decision rights and accountability framework to encourage desirable behavior in using IT. ITG can be persisted in an organization as a set of organizational arrangements and patterns of authority addressing the major areas in the organizational IT (Sambamurthy and Zmud 1999). In (De Haes and Van Grembergen 2015), the authors emphasize the importance of IT governance in the organization, as an integral part of the corporate governance, overseeing the definition and implementation of processes, structures and relational mechanism in the organization that enable both business and IT people to execute their responsibilities in support of Business/IT alignment and the creation of business value from IT-enabled business investments. In this chapter, we examine IT Governance in the context of public organizations. According to (Moore 1997), strategies in public sectors are 1) focused on long-run over short; 2) attending to large issues with big impact on performance rather than small issues with impacts on productivity; 3) concentrating on ultimate ends rather than needs. Missions of public organizations are mostly associated with important social outcomes and require long-term strategies and strong commitment in all the operational areas, including IT. Whereas opportunities are limitless, resources are scarce and complexities are growing. In order to be successful in creating value from their IT, modern public organizations have to ensure: (i) continuous analysis of the aimed value, including social and political impact; (ii) continuous engagement of beneficiaries (clients, customers, citizens) into setting the objectives for the IT and evaluation of the results; (iii) continuous engagement of partner organizations (co-producers) into standard creation and use (Weill and Ross 2004), (Moore 1997), (Moore and Khagram 2004). Meeting these requirements is challenging due to inherently hierarchical structure of public organizations and centralized decision making that also applies to their IT. According to (Mintzberg 1979), “Centralization is the tightest means of coordinating decision making in the organization. All decisions are made by one individual, in one brain, and then implemented through direct supervision”. Whereas efficient in closed stable environment, centralized decision making shows serious drawbacks in open environments driven by innovations. Modern public organizations need to become a part of dynamic innovative ecosystem where they co-create value with citizens, government, policy makers, other public and private organizations and institutions. To succeed in their missions, public organizations need to master governance styles that overcome the shortcomings of hierarchical structures and centralized decision making. Whereas centralized organizational structures have been long dominating in the past with their governance styles (Weill and Ross 2004), (Gordon 2014), organizations adopting federated and decentralized decision making are gaining attention over the last decade (Morgan 2014), (DuMoulin 2015). Decentralized style of ITG does not rely on traditional structures such as executive committees, boards and management hierarchies, but rather on a broad participation of stakeholders. Decentralization should not however be confused with anarchy that accepts neither structure nor control: modern decentralized organizations support hierarchies based on merit and experience (not on a position in an administrative ladder); they adopt decentralized communication, coordination and control following the principles of social peer-to-peer. IT-enabled value creation depends strongly on the organizational context. To effectively develop its ITG structures, processes and relational mechanisms, the organizations need first to define the context of IT governance and to identify the governance style that fits best to this context. We adopt the public value approach for strategic management in public sector developed in (Moore 1997), (Moore and Khagram 2004). Public value describes the value that an organization contributes to society. For public organizations, it is the value developed for individual citizens, communities and organizations through provisioning new services and improving existing services, including IT services and services enabled by IT. Thus, we consider public value as an important concept for ITG creation. We propose to examine the following elements in order to define the ITG context: (1) the public value from the IT that the organization is going to produce (2) the sources of legitimacy and support that would authorize the organization to act and (3) capabilities the organization will need in order to deliver the result. We identify four context models and map these models on three ITG styles – centralized, federated and decentralized (Fig. 1). To facilitate a value, and context-based formalization of ITG, we consider it as an organizational capability. In the context of business planning, it is becoming recognized as a fundamental component to describe what a core business does and, in particular, as an ability for delivering value, beneath the business strategy (Ulrich and Rosen 2011). The interest in reasoning about ITG in an organization in terms of capabilities is twofold: a) ITG capabilities can describe the value from IT and an ability for delivering this value by encouraging context-specific relevant behaviors in using IT; b) ITG capabilities support configurability of ITG structures, processes and relational mechanisms on a higher level. We distinguish between three ITG styles (centralized, federated and decentralized). These styles imply the use of different ITG capabilities. We define the corresponding ITG capability patterns as the regular and repetitive means for the implementation of the three ITG styles. These patterns can be used to simplify and to guide a context- specific description of ITG structures and processes. Fig. 1 presents a short overview of the chapter, linking together the underlying theories (ellipses), established concepts (rectangles) and our contributions (rounded rectangles). Fig. 1. Organization of this chapter. The rounded rectangles depict the main contributions of this chapter; the ellipses depict the main theories; the rectangles depict the main concepts we address. The reminder of this paper is organized as follows: in Section 2 we present the relevant theoretical background; in Section 3 we discuss the results and in Section 4 we illustrate these results on a case of the Higher Education sector. Section 5 provides concluding remarks and directions of future work. 2 Theoretical Background In this section, we present the main theories and concepts that leaded us to the definition of IT governance capability that will be addressed in the next section. First, we provide a definition of IT governance and acknowledge several frameworks in IT governance. In particular, we focus on the IT governance framework developed in the Center for Information Systems Research, Sloan School of Management of MIT (Section 2.2). This framework is widely recognized by both academics and practitioners and it provides an important foundation for our work. Than we discuss some challenges of IT governance in public organizations (Section 2.3). In order to understand the nature of these challenges, we discuss organizational structures (Section 2.4). Our experience and observations show that modern public organizations have complex structures and tend to exhibit the properties of centralized, federated and decentralized organizations. Thus, adopting relevant IT governance styles can be of a vital importance. Capacity and ability of an organization to execute various IT governance mechanisms depending on its value-creation context can be considered as an IT governance capability. We discuss the capability-driven approach in Section 2.5. 2.1 IT governance and Related Frameworks According to the Organization for Economic Cooperation and Development (OECD), corporate governance provides the structures for determining the organizational objectives, for attaining those objectives and for monitoring performance (OECD 2015). As a part of corporate governance (Weill and Ross 2004), IT Governance is focused on the management and use of IT to achieve corporate performance goals needing to address three questions: 1. What decisions must be made to ensure effective management and use of IT? 2. Who should make these decisions? 3. How will these decisions be made and monitored? For implementing ITG, an organization has to identify the scope of IT and the main areas/issues where decisions have to be made; it has to define decision- making structures (i.e., organizational units, specific roles, committees) responsible for making these IT decisions; it has to design and implement processes for IT decision-making and IT monitoring to ensure the desired behaviors using IT; eventually, it has to specify the mechanisms supporting the active participation of, and collaborative relationship among entities appointed to defined governance structures, and according to an organizational governance style (Weill and Ross 2004), (De Haes and Van Grembergen 2015). Over the years, a number of frameworks have emerged, such as: ISO 38500 (ISO/IEC 38500 2015) is an international standard for corporate governance of IT at the highest level of organizations to understand and fulfill their legal, regulatory, and ethical obligations in respect of their organizations use of IT; COBIT (ISACA/COBIT 5 2012) provides a framework for governance and control process of IT with the focus of aligning it with business; IT BSC (Grembergen and De Haes 2005), where the theory of the balance scorecard is used as a performance measurement system for IT governance enabling strategies for improvement. Once an ITG framework is designed and implemented, the effectiveness of ITG can be measured based on some indicators. For example, the effectiveness of ITG can be perceived through the (increased) effectiveness and efficiency of the organizational IT, improved Biz/IT alignment, risk management etc. (De Haes and Van Grembergen 2015), (Wiedenhöft et al 2014). In our study we focused on a single perspective of ITG, i.e. its patterns for different organizational governance styles ranging from centralized to decentralized. Not aiming to design a new framework, we ground our proposal on a well-established ITG theory of Weill and Ross provided in (Weill and Ross 2004). 2.2 IT governance Framework of Weill and Ross Peter Weill and Jeanne W. Ross from the Center for Information Systems Research, Sloan School of Management of MIT, proposed an approach toward designing and implementing IT governance based on an extensive research and experience with a large number of organizations working in private and public sector (Weill and Ross 2004). The main concerns of their approach are the following: IT Decision Domains: Five interrelated IT decision domains are: IT principles, IT architecture, IT infrastructure, Business application needs, IT investment and prioritization. IT principles define desirable behavior for IT professionals and users. IT architecture is an organizing logic for data application and infrastructure aiming to achieve a desired level of business and technical standardization and integration. IT infrastructure is the foundation of shared capabilities (both technical and human). Business application needs includes decisions about specific needs that directly generate value. IT investment and prioritization consist of choosing which initiatives to fund and how much to spend on them. IT Governance Archetypes: they describe the combination of people or roles who have decision rights or who provide the input for one or several decision domains described above. Weill and Ross define six archetypes: Business Monarchy, IT Monarchy, Feudal, Federal, IT Duopoly and Anarchy. Except Anarchy, the archetypes strongly rely on the hierarchical structure of the organization (and its IT), requiring the involvement of CxO, BU leaders. They also imply that the decisions in main IT areas will be done within the organizational boundary and will not involve external stakeholders (e.g., partners, clients, government, employees). Besides the two “monarchy” archetypes that refer to a centralized organization already in their name, the three “non-monarchy” archetypes (i.e., federal, feudal, duopoly) also rely on leaders (business or IT) of the organization and do not specify further decentralization. The anarchy archetype specifies that the input will be provided or decision will be made by the business unit that owns the business process, by the project team or by the end users, supporting no structure or control. This is the reason why the anarchy archetype is rarely adopted by organizations. This corroborates with the empirical data collected by Weill and Ross and many other sources in the literature. Implementation Mechanisms: The approach provides three categories of mechanisms to specify how the decisions made by the identified individuals (or groups) will be enacted: decision-making structures, alignment processes and communication approaches. Decision-making structures clarify who is responsible and accountable for decisions. Examples of these structures are committees (IT project, IT security, Architecture, steering committee), executive teams, business unit leaders, IT leaders, heads of functional areas, key business process owners. Alignment processes ensure effective input to decision makers and implementation of their decisions. Examples of these processes are formal IT performance measurements, service-level agreements (SLAs), KPI, knowledge management. Communication approaches allow for disseminating governance processes and responsibilities to concerned actors. Examples of these approaches are CIO announcements, web portals, focused groups meetings, co-location, cross-training. The implementation mechanisms reflect and fit the governance style defined by the archetypes. Thus, not much support for decentralized decision making and open innovation is defined. 2.3 IT Governance in Public Organizations Public organizations deliver services that can be considered as a public good, or that are established by a government policy. Whereas for private organizations, value translates into the client satisfaction, public organizations are interested in achieving social outcomes, which are not always associated with client satisfaction (e.g., law enforcement, tax collection, etc.). The client of a public or governmental organization becomes a mean to an end rather than an end in itself (as for private organizations) (Moore and Khagram 2004). In both (Weill and Ross 2004) and (Moore and Khagram 2004) the authors recognize the following challenges of public organizations that need to be addressed when designing ITG: Measuring performance and value from IT: the value from IT does not translate into revenues or customer satisfaction. Measurement of concrete outputs and activities (e.g., a number of customers visiting a web page, a number of submitted/closed demands etc.) is often used as an alternative but inefficient. Organizations need to study and measure social outcomes from the IT. This measurement requires continuous communication with beneficiaries (citizens) and their engagement into both setting up the objectives for the IT as well as for evaluation of the results. Funding and prioritization of IT programs: Funding and prioritization decisions about IT programs are challenging as they can hardly rely on performance measurement due to above. Moreover, funding decisions are often made by political power holders who may not directly benefit from the program or service. To prioritize the investments into IT infrastructure and services, the analysis of their aimed value and their beneficiaries is required. Interoperability and partner engagement: To create and to benefit from co- production opportunities, public organizations need to develop and promote interoperability. The main challenge is how to encourage external co-producers to participate in standard creation and to invest in standard compliant systems and processes? Strong engagement with partner organization is required. More recently, dFogIT governance framework (CIPFA and IFAC 2014) which is based on the ISO/IEC 38500 emphasized the key principles of governance in public organizations - commitment to integrity and ethical values and openness and comprehensive stakeholders’ engagement; followed as well by the need for outcomes in terms of economic, social and environmental benefits, management of risks and performances, implementing good practices, etc. The work presented in (Juiz et al 2014) utilized this framework to illustrate a way to implement the ISO/IEC 38500 standard in a public high-education sector to assess maturity of organization’s IT governance, as well as to and further refine the principles of governance in the public sector as suggested in dFogIT framework. In (Janahi et al 2015) the authors have proposed a conceptual approach to the management of ITG in the public sector, where strategic/organizational objectives, human resources, IT resources and processes and activities are seen as the main concepts having well defined interconnection relations between them. When proposing our capability-oriented ITG patterns we took in the consideration both the outlined principles and the concepts as it can be well seen in section 3.2. 2.4 Organizational Structures and Decentralization The terms centralization and decentralization often refer to the power over the decisions made in the organization. According to (Mintzberg 1979), when all the power for decision making rests at a single point in the organization - the structure should be called centralized; to the extent that the power is dispersed among many entities, the structure should be called decentralized. Military organizations are typically examples of centralized organizations. They have an explicit hierarchy, with responsibilities and decision making power clearly defined and fixed for the positions within this hierarchy. Many public organizations have a hierarchical structure with federated decision making, where decisions are made by a group of individuals (a board or committee) appointed by the authority or government. This also applies to their IT. Centralized and Federated organizations are very stable and robust but they cannot respond easily to change and are slow to act (Mintzberg 1979). Information flow in these organizations is also an issue: once the organization grows, all the decisions cannot be understood in one center (Mintzberg 1979). People who see new opportunities and who understand what needs to be done to adapt, are not always sitting at the top of the organization according to (Morgan 2014). Following (Mintzberg 1979) and (Morgan 2014) we summarize the following reasons for decentralization in IT: Decision-making powers need to be shared. Power has to be placed where the (cid:120) knowledge is. Innovation through IT requires an extreme agility from organizations. Making (cid:120) decisions locally improves agility and reduces time needed to address the issue. Creative people require considerable room for maneuver. Resistance to new (cid:120) technologies due to the lack of understanding or fear to put at risk the existing position often comes from the center and jeopardizes new opportunities. 2.4.1. Decentralized Decision Making A number of organizational structures supporting decentralized decision making have recently emerged and became used. These structures are often addressed as “post-modern” organizations. These organizations are often grounded on the principles of social P2P (Bauwens 2005), implementing peer-production, peer- trust, peer-review, and peer-vote mechanisms for decentralized communication and decision making. The examples of post-modern organizations include Collaborative Network (CN), Virtual Organization, Coopetitions, and Sociocratic organizations. They distinguish from both centrally and federally governed organizations and from anarchies. Collaborative Network (CN) and Virtual Organization both refer to a group of independent business entities (or complete organizations) that share resources and skills to achieve their goals (Camarinha-Matos and Afsarmanesh 2005). Coopetition describes a complex relationship between two or more organizations that simultaneously are in competition and cooperate together (Bengtsson and Kock 2000). Sociocracy is a method for governance used by public, private, non-profit, and community organizations and associations. It represents an alternative to the traditional organizational structure based on cybernetic principles (i.e., as a system with feedback loops) (Buck and Villines 2007). The decision-making power is distributed within the organization (Romme 2016). Three fundamental principles of sociocracy include: (1) Decisions are made when there is a consent from all participants (i.e., no objections); (2) A sociocratic organization is composed of a hierarchy of semi-autonomous circles; (3) Each circle is linked to one above and to one below via individuals acting as links functiononing as full members in the decision-making of both (double-linking) (Endenburg 1998a), (Endenburg 1998b). Compared to a regular committee or board, a sociocratic circle is self-managed. While committee members might be appointed by an authority, individuals are elected to roles in sociocracy circles in open discussion using the same consent criteria used for other policy decisions. Currently socioracy is used by public, private, non-profit, and community organizations and associations. It represents an alternative to the traditional organizational structure based on hierarchy on one hand and to the flat management on the other hand. New branches that incorporate some of Endenburg's principles of sociocracy include holacracy. The examples of holacratic organizations include Sun Hydraulics, Valve, GitHub, Zappos. Among the core principles behind the post-modern organizations are self- organization and peer-to-peer (P2P) that were extensively studied in the literature. The idea of a process based on self-organization of equipotent participants was proposed in computing: Peer-to-peer is a distributed application architecture where peers make a portion of their resources, such as processing power, disk storage or network bandwidth, directly available to other network participants, without the need for central coordination by servers or stable hosts (Schollmeier 2002). Peer-to-peer architecture was also explored by social science, where the concept of commons-based peer-production (or social production) was proposed as an alternative mode of socioeconomic production (Benkler 2006). According to this principle, a large number of people work cooperatively, in contrast to traditional firm production, where the tasks are delegated by some central authority. (Bauwens 2005) describes P2P as “a template of human relationships”, a \"relational dynamic\" which is springing up throughout the social fields. The dynamics is based on free participation of equipotent partners, engaged in the production of common resources, without recourse to monetary compensation as key motivating factor. Social P2P does not deny 'authority', but only fixed forced hierarchy, and therefore accepts authority based on expertise. Related concepts include open innovation. It is a paradigm that assumes that looking to advance in their technologies, organizations should use external ideas as well as internal ideas, and internal and external paths to market (Chesbrough 2003). To conclude, we would like to illustrate organizational decentralization with examples of decisions that can be made in some of the five main IT domains defined by the IT governance framework of Weill and Ross (see section 2.2 for details). Examples of IT principles supported by a decentralized organization can (cid:120) include: knowledge management, technology-supported open innovation, use of open standards/ co-creation of standards, interoperability, compliance with (industry) standards. Examples of IT architecture decisions include: distributed (P2P) architecture, (cid:120) data integration, standard interfaces for communication, high cohesion/low coupling, SOA. Examples of IT infrastructure decisions include: use of cloud computing, (cid:120) grid computing; public infrastructure (e.g., the Internet, telecom networks); shared standard applications (ERP, CRM, SCM, etc.), shared standard application for communication and coordination (social networks, knowledge sharing platforms, groupware, VOIP etc.) 2.5 Capability-Driven Approach The capability notion originates from competence-based management and military frameworks, further advancing the traditional enterprise modeling approaches by representing organizational designs from a result-based perspective. From the business perspective, a capability describes what the business does that creates value for customers. It represents a design from a result-based perspective including various dimensions including organizational values, goals, processes, people, and resources. The notion has a growing presence in the current business and IT alignment frameworks starting from more business-oriented such as Business Architecture and Business Modeling, towards the alignment-oriented represented by Enterprise Architecture (EA), and Enterprise Modeling (EM). In brief, the emergence of the use of the capability notion seems having the following motivations: (a) in the context of business planning, it is becoming recognized as a fundamental component to describe what a core business does and, in particular, as an ability for delivering value, beneath the business strategy (Ulrich and Rosen 2011); (b) it supports configurability of operations on a higher level than services and process, and according to changes in operational business context (B(cid:413)rziša et al 2015). Following the above, we consider IT Governance capabilities as abilities and capacities of an organization to ensure maximum value from its IT in a given context. We further define ITG capability patterns to provide guidelines for practical adoption of the governance styles (centralized, federated and decentralized) in public organizations. 3 ITG Capability for Public Organizations 3.1 Decentralization in IT Governance Following (Weill and Ross 2004), we consider three distinctive IT governance styles: Centralized (C), Decentralized (D) and Federated (F). Centralized ITG relies on Business or IT monarchies in most of decision (cid:120) areas. This governance style is relevant when the high degree of standardization is required and cost-efficiency is one of the primary value sources. Federated ITG relies on duopolies and federal governance structures. This (cid:120) style can be beneficial for organizations seeking for cost-efficient use of the assets and IT-enabled innovation. Decentralized ITG fits to organizations focusing on innovation and time to (cid:120) market and with the tendency to delegate decision making from the center to local units or project teams. Weill and Ross relate the ITG to the value an organization seeks to create from IT, but it does not provide an explicit link to the organization’s environment where this value will be created. A modern public organization can be seen as a part of dynamic ecosystem, where it maintains the relationships of different nature with the other organizations and individuals (Fig.2). In order to successfully achieve its goals in this complex environment, the organization needs to master different governance styles and use them according to the context. Fig. 2. Organizational ecosystem. C (centralized), D (decentralized) and F (federated) characterize the relationships between the organization and the other parts of its environment. 3.1.1 Applying the Theory of Public Value for Defining the ITG context In order to identify the context for public organizations, we adopt the theory of Public Value proposed in (Moore and Khagram 2004) identifying three main characteristics of public organizations: public value, authorizing environment providing legitimacy and support, and core organizational capabilities (internal and external) required to produce this value. Public value describes the value that an organization contributes to society. Value for the public is a result of evaluations about how basic needs of individuals, groups and the society as a whole are influenced in relationships involving the public (Meynhardt 2009). Whereas private value is associated with satisfying individual desires, public value is mostly focused on achieving social outcomes. The Center for Technology in Government published a report, where they studied five U.S. and international governments, examining the full value of government IT investments (Cresswell et al 2006). The results of this research demonstrate that the IT investments generate public value of two distinctive types: value from improvement of internal government operations and processes (e.g., improving quality of service, cost reduction) and value from broader political and social outcomes (enabling new services, creating working places, contributing to individual and community well-being). We generalize these findings and propose to distinguish between: 1. Value from delivering specific benefits directly to citizens 2. Value from improving the organization itself as a public asset Each of these value types can be associated with one or multiple value sources: cost saving, increase in quality of service, enabling new services, and intrinsic enhancements (i.e., changing environment providing political, social, cultural impact, improving general quality of life of an individual or a group). Legitimacy (and support). Public organizations are not free to choose their market - they are authorized to provide their services by their environment that comprises government, customers, employees, suppliers, local communities, citizens, policy makers, controlling organizations etc. The authorizing environment provides the organizations with legitimacy and support and may vary depending on the scope of the IT project and its aimed value. For example, public organizations can be mandated by their authorizing environment to deliver a specific service, ensuring compliance with regulations, recommendations and standards. In return, they benefit from their support (financial and legislative) while providing their services. Required core capabilities. Public organizations need to develop and manage their core capabilities (functionalities) in order to deliver results. Compared to private organizations, much of capacity required to produce public value lay outside the public organization and thus not under its direct control. To succeed in their missions, public organizations need not only to develop internal capabilities controlled by the organization itself, but also to explore co-production opportunities with external partners (e.g., other public and private organizations, volunteers, associations etc.) by means of external capabilities. We examine the three elements above (public value, legitimacy, and core capabilities) in order to define the ITG context asking the questions: 1. What is the important public value you are seeking to produce with the IT? 2. What sources of legitimacy and support authorize the agency, or wider system, to take action and provide resources to create that value? 3. What core capabilities does the agency and service provider require to deliver this result? We propose a model (Fig. 3) that represents four different situations (value- creation contexts) for ITG in public organizations and links them to the three ITG styles (C- Centralized, D- Decentralized, and F- Federated). We define the contexts based on two parameters: type of aimed public value (horizontal axis) and type of required core capabilities (vertical axis). The four models of value creation context are depicted as four quadrants on the plane separated by the dashed lines. For each of these four context models we give an example of a value source and a source of legitimacy and support: Context Model 1 (bottom left): IT programs aiming at improving the organization through cost saving and/or efficiency of their services (ex.: electronic registration for residence permit, marriage certificate) are receiving legitimacy and support from authority (controlling organizations and government). Authorizing organizations also can be main clients/requestors of such programs and provide funding for them. The organization in this case has to focus on compliance with regulations and standards. It creates foundation for internal operational capabilities. Full value may comprise the increased transparency of the organization and improved reputation. Context Model 2 (top left): IT programs aiming at improving the organization through enabling new services (ex.: on-line real time transport schedule and route planner) are receiving legitimacy and support from both authority (controlling organizations and government) and employees and external suppliers. Organization creates foundation for internal and external operational capabilities using co-production. Organization enables new capabilities both internally and externally (for community, co-producers etc.). Full value from the program can expand the organizational boundaries having an impact on society. Fig. 3. A model that links the ITG context to the ITG style Context Model 3 (top right): IT programs aiming at delivering benefits to a customer through co-creation of value with external partners (ex.: MOOC) are receiving legitimacy and support from local communities. Organization creates foundation for shared operational capabilities using co-production. Intrinsic enhancements and enabling new services for communities is the main focus. Context Model 4 (bottom right): IT programs aiming at delivering benefits to a customer through enhancing local capabilities (ex.: eTax, FATCA) are receiving legitimacy and support from authority (controlling organizations and government). Authorizing organizations typically are the main requestors (mandated services). Intrinsic enhancements and quality of service are the main focus. The three ITG styles (C, D and F) correspond to the areas below, above and between the solid curves accordingly. These areas are overlapping and covering more than a single quadrant: this indicates that the same governance style can be appropriate in different context models in order to encourage some desired behavior in using IT. One can interpret the figure as follows: Context model 3 requires mostly decentralized ITG; Context model 4 requires mostly centralized ITG since high control is required and resources are provided by the authority. Context model 1 requires Centralized or Federated ITG. Context model 2 - Federated or Decentralized ITG. Whereas Centralized and Federated ITG are widely addressed in the literature and supported by empirical study in (Weill and Ross 2004), governance arrangements, and mechanisms for Decentralized ITG are only gaining attention. As discussed in Section 2.2, the ITG framework proposed by Weill and Ross relies on (mostly) centralized governance archetypes. Considering the increasing interest of organizations in decentralized decision making, open innovation and co-creation of value, we find it justified to extend the list of governance archetypes proposed in (Weill and Ross 2004) to cover the gap between monarchy, feudal, federal and anarchy archetypes. 3.1.2 Decentralized decision making: Extending the list of governance archetypes Public organizations benefit from stability and robustness embedded into their centralized or federated structures. However, to meet the reality of a modern society where they are operating, public organizations need to master governance styles that overcome the shortcomings of centralized decision making. Collaboration and innovation opportunities are driving factors for modern organizations. To foster the innovation, modern public organizations need to encourage different behaviors with respect to the IT by exploiting a wide range of IT governance structures. In particular, the archetypes surmounting hierarchical structures and supporting decentralized decision making are of a great interest. Dismantling the (fixed) hierarchies does not necessarily lead to anarchy or absolute lack of control (Bauwens 2005). We introduce a sociocracy (Endenburg 1998) governance archetype and consider that it can cover the gap between centralized (or monarchy-based) archetypes and anarchy. In sociocracy, the inputs for the decisions can be provided by various stakeholders including project leaders, IT and domain experts, customers, employees, suppliers, local communities, representatives of controlling/regulating organizations, policy makers. Governance archetypes defined in (Weill and Ross 2004) specify the decision making rights either as appointed to the specific (fixed) positions in the organization (ex.: C-level executives, business process owners) or undefined, where all the user can do as they please (anarchy archetype). Sociocracy specifies another way, where the stakeholders are self-appointed or peer-appointed (peer-voted) to provide the input or participate in the decision- making. We consider that representatives of the controlling organizations play mostly advisory role providing an input for all the areas. The project teams, in contrast, can provide the input and make the decisions in all the areas. Partners and co- producers provide the input and can make decisions regarding IT architecture, infrastructure and business application needs. We consider that all the stakeholders can provide the input for the business application needs area and can participate in decision making regarding prioritization and funding. 3.1.3 Distributed decision making and P2P: Extending the list of governance implementation mechanisms Following the identification of decisions and the specification of input and/or decision rights, an organization must decide on detailed decision responsibility and accountability, how alignment will occur, and how information will be communicated throughout the organization. To support decentralized ITG arrangements, specific alignment processes and communication approaches that go beyond traditional organizational structure need to be defined. For example, responsible/accountable relations formally defined by hierarchical organizations might need to be replaced with principles of self-organization and relations based on trust and reputation. P2P provides a foundation for governance mechanisms supporting decentralized decision making. Practical implementation of sociocracy relies on the use of technology: application for cooperation, social software, groupware, and social networks are an integrated part of it. For example, to provide the input and facilitate the decision making in the business application needs area, an organization might need to create innovation labs (Magadley and Birdi 2009), define peer-review, peer-trust, peer-voting, and crowdsourcing (Howe 2006) processes. We consider the decentralized ITG as relying upon decentralized structures and adopting P2P principles for decision making at least for one IT area where decision should be made. Business applications and IT funding and prioritization are primary candidates. Sociocracy describes the decentralized structures and P2P provides foundation for the mechanisms. Based on the context, federal, duopoly and IT monarchy archetypes can be used for decision making regarding IT principles, architecture and infrastructure. In the next section, we explain how an organization can design its ITG capabilities by following three generic capability patterns reflecting the centralized, federated and decentralized IT governance. 3.2 ITG Capability Patterns The idea behind ITG capabilities is to specify what a public organization should be able to do to ensure viability and adopt its IT Governance strategies. We propose capability as a high-level functional concept (i.e. ability and capacity, Section 2.5), ensuring a set of organizational values which in turn determine a context, as well as set of goals realized by processes and resources. We have formalized the outlined concepts and relationships in a model (Figure 4) Context: it represents the information that can be used to characterize the (cid:120) situational environment of a public organization. The context of an ITG capability is defined by analyzing the public value the organization aims to create, its sources of legitimacy and support and core capabilities (Section 3.1.1), which eventually lead to the goals to be achieved and the processes and resources to support the goals. Legitimacy and Core Capabilities Support Defines Defines Context Public Values Defines Is valid in Lead to Actors ITG Capability Goals Fulfills Responsible Supported by Contains ITG Processes Resources Require Fig. 4. A model for IT Governance using capability based on organizational values and context Goal: it is a desired state of affairs that needs to be attained to realize (cid:120) established value(s). Goals can be refined into sub-goals forming a goal model refining desired behaviors in using IT, such as cost-effective use of IT, or effective use of IT for growth; and effective use of IT for business flexibility. Actor: it is a person or even a part of the organization holding the responsibility (cid:120) for the achievement of a goal; for ITG, these actors may be organization’s executives, IT decision makers, etc. Process: it is a series of actions that are performed in order to support one or (cid:120) more of the established goals. In the ITG domain the processes concern decision making about IT, coordination of IT processes, IT monitoring, performance management and other. Resource: When initiated, a process is perceived to engage or consume (cid:120) resources - people, materials, software. ITG processes rely for example on the actors involved in IT decision-making enactment and monitoring, as well as on the needed entities – technology and infrastructure supporting processes’ execution, as well as coordination and communication between involved actors. We formalize ITG capability patterns that specify the elements from the model in Fig. 4 following centralized, federated and decentralized ITG styles. Based on a vision of its public value (i.e., improving the organization or direct delivering of benefit to a customer), sources of legitimacy and support and required core capabilities (i.e., internal or co-production), a public organization could specify the context, goals, actors, processes and resources required to define its ITG (Section 3.1). Table 1. Capability pattern: Centralized IT Governance, public sector Context Using IT for delivering direct benefit to a customer or to a community: value sources may range from improving user experience from the existing public services, to providing new services unavailable before, and to broader impact on the society via intrinsic enhancements (e.g., improving quality of life, ecology, economic growth, sustainability) Using IT for improving the organization (i.e., organizational business processes, applications, infrastructure): value comes from improving efficiency and effectiveness of the organization itself, resulting in the reputation and public opinion. Government, controlling organizations and policy makers are the main funding source and the main client of the program/project. They authorize the program/project and support it in a form of appropriate recommendations, laws, directives, standards Example: mobile and radio communication standards, regulations on privacy/security. Partner organizations can provide some capacities for “non-core” operations For achieving the broader social impacts, individual citizens become “a means to an end”; customer satisfaction is not always a priority. Goals Cost-effective use of IT; Compliance with the provided business and technology standards; High process standardization and/or high process integration; Centralized data management; Centralized change management and exception handling Actor IT governance structures follow monarchy, feudal, federal archetypes – the relevant actors are C-level executives, Representatives from authority, Corporate IT and/or unit IT leaders, Heads of functional areas, Key business process owners. Process IT performance measurement based on KPIs, IT portfolio management, SLAs, formal communication/coordination processes based on hierarchy (steering committees and boards). Resource Actors (see above), enterprise-wide standard solutions providing centralized reporting, Business Intelligence, ERP, CRM, SCM. ITG backend Organizational IT plays the role of a in the integrated value- Capability creating system, supporting the business logic. The organization creates the foundation for its internal core capabilities. Table 2. Capability Pattern: Federated IT Governance, public sector Context Using IT for improving the organization: value comes from improving efficiency and effectiveness of the organization itself (e.g., cost saving, improved processes, enabling new internal services), resulting in reputation and public opinion. Using IT for delivering direct benefit to a customer or to a community: value sources include improving user experience from the existing services, enabling new services for citizens, enabling cooperation opportunities and partnerships for other organizations, and broader impact on the society via intrinsic enhancements. Government, controlling organizations and policy makers are the main client of the program/project. They authorize the program/project and support it in a form of appropriate recommendations, laws, directives, standards Example: mobile and radio communication standards, regulations on privacy/security. Communities and citizens indirectly evaluate the outcomes expressing their opinion about the organization as a whole. Goals Cost-effective use of IT; Effective use of IT for asset utilization; Effective use of IT for growth; High process integration; Centralized data management Actors IT governance structures follow duopoly, feudal and federated archetypes – the relevant actors are: C-level executives, Representatives from authority, Project Leaders (internal and external), IT and domain experts (internal and external), Representatives from controlling organizations Processes IT performance measurement based on KPIs, SLAs, Processes for conflict resolution between local control (at co-producers) and global control (organization and authority), Coordination between the central and local production, Semi-formal processes for communication and coordination on the horizontal level (focused groups, discussions, communities of practice) supported by technology. Resources Internal production and co-production based on shared resources (knowledge, technology, infrastructure, services): Infrastructure and solutions supporting coordination within and between levels (i.e., groupware, social networks); Standard solutions providing centralized reporting, Business Intelligence, ERP, CRM, SCM. ITG Organizational IT plays the role of a mediator (service bus), coordinating Capability and controlling the inter-organizational processes between partners. The organization itself provides the standards to ensure coordination/ communication between co-producers. It also links the co-producers with the end users (citizens). Table 3. Capability Pattern: Decentralized IT Governance, public sector Context Using IT for delivering direct benefit to a community: value sources may range from enabling new capabilities (co-production and cooperation opportunities, innovative ways to service delivery) to broader impact on the society via intrinsic enhancements (e.g., improving quality of life, ecology, economic growth, sustainability). Using IT for improving the organization (i.e., organizational business processes, applications, infrastructure): value comes from improving communication and knowledge sharing in the organization itself and within the extended organizational environment (e.g., organizational learning, communities of practice, social networks), resulting in increased creativity, motivation and commitment of individual employees. Government, controlling organizations and policy makers authorize the program/project and support it in a form of appropriate recommendations, laws, directives, standards. Example: mobile and radio communication standards, regulations on privacy/security. In value creation, an organization cannot rely uniquely upon its internal operational capabilities (e.g., it cannot respond to the demand or it does not have required expertise). Partner (public and private) organizations, community, individual citizens or associations provide (external) operational capabilities and expertise required to (co)produce value (e.g., IT service providers, mobile service providers, other non-IT organizations etc.) Communities and individual citizens participate in evaluation of the program/project (i.e, providing their opinion, feedback, “liking”, “sharing”, discussing, browsing, using etc) and its evolution. Goals Effective use of IT for growth and business flexibility; Distributed data management; Interoperability and (open) technology standards; Peer- production, Support for open innovation Actors IT governance structures follow federated and sociocracy archetypes – the relevant actors are: Project teams or their representatives, Representatives from controlling organizations, C-level executives, IT and domain experts (internal and external). Processes Focus is on decentralized decision making mechanisms (P2P based) and efficient communication and collaboration processes (supported by technology). Processes include: Peer-voting, Peer-trust management, Peer-review. Resources Co-production based on shared resources (knowledge, technology, infrastructure, services): Infrastructure and solutions supporting cooperation (i.e., groupware, social networks); ITG Organizational IT plays the role of a frontend providing the means for Capability information, knowledge and service discovery and sharing for the stakeholders defined by the context. Organization creates foundation for internal and external core capabilities based on P2P. It promotes interoperability and supports co-creation of standards, innovation in technology (internally and externally); as well as Open innovation. ITG capability patterns presented above provide organizations with the practical guidelines and support configurability of their ITG structures, processes and relational mechanisms on a higher level. To efficiently design and adopt ITG mechanisms, an organization needs to: 1. Define its IT governance context using the context models shown in 3 2. Define its fitting governance style (C, F or D) 3. Adapt the IT governance design (specify “more centralized” or “more decentralized” governance structures and mechanisms) 4. Enact the IT governance by instantiating the IT governance pattern (fill in values and context from; determine goals/actors and processes and resources based on the generic types from Table 1-3). The ITG capability patterns defined above are meant to provide guidelines and facilitate the application of ITG mechanisms. Whereas some elements vary strongly between patterns, some can remain very similar. For example, the need to respect regulations or follow standards (radio, telecom, IT development etc.) can be present in all three patterns, whereas the way these regulations will be met and the compliance control can be totally different. 4 Study of the Results Over the last few years’ higher education in Sweden was analyzed in (Zdravkovic et al 2015). The objective was to observe the alignment between the organizational structure and governance rules in use. As common, universities include a number of entities - faculties, faculty departments, and units. Nowadays, the entities are becoming more independent than before, due to geographical dislocation, decentralization of management and because of formal as well as informal communication patters in use. Universities therefore show an obvious need to adjust governance of their IT according to the organizational structure and decision-making in place. Providing education and conducting research activities are two core missions where university creates (public) value. The following three cases illustrate how the ITG capability patterns from the previous section can be instantiated to support the first mission – Education: Case 1: Improving education management with IT A Swedish university delivers a standard education service to its students. IT infrastructure and systems for managing student subscription for the university, their curriculum, results are typical examples of IT investments in the university. The aimed public value from the IT in this case can be expressed as improving the quality of educational service as well as intrinsic improvements for the community (i.e., social, cultural, financial impacts are envisaged). This corresponds to the Context Model 1 (bottom left) in Fig. 3. The government and high education policy makers on the country level can be considered as an authorizing environment issuing directives related to learning objectives, degrees, and quality requirements. Local communities and professionals can provide directions indicating the skills “most needed”. The university develops internal capabilities in order to create the aimed value. According to this context, the centralized ITG (Fig.3) is relevant. This implies the instantiation of the centralized ITG capability pattern presented in Table 1. Here organizational IT takes the role of a backend, supporting a more effective management of studies and the followed documentation. As for the IT-related goals, nowadays they target a cost-effective use of IT through centralization and standardization of software platforms and centralized data management, analytics and reporting. The actors at this level are the rector and a group of high-level administration officers centrally managing activities and making decisions which are then spread to faculties and departments. This corresponds to Business monarchy archetype. The processes and the resources supporting the main service are standardized, and controlled performance measurement based on from the top of the organization, such as IT KPIs, or IT portfolio management. Case 2: Enabling student mobility with IT Another example illustrates the use of federated ITG capabilities. Universities in Sweden are actively involved in national and international programs that support mobility of undergraduate, graduate and doctoral students and faculty members: joint master programs, Erasmus exchange programs etc. The aimed public value is to improve service quality and to enable new capabilities for universities, students and communities. Co-creation of value with partners from another organizations (universities) is required in this context. To ensure comparable, compatible and coherent systems of higher education, the partners (co-producers) need to comply with Bologna Process (Wikipedia/Bologna Process 2016). This compliance requires significant changes in the organizational IT. Therefore, the sources of legitimacy and support in this context include university authorities and policy makers on the country and European level (for Bologna Process). This corresponds to the Context Model 2 (top left) in Fig. 3. The federated ITG is relevant. This implies the instantiation of the federated ITG capability pattern (Table 2). Here organizational IT plays the role of a mediator, coordinating and controlling It also links the (partner) the inter-organizational processes between partners. universities with the students. Data integration between universities and standards for data exchange are of the main interest. The actors include university international office, head of faculties, program managers, faculty members, IT department. This corresponds to duopoly and federal archetypes. The processes and the resources supporting the main service are standardized, and controlled by the European representatives and university authorities in order to ensure the compliance. Examples of processes: SLAs, formal transformation of local grades to Bologna grading systems etc. Case 3: Personalized educational program for everybody Our last example illustrates the use of decentralized ITG capability. The concept of open education (Wikipedia/Open Education 2016) describes “institutional practices and programmatic initiatives that broaden access to the learning and training traditionally offered through formal education systems”. Open education programs include distant learning, e-Learning, MOOC. Adoption of technologies and processes for developing these programs is a strategic goal for many universities. The aimed public value is to provide a wider access to education. This is a direct service for customers. This implies an important social impact and intrinsic improvements. The source of legitimacy and support comprises communities and individual citizen willing to benefit from the program. Co-creation of capabilities is required. This corresponds to the Context Model 3 (top right) in Fig. 3. According to the context, we consider that the decentralized ITG is relevant. This implies the instantiation of the decentralized ITG capability pattern (Table 3). Here the organizational IT can play the role of a frontend: OpenCourseWare (OCW) is an example where the courses and supporting materials created at universities and published for free via the Internet. Interoperability is the main focus. A university needs to use some open standard or co-create a standard with other universities for on-line course delivery (ex.: https://studyinsweden.se/news/moocs-at-swedish-universities/- is a MOOC platform for Swedish HE). Organization creates foundation for internal and external operational capabilities based on P2P, e.g., peer-review, social platforms for discussions. Actors can include faculty teams creating the content and collaborating with web designers and IT experts; students, university authorities. Federal and sociocracy archetypes can be used for ITG. The processes and the resources supporting the main service are standardized. Compared to the previous example, the standards are not provided but peer-voted or co-created by the participants. 5 Conclusions Values created by public organizations and their IT in particular, expand the boundaries of these organizations. Therefore, modern public organizations need to be seen as a part of dynamic ecosystem, where it maintains the relationships of different nature with individuals and other organizations. To fulfill their goals, public organizations need to adapt to their context, exhibiting as consequence various behavior in using IT. In this study we proposed a value-driven solution for IT governance in public organizations distinguishing from centralized and federated, to decentralized structures. Whereas centralized and federated governance structures are well covered by a palette of governance archetypes proposed in (Weill and Ross 2004), we observed a lack of decentralized governance structures - only anarchy is mentioned and, as empirical studies demonstrate, very rarely used by the organizations. To narrow the gap between the business and IT monarchy, feudal, duopoly, federal archetypes and anarchy, we defined a sociocracy governance archetype. Sociocracy and its variants (e.g., holacracy) are used by public and private organizations, some examples are Valve, W. L. Gore, Morning Star, GitHub, Zappos. Sociocracy supports self-organization and decentralized decision making. Next, the organization needs to examine its context: what public value it desires to provide, what its authorizing environment is (i.e., who will authorize and support the value provisioning, who will benefit from it), and, eventually, what kind of core organizational capabilities will be required. These elements are interrelated and provide an understanding of a context where the IT governance will be enacted. We suggested that the model of IT governance can become “more centralized” or “more decentralized” according to this context. We considered IT governance as an organizational capability and defined three ITG capability patterns that reflect centralized, decentralized and federated ITG styles accordingly. An ITG capability pattern can be seen as a guideline on how to define IT governance to support desired public values driving different context situations. We illustrated how the IT governance patterns can be instantiated on the case of the Higher Education public sector in Sweden. Our motivating assumption was that public organizations need to master a wide range of ITG mechanisms and to deploy them depending on their value-creation context. In particular, we identified and discussed such mechanisms for decentralized ITG. We consider that our findings can be interesting for the organizations (public or private) that already experienced negative effects of centralized IT governance and decision making (e.g., project failures due to lack of commitment, poor adoption, bad user experience, etc.). This work can also interest the organizations that explicitly move towards decentralization in their IT and are seeking to adjust their ITG. The model proposed in Fig. 4 provides an organizing logic that can help organizations to position, justify and govern their IT projects in a consistent way, based on the public value concept. However, we deliberately show that the ITG styles are not mutually exclusive and that several styles can be used in the same value-creation context. We plan to elaborate the guidelines and recommendations in the future, by conducting multiple empirical studies and collaborating with practitioners. The ITG capability patterns presented in tables 1 - 3 are intended to facilitate the application of ITG mechanisms. They provide a general idea; the concrete “recipe” has to be elaborated for each particular organization. In our future work, we envisage to design an approach for measurement of efficiency of ITG with respect to the ITG style. Relevant KPIs can be integrated into ITG capability patterns presented in this work.",
  "2017-ZR": "1 Introduction Information technologies are evolving in business use at an endlessly increasing extent - e-Government, distance/hybrid education, e-Health, e-Commerce, e-collaboration, are just few examples of influential applications, which shape strategies in both private and public business sector. Getting more value from IT is an increasingly important organ‐ izational competency [2]; in this context, IT Governance is an instrument aimed to ensure that business organizations will meet their strategic goals. IT Governance is a part of corporate governance, focused on specifying the decision rights and accountability framework to encourage desirable behavior in using IT [2]. In [2], the authors emphasize the importance of IT Governance in organizations as an inte‐ gral part of the corporate governance. The purpose of IT Governance concerns over‐ seeing design and implementation of processes, structures and relational mechanisms in organizations to enable both business- and IT people to execute their responsibilities © Springer International Publishing AG 2017 A. Metzger and A. Persson (Eds.): CAiSE 2017 Workshops, LNBIP 286, pp. 39–49, 2017. DOI: 10.1007/978-3-319-60048-2_4 40 J. Zdravkovic and I. Rychkova in support of Business/IT alignment as well as the creation of business value from IT-enabled business investments. In this study, we examine IT Governance in the context of public organizations. The values of public organizations are notably associated with important social outcomes and require therefore long-term strategies and strong commitments in all their opera‐ tional areas, including IT. Whereas opportunities are limitless, resources are scarce, and operations’ complexities are growing. In order to be successful in creating values from their IT, modern public organizations have to ensure continuous engagement of bene‐ ficiaries (citizens) into setting the objectives for the IT and evaluation of the results as well as continuous engagement of partner organizations (co-producers) into standard creation and use [1, 3]. Meeting these requirements is challenging due to inherently hierarchical structure of public organizations and centralized decision making that also applies to their IT management. Whereas efficient in closed and stable business environments, centralized decision making shows serious drawbacks in open environments driven by innovations. Modern public organizations need to become a part of dynamic innovative ecosystem where they co-create value with citizens, government, policy-makers, as well as with other public and private organizations and institutions. To succeed in their missions, public organizations need to master governance styles to overcome the shortcomings of hierarchical structures and centralized decision-making. Public value describes the value that an organization contributes to society. For public organizations, it is the value developed for individual citizens, communities and organizations through provisioning of services, including lately increasing extent of online services enabled by IT. Thus, we consider public value as a foundational concept for structuring of IT Governance in the public sector. Furthermore IT-enabled value creation heavily depends on the organizational context – for effectively designing IT Governance structures, processes and relational mechanisms, public organizations need to define the context of IT Governance, which aside from organization’s value is influ‐ enced by its core functionalities, involved authorities, legislations, and other. we consider IT Governance as an organizational capability. In this research study, The interest in reasoning about IT Governance in an organization in terms of capabilities is twofold: (a) capability can operationalize the value from IT by defining the ability for delivering this value by compounding context-specific relevant behaviors in using IT; (b) capability can support configurability, re-use and mapping of IT Governance struc‐ tures, processes and relational mechanisms in an organization. The remainder of the paper is organized as follows: in Sect. 2 we present the relevant theoretical background; in Sect. 3 we describe our approach for structuring IT Gover‐ nance as capability, and in Sect. 4 we illustrate these results on a case of the Higher Education sector. Section 5 provides discussion, concluding remarks and directions of future work. A Capability-Oriented Approach to IT Governance 41 2 Background 2.1 Organizational Styles The terms centralization and decentralization often refer to the points of power over the decisions made in an organization. According to [4], when all the power for decision making rests at a single point in the organization (“center”) - the structure should be centralized; called when the power is dispersed among many organizational entities, the decentralized; structure should be called when decision making is shared between the federated. center and the other organizational entities, then the structure is Centralized organizations are very stable and robust but they cannot respond easily to change and are typically slow in acting. This also applies to their IT. Following [4, 5], we summarize the following reasons for decentralization in IT: – Decision-making powers need to be shared. Power has to be placed where the knowl‐ edge is. – Innovation through IT requires an extreme agility from organizations. Making deci‐ sions locally improves agility and reduces time needed to address the issue. – Creative people require considerable room for maneuver. Resistance to new tech‐ nologies due to the lack of understanding or fear to put at risk the existing position often comes from the center and jeopardizes new opportunities. 2.2 IT Governance in Public Organizations For implementing IT Governance, an organization has to identify the scope of IT and the main areas/issues where decisions have to be made. The organization has to define its decision-making structures (i.e., organizational units, specific roles, committees) respon‐ sible for making these IT decisions; it has to design and implement processes for IT deci‐ sion-making and IT monitoring to ensure the desired behaviors using IT; eventually, it has to specify the mechanisms supporting the active participation of, and collaborative rela‐ tionship among entities appointed to defined governance structures [1, 2]. Over the years, a number of IT Governance frameworks have emerged, such as ISO 38500 [6] and COBIT [7]. In our study, we are not aiming to design a new framework but rather to consider reusable IT Governance solutions for different organizational styles ranging from centralized to decentralized types. Centralized IT Governance fits when Business or IT monarchies are applied in most of decision areas [2]. This governance style is relevant when the high degree of stand‐ ardization is required and cost-efficiency is one of the primary value sources. Federated IT Governance follows duopolies and federal governance structures. This style can be beneficial for organizations seeking for cost-efficient use of the assets, and at the same time, IT-enabled innovation. Decentralized IT Governance fits to the organizations focusing on innovation and time to market, and with the tendency to delegate decision making from the center to local units or project teams. A modern public organization can be seen as a part of a dynamic ecosystem, where it maintains the relationships of different nature with other organizations and individuals (Fig. 1). To successfully achieve business goals in this complex environment, the 42 J. Zdravkovic and I. Rychkova organization needs to master different governance styles and use them according to a given context. Fig. 1. Organizational ecosystem. C (centralized), D (decentralized) and F (federated) characterize the relationships between the organization and other parts of its environment. 2.3 Capability-Driven Approach a capability describes what the business does that creates From the business perspective, value for customers [8]. It represents a design from a result-based perspective including various dimensions including organization’s values, goals, processes, people, and resources. The notion is obtaining a growing presence in the business and IT alignment frameworks [9] starting from more business-oriented such as Business Architecture and Business Modeling, towards the alignment-oriented represented by Enterprise Archi‐ tecture (EA), and Enterprise Modeling (EM). In brief, the emergence of the use of the capability notion seems having the following motivations: – In the context of business planning, capability is becoming recognized as a funda‐ mental component to describe what a core business does and, in particular, as an ability for delivering value, beneath the business strategy [8]; – Capability supports configurability of operations on a higher level than services and process, and according to changes in operational business context [10]. Following the above, we consider IT Governance capabilities as the abilities and capacities of an organization to ensure maximum value from its IT in a given context. A Capability-Oriented Approach to IT Governance 43 3 ITG Capability for Public Organizations Public value describes how an organization contributes to society. Value for the public is a result of evaluations about how basic needs of individuals, groups and a society as a whole are addressed in relationships involving service provisioning [11]. Whereas private value is associated with satisfying individual desires, public value is mostly focused on achieving social outcomes. We generalize the findings of [12] on how IT investments generate public value and propose to distinguish between: 1. Value from delivering specific benefits directly to citizens 2. Value from improving an organization itself as a public asset Each of these value types can be associated with one or multiple value sources: cost saving, increase in quality of service, enabling new services, and intrinsic enhancements (i.e., changing environment providing political, social, cultural impact, improving general quality of life of an individual or a group). Public organizations are not free to choose their market - they are authorized to provide their services by their environment that involves government, employees, suppliers, local communities, citizens, policy makers, controlling organizations, etc. The legitimacy and support authorizing environment provides the organizations with and may vary depending on the scope of the IT project and its aimed value. For example, public organizations can be mandated by their authorizing environment to deliver a specific service, ensuring compliance with regulations, recommendations and standards. core capabilities Public organizations need to develop and manage their in order to deliver results. Compared to private organizations, much of capacity required to produce public value lay outside the public organization and thus not under its direct control. To succeed in their missions, public organizations need not only to develop internal capa‐ bilities controlled by the organization itself, but also to explore co-production opportu‐ nities with external partners (e.g., other public and private organizations, volunteers, associations etc.) by means of external capabilities. IT Governance context In our view, the three elements above together define an answering (a) what public value(s) the organization is seeking to produce by support of IT; (b) what sources of legitimacy and support will authorize, provide, or consume resources to create that value; and (c) which core capabilities are in place to deliver the main service of the organization. For a given context, being determined by its three constituting elements, an ITG capability is used to specify what a public organization should be able to do to ensure support for that context by means of processes and resources, which in turn support organization’s goals measured by KPIs (Key Performance Indicators). These indicators are also highly important for public organizations as their stakeholders (state, munici‐ palities, citizens and other) want to ensure that an implementation of IT delivers values, which can be measured by corresponding KPI [13]. We have formalized the effectively above outlined concepts and relationships in a model (Fig. 2, below). Legitimacy (and Support): • to whom the organization is authorized to provide its services and by whom it gets support. 44 J. Zdravkovic and I. Rychkova Core Capabilities: • they are describing what the organization is essentially doing. Public Values: • they describe how the organization aims to contribute to the society. Context: • it represents the information that can be used to characterize the situational environment of a public organization. The context of an IT Governance capability is public value defined by analyzing the the organization aims to create, its sources of legitimacy and support core capabilities, and which eventually lead to the goals to be achieved and the processes and resources to support the goals. Goal: • it is a desired state of affairs, which needs to be attained to realize established value. Goals can be refined into sub-goals forming a goal model refining desired behaviors in using IT, such as cost-effective use of IT, or effective use of IT for growth; and effective use of IT for business flexibility. Key Performance Indicator (KPI): • it is a measurable property that can be seen as a target for achievement of a Goal. Actor: • it is a person or even a part of the organization holding the responsibility for the achievement of a goal; for IT Governance, these actors may be organization’s executives, IT decision makers, etc. Process: • it is a series of actions that are performed in order to support one or more of the established goals. In the IT Governance domain the processes concern decision making about IT, coordination of IT processes, IT monitoring, performance manage‐ ment and other. Resource: • When initiated, a process is perceived to engage or consume resources - people, materials, software. IT Governance processes rely for example on the actors involved in IT decision-making enactment and monitoring, as well as on the needed entities – technology and infrastructure supporting processes’ execution, as well as coordination and communication between involved actors. Core Capabili(cid:2)es Legi(cid:2)macy Defines Define Context Defines Public Values KPI Measured by Is valid in Lead to ITG Capability Fulfills Goals Responsible Actors Supported by Contains ITG Processes Require Resources Fig. 2. A model for IT Governance using capability based on organization’s values and context A Capability-Oriented Approach to IT Governance 45 IT Governance Capability: • it is ability and capacity of to ensure maximum value from its IT in a given Context. We explained in Sect. 2.1 that organizations may follow different organizational styles, where centralized, federated and decentralized are the essential; in addition, new styles are emerging. In [14] we have proposed different IT Governance solutions using the capability notion to fit each of the organizational styles to obtain the patterns that could be applied in concrete cases for forming adequate IT Governance structure. This study differs in the way that we here elaborate a case from the public sector (student mobility, see Sect. 4) for which the IT Governance capability pattern for the federated organization style applies (Table 1): Table 1. federated IT Governance, public sector Capability pattern: Using IT for improving the organization: Context value comes from improving efficiency and of the organization itself (e.g., cost saving, improved processes, enabling effectiveness new internal services), resulting in reputation and public opinion Using IT for delivering direct benefit to a customer or to a community: value sources include improving user experience from the existing services, enabling new services for citizens, enabling cooperation opportunities and partnerships for other organizations, and broader impact on the society via intrinsic enhancements Government, policy makers controlling organizations and are the main client of the program/project. They authorize the program/project and support it in a form of appropriate recommendations, laws, directives, standards Example: mobile and radio communication standards, regulations on privacy/security Communities and citizens indirectly evaluate the outcomes expressing their opinion about the organization as a whole Cost-effective use of IT; Effective use of IT for asset utilization; Effective use of IT for Goals growth; High process integration; Centralized data management Measures of the Goals; KPIs IT cost/total cost, Number of processes integrated, Centralized data/all data Actors IT Governance structures follow duopoly, feudal and federated archetypes – the relevant C-level executives, Representatives from authority, Project Leaders (internal actors are: and external), IT and domain experts (internal and external), Representatives from controlling organizations IT performance measurement based on KPIs, SLAs, Processes for conflict resolution Processes between local control (at co-producers) and global control (organization and authority), Coordination between the central and local production, Semi-formal processes for communication and coordination on the horizontal level (focused groups, discussions, communities of practice) supported by technology Internal production and co-production based on shared resources Resources (knowledge, Infrastructure and solutions supporting technology, infrastructure, services): coordination within and between levels (i.e., groupware, social networks); Standard solutions providing centralized reporting, Business Intelligence, ERP, CRM, SCM ITG capability Organizational IT plays the role of a mediator (service bus), coordinating and controlling the inter-organizational processes between partners. The organization itself provides the standards to ensure coordination/communication between co-producers. It also links the co-producers with the end users (citizens) 46 J. Zdravkovic and I. Rychkova 4 Example Case - Enabling Student Mobility with IT Universities (including the ones the authors of this paper work for) show an increasing need to adjust governance of their IT according to the organizational structure and deci‐ sion-making in place. In this example, we focus on the Federated IT Governance style and its corresponding capabilities. The Erasmus Programme (European Region Action Scheme for the Mobility of University Students) is a European Union (EU) student exchange programme estab‐ lished in 1987 (see also Wikipedia/Erasmus Programme). Erasmus students can spend between one and three academic years in another European country studying or making an internship in another (host) university. Universities in Sweden and France are actively involved in Erasmus mobility. The aimed public value is to improve service quality and to enable new opportunities for universities, students and communities. Co-creation of value with partners from other organizations (universities) is required in this context. One of the basic rights each exchange student has is the full recognition of courses passed successfully abroad by the home university. Before leaving the home university, a participating student signs the Learning Agreement a document that describes the - programme of studies followed in the host university. At the end of the stay, the host university prepares for the student a document called Transcript of Records, which confirms the completed studies’ programme and the results. These documents are the legally binding for all parties involved (i.e. the home and host university). Should a student face problems in recognition, the student can seek a help of student organizations to make the courses validated [15]. In practice, Erasmus requires a tight coordination between university international department, university administration, faculty administration and European authorities. The following issues illustrate the need of flexible yet explicit IT Governance mecha‐ nisms for supporting Erasmus program at the universities: Course planning Learning agreement typically allows a student to choose courses from different master programs, different levels of study (e.g., master of the first or second year) sometimes even offered by different university departments. Planning the courses while allowing maximum flexibility for exchange students require a tight collaboration and coordination between different administration levels and departments at the involved universities. Grading and grade mapping Different approaches to education, cultural specifics, language and local grading systems are hard to merge and to map to single objective evaluation greed. Even though the Bologna system offers one, it needs to be adjusted locally, according to the university and country specifics. To ensure comparable, compatible and coherent systems of Higher Education, the partners (co-producers) need to comply with Bologna Process [16]. This compliance requires significant changes in the organizational IT. Therefore, the sources of legiti‐ macy and support in this context include university authorities and policy makers on the country and European level (for Bologna Process). This context reveals that the A Capability-Oriented Approach to IT Governance 47 federated IT Governance is the most relevant (Table 1, the previous section). When implementing the federated IT Governance capability pattern presented in Table 2, organizational IT plays the role of a mediator, coordinating and controlling the inter- organizational processes between partners. It also links the (partner) universities with the students. Data integration between universities and standards for data exchange are of a main interest. The actors include university international office, head of faculties, program managers, faculty members, IT department. The processes and the resources supporting the main service are standardized, and controlled by the European represen‐ tatives and university authorities in order to ensure the compliance. Table 2. Instantiating Federated IT Governance pattern for supporting student mobility Context Value comes from efficient processes and services for managing mobility programs: e.g., providing the incoming students with accommodation, insurance, transport cards, easy access to the university facilities, language courses, etc. Coordination and planning the curricula, providing supporting material in English if the courses are in local language etc. Internal services for course planning if an incoming student selects modules from different programs/departments/faculties University administration, faculty administration, European level authorities can be considered as the main client of the Erasmus mobility programme. They authorize the program and support it in a form of appropriate standards (i.e., Bologna agreement) Communities and citizens indirectly evaluate the outcomes expressing their opinion about the organization (i.e. University) as a whole Goals High process compliance and integration and centralized data management in order to ensure compliance with Bologna and seamless coordination/ communication with partner universities (host universities – home universities) KPIs Number of processes integrated, Number of compliant processes, Centralized data/all data Actors The relevant actors are: European committees for higher education, Erasmus coordinators on European, country and university levels, faculty administration, representatives from controlling organizations Processes IT performance measurement based on KPIs, SLAs, Processes for conflict resolution between local control Universities and faculties) and global control (universities, European level organizations), Coordination between the central and local production following Bologna; Semi-formal processes for communication and coordination between universities, translating and mapping the academic records according to Bologna rules etc. (many issues are solved case-based, between local program coordinators, by e-mail) Resources Internal production and co-production based on shared resources (knowledge, technology, infrastructure, services): Infrastructure and solutions supporting coordination within and between levels (our experience shows very weak automation so far) ITG capability Organizational IT (i.e. the host- and home university student management systems) plays the role of a mediator (service bus), coordinating and controlling the inter-organizational processes between partners. It also links the (partner) universities with the students 48 J. Zdravkovic and I. Rychkova 5 Discussion, Conclusions and Future Work Values created by public organizations and their IT in particular, expand the boundaries of these organizations. Therefore, today’s public organizations need to be seen as a part of dynamic ecosystems, where they maintain the relationships of different nature with individuals and other organizations. To fulfill their goals, public organizations need to adapt to their context, exhibiting consequently various behavior in using IT. Next, the organization needs to examine its context: what public value it desires to provide, what its authorizing environment is (i.e., who will authorize and support the value provisioning, who will benefit from it), and, eventually, what kind of core organ‐ izational capabilities will be required. These elements are interrelated and provide an understanding of a context where the IT Governance will be enacted. We suggested that the model of IT Governance could become “more centralized” or “more decentralized” according to this context. We therefore considered IT Governance as an organizational capability and proposed to use IT Governance capability patterns for different organi‐ zational styles. An IT Governance capability pattern can be seen as a guideline on how to define IT governance to support desired public values driving different context situa‐ tions. We illustrated how the IT Governance patterns can be instantiated on the case of the student mobility in the Higher Education public sector of EU. Our motivating assumption has been that public organizations need to master a wide range of ITG mechanisms and to deploy them depending on their value-creation context. The model proposed in Fig. 2 provides an organizing logic that can help organiza‐ tions to position, justify and govern their IT projects in a consistent way, based on the public value concept. However, argue that the IT Governance styles are not mutually exclusive and that several styles can be used in the same value-creation context. We plan to elaborate the guidelines and recommendations further in the future, by conducting multiple empirical studies and collaborating with practitioners. IT Gover‐ nance capability patterns are intended to facilitate the application of IT Governance mechanisms for different governance styles. They provide a general idea; the concrete “recipe” has to be elaborated for each particular organization.",
  "2018-RR": "1 Introduction Business process models treat all participating actors as deterministic machines. In a given fork only very few options are available and they don't change over time. One of the problems with these models is that it doesn’t take into account the mindset of the participating actors. This forces people into very narrow decision paths. However, we know from experience that people tend to change their behavior, sometimes from one instance of business process execution to another. Adding a socio-technical perspective we attempt to explore more the mindset of the actors to understand the reasons and range of for some of the options they may consider and the decisions they may reach. In this paper we will briefly explain homeostasis and the appreciative system, and will show how they can be applied to business process modeling with statecharts. We use the example of the recruitment process of a doctoral student (PhD) by a university professor. This example is based on some personal experience with this kind of process. This is a preliminary work, at the stage of an idea. With this work, we do not intend to demonstrate the practical utility of appreciative system models. Instead, we would like to show that it is feasible to model an appreciative system with statecharts, to link it to a business process model, and therefore to enlarge the technical scope of business process modeling with a social, mindset perspective. Edited by S. Kowalski, P. Bednar and I. Bider 87 Proceedings of STPIS'18 2 In Section 2 we present Cannon’s framework of homeostasis and explain how it affects the decision making through Vickers’s Appreciative System. In Section 3 we introduce the example of the PhD recruitment process with a statechart model. We extend this model with an Appreciative System statechart in Section 4. We discuss our model and show that new scenarios emerge in Section 5. We propose our conclusions in Section 6. 2 Homeostasis and Appreciative Systems A business process is often associated with achievement of some or several well- defined goals. This can be seen as the direct implementation of Cybernetics as defined by Rosenblueth, Wiener and Bigelow [9]. Rosenblueth was a collaborator of Cannon [4] who, in the 1920s, coined the term Homeostasis in order to explain how an animal body maintains the steady states that are the basis of its survival [8, 1]. Cannon explained that living organisms somehow found a way to maintain steady states even though they are made of unstable internal elements and live in an unstable external environment. It is the maintenance of these more or less stable internal states that maintain a living being’s identity and therefore its survival. In Cannon’s work, as explained by [13], there is no goal to be achieved, just the maintenance of steady states. Rosenblueth, Wiener and Bigelow simplified Cannon’s work and defined teleological, purposeful, behavior as achieving a well-defined final state through the use of a negative feedback mechanism [4]. However, whereas the early work in Cybernetics involved the study of man-made systems, it was very quickly applied to socio-technical systems. Business process management is one such example. The goals that are to be achieved by business processes are the modern-day descendants of this early teleological work. Writing from a social process perspective, Vickers [11, 12] took the work created in Cybernetics and re-expanded it with maintenance in mind, but this time writing about the maintenance of relationships instead of states. The maintenance of relationship is in fact the maintenance of relationship in a given state. Vickers wrote about attaining, maintaining and eluding relationships [12]. Maintaining and eluding a relationship can both be seen as keeping it in a specific state, either close or distant. In Vickers’s work these relationships are maintained with respect to norms, states that remain more or less the same over time, just like Homeostasis is the maintenance of more or less stable states. For Vickers, norm-holding was very different from goal- seeking (goal achievement in today’s parlance). Goals are to be achieved once and for all, and determine a well-defined end point, whereas norm holding defines an on- going activity of matching the current state of affairs with the relevant desired state [12]. Norm holding has no beginning and end other than the survival of the subject. Just like homeostasis, where, as long as the animal is alive it strives to maintain its identity by maintaining steady states. This results in what Vickers called the appreciative setting and we may also refer to as the homeostatic state. ©Copyright held by the author(s) 88 Proceedings of STPIS'18 3 In the example of a university professor wishing to recruit a PhD student, norm holding requires to understand the maintenance of scientific production by a team of researchers rather than the outcome (or goal) of writing a scientific paper or recruiting a PhD student. Vickers’s appreciative system contains 3 distinct, but interrelated elements [7]: Reality Judgments (RJ), Value Judgments (VJ) and Action Judgments (AJ). Reality judgments correspond to what people perceive of their situation. Value judgments correspond to how they compare these reality judgments to relevant norms. Action judgments correspond to the relevant behavior that will be selected. The repeated exercise of the appreciative system leads to what Vickers calls readiness in each of its components: Readiness to see (RJ), Readiness to value (VJ) and Readiness to act (AJ). At any given moment this readiness defines a specific state of the appreciative system, which Vickers calls the appreciative setting [11]. Vickers further detailed the stages of the Value Judgment component as [7]: Attaching a Reality Judgment to an existing category and thereby defining the relevant norm (Matching). Evaluating the Reality Judgment (the state of affairs) on present and future relations with the help of the norm (Weighing). Creating a new category for future exercises of the appreciative system (Innovating). The appreciative system model can be seen as a cognitive model of a process participant that explains her decision-making (i.e., decisions to act, not to act, to cancel or alter the already started course of activities) behind the PhD recruitment within the process. For example, the university professor may have reality judgments about the number of PhD student she has and the resources she has available to support them. She may have value judgments that compare these states with relevant norms, and will decide to recruit another PhD student if the comparison shows that she needs one more PhD student and can support them. In our model, explained below, we present a statechart version of the appreciative system. This model is quite simplified and approximate. For example, the relevant norm is selected in the Reality Judgment and we don’t have an innovation stage to create new categories. Over the years several simplified models of Vickers appreciative system have been proposed, most notably by [2, 3 and 14]. 3 Example: the PhD recruitment process 3.1 PhD recruitment In this work we use an example of a PhD recruitment process of a University. This process involves three actors: a faculty member (FM) – a professor leading a research group; a graduate student (GS) searching for a PhD position; a doctoral school (DS) that distributes funding for PhD and manages the PhD candidates until their graduation. DS releases calls for PhD proposals, receives and revises submissions from FMs and attributes a number of grants. Edited by S. Kowalski, P. Bednar and I. Bider 89 Proceedings of STPIS'18 4 GS applies for a PhD by sending an application to a FM. The FM can approve or reject the GS based on the application assessment. If the FM approves a candidate and if the DS accepts the PhD proposal, the GS gets recruited for the PhD. FM prepares and submits a PhD proposal, which includes a presentation of a research subject and a file of a PhD candidate (optional). The FM also has to be eligible for supervising a PhD (e.g., must have a habilitation, should not exceed some quota of PhD students in the group etc.). If the PhD proposal is accepted by the DS, the recruitment process starts: if the FM already has an approved candidate, this candidate becomes formally accepted by the doctoral school and pursues her PhD. Otherwise, the FM searches for a candidate. If the PhD proposal is rejected, the FM can revise the subject and/or resubmit the proposal for the next call. 3.2 Statecharts and YAKINDU SCT. Statecharts. We model the PhD recruitment process using statecharts defined by Harel [5]. Compared to a workflow paradigm widely used for process modeling, we specify the process with a set of states and transitions between states. This allows us to omit specification of concrete activities associated with recruitment but to focus on the process goals and milestones. This state-transition paradigm also applies for representing homeostasis. Here Reality judgment, Value judgment and Action judgment can be modeled as states. Adjusting the norms, acting, not acting or stopping some action are of the main interest can be captured with states transitions. Thus, statecharts provide a uniform modeling notation to reason about the (business) process as specified between the process participants and about the cognitive process associated with decision-making of each participant. Statecharts formalism. The statecharts formalism specifies a hierarchical state machine (HSM) that extends classical finite state machine (FSM) [5] by providing: (i) Hierarchy (or depth) - the possibility to model states at multiple hierarchical levels, with the notion of abstraction/refinement between levels; (ii) Orthogonality - the possibility to model concurrent or independent submachines within one state machine; (iii) Broadcast communication - the possibility to synchronize multiple concurrent submachines via events. As for FSM, states and transitions are central concepts in statecharts. State specifies a state of the modeled system. A state can have a behavior that describes which actions are taken under which conditions. Transitions between states are triggered by events. If a transition t takes place only if an event e occurs – e is called a triggering event of the transition t. Triggering events can be complemented with a guard conditions: e[c]. In this case, we say that a transition t takes place when e occurs and c holds. During an execution of a state machine, a state can be active or passive. Due to a hierarchical structure, where several (sub)states can be embedded in a (parent) state, a multiple states of a statecharts can be activated at a given moment of execution. These states are called an active configuration of the state machine. Compared to a FSM ©Copyright held by the author(s) 90 Proceedings of STPIS'18 5 transition, a statechart transition can be seen as a change from one active configuration to another. Yakindu. We use YAKINDU Statechart Tools (YAKINDU SCT) [15] for modeling, simulation and analysis of the process. YAKINDU is a modular toolkit for developing, simulating, and generating executable finite-state machines (FSM). YAKINDU statecharts are organized in regions. Hence it is possible to organize multiple state machines in different regions and to run them concurrently. Regions contain states and transitions. A state in turn can contain one or more regions, turning it into a composite state (contains one region) or an orthogonal state (contains two or more regions) [15]. A state can define behavior in the form of one or several trigger [guard] / effect statements. These statements provide a declarative (non-prescriptive) specification of behavior as no execution order is applied. Triggers are events that can be complemented with a guard condition (guard). The effect will only be executed if the trigger occurs and if the guard condition holds. The effect can include one or several actions such as assigning a value to a variable, raising an event, calling a function. Example (see Fig.2): DS.sendCall /N1=1; raise vj - when the DS.sendCall event occurs, the N1 variable is assigned to 1 and the vj event is raised. We simulate the statechart models with Simulation view in YAKINDU STC. The framework allows us to manually raise events, to inspect and modify variables of a running simulation and to observe the model’s behavior as a sequence of active configurations triggered. We use this simulation tool to play and discover different process scenario. More advanced features for model simulation are available in YAKINDU. They are out of the scope for this paper. 3.3 Modeling PhD recruitment with Statecharts. Fig. 1 presents a statechart model of the PhD recruitment process developed with YAKINDU. The diagram consists of three regions – FM, DS, GS – representing the process actors. Each region contains a statechart describing a behavior of the corresponding actor. The GS region describes a behavior of a graduate student who applies for a PhD position. The statechart is specified with four states: PhDApplication (the default starting state), Rejected, Approved and Accepted states. State transitions (depicted by the arrows between states) are triggered when their corresponding triggering conditions (trigger [guard] expressions attached to the arrows) are satisfied. For à example, a transition PhDApplication Rejected is triggered when the rejectCandidate event is raised (no guard is specified). The DS and the FM regions describe behavior of a doctoral school and a faculty member respectively. Whereas most of the states in our model are atomic (i.e., have no substates), the Preparation and the Recruitment states of the FM region are composite states. The Preparation state is an orthogonal state with two concurrent Edited by S. Kowalski, P. Bednar and I. Bider 91 Proceedings of STPIS'18 6 regions called Subject and Candidate. These regions specify the subject and the candidate selection as two independent (concurrent) behaviors for a faculty member. The DS, FM, GS statecharts are running concurrently: each statechart reacts on the events raised by the others (internal events) or by the environment (external events). In Table 1 we present a list of events defined for the PhD recruitment process. Table 1. Events for PhD Recruitment process. Event Description GS.apply An application from a graduate student is received DS.sendCall A call for PhD proposals is sent by a doctoral school defSubj (revieSubj) A faculty member (FM) defined (revised) a PhD subject approveCandidate A PhD candidate is approved (rejected) by FM (rejectCandidate) submit (resubmit) A PhD proposal is submitted (resubmitted) by FM stop FM stopped the submission accept (reject) DS accepted (rejected) the proposal notifyAccept (notifyReject) The notification from DS is received Fig. 1 The statecharts model of the PhD recruitment ©Copyright held by the author(s) 92 Proceedings of STPIS'18 7 4 Appreciation for the PhD recruitment We extend the statechart model of the PhD recruitment in Fig .1 with a model that illustrates the appreciative system (or internal regulative process) of one of the process participants – a faculty member (FM). This model explains the FM’s decisions within the process (i.e., decisions to act, not to act, to cancel or alter the already started course of activities). The appreciative system is modeled with a supplementary region in the statecharts diagram called FM_. The complete statecharts diagram now consists of four regions that we will further address as the statechart model of the PhD recruitment (Fig.1) and the statechart model of the FM Appreciative system (Fig.2). The statechart model of appreciative system consists of one composed state called Maintenance. The Maintenance state represents the appreciative setting (the homeostatic state) of the actor and can be associated with the set of norms N the actor seeks to maintain. This state can be affected by interaction with the environment. The Maintenance state contains three substates: reality judgment (RJ), value judgment (VJ) and action judgment (AJ). These states are connected by transitions reflecting the appreciative system model. The objective of this regulative process (also called appreciation) is to maintain the appreciative setting (the homeostatic state) of the actor. Each state has a behavior specified with a list of trigger[guard]/effect statements. We codify the norms with integer variables N1, N2, .... and use them in the guard and effect parts as explained below. Fig.2 Appreciative system model for the Faculty Member (FM) Edited by S. Kowalski, P. Bednar and I. Bider 93 Proceedings of STPIS'18 8 4.1 Statecharts Model of Appreciation. We model the stable state (or the identity) of a faculty member with the following norms: N1 - ensure scientific production; N2 - ensure the team cohesion (size and skills); N3 - ensure originality and relevance of the researched topic; N4 - ensure mentoring and management of the team. Interactions between FM, DS and GS as described in the PhD recruitment statechart model may affect the identity of FM. In the RJ state (Fig.2), FM interacts with her environment and makes observations. Once an event relevant to one or several norms occurs – the corresponding norm variable N1..N4 is assigned to 1 and a transition to value judgment state (VJ) in the FM_ statechart is triggered. This is expressed with the behavior statements in the RJ state in Fig. 2. Example: GS.apply / [N2=1]; raise e - this statement describes a behavior triggered when FM receives an application from a graduate student (GS.apply); the observed event is relevant to the norm N2 (ensuring a team cohesion) as a new member can eventually join the team. Thus N2 is set to 1 and the transition to VJ is triggered (N2=1; raise e) as an effect of this behavior. In Table 2, we provide a list of events that can be observed in RJ. In the VJ state, the current situation is evaluated as being a threat or an opportunity (isThreat and isOp variables in the statecharts model) for the actor’s stable state. A combination of isOp / isThreat is used further in the AJ state for decision-making. In Table 3, we provide a list of assessments, their outcomes and their perceived impact on the stable state. Example: FM_Cand.isGood / isOp = 1; raise aj – this statement describes a behavior of the faculty member who examines the application and believes that the candidate is good (FM_Cand.isGood); in our model, this is perceived as an opportunity and is followed by transition to AJ (isOp = 1; raise aj). In the AJ state, we model the visible decision-making logic of FM: the faculty member can decide on acting, doing nothing (this triggers a transition to RJ), or re- assessing the situation (this triggers a transition to VJ). Table 2. Reality Judgments (Observed (relevant) events) Event Relevant to Description a norm: GS.apply N2 An application from a graduate student received An example of career change that may impact a FM.getPromoted N4 capacity to recruit PhD students DS.sendCall N1 A call for PhD proposals received A PhD proposal was accepted for funding by the notifyAccept N1 Doctoral School notifyRegect N1 A PhD proposal was rejected by the Doctoral School ©Copyright held by the author(s) 94 Proceedings of STPIS'18 9 Table 3. Value Judgments (Situation assessment) Assessment Outcome Is Opportunity Is Threat FM_Cand.isGood 1 - Qualification of a candidate FM_Cand.isBad 0 - Professional situation and its FM_Status.isNok - 1 impact on the role of FM_Status.isNok [when N1] 0 - research group leader FM_Subj.isGood [when N1] 1 - Quality of the current research subject FM_Subj.isBad 0 - FM_PhD.isRequired - 1 Size and efficiency of the group FM_PhD.isNotRequired - 0 AJàVJ In our model, transition can be triggered whenever changeVj event is raised. This is used during simulation in order to mimic a human process of decision making, which is the essence of appreciation: indeed, an individual may undertake a VJàAJàVJà.. thorough reflection cycle , with various assessments concurrently raising and dropping isOp and isThreat flags, until the moment she is satisfied with her value judgment. And conversely, an individual can make a hasty conclusion, VJàAJàRJ. making a judgment based on one parameter only and passing to action: Once FM terminates the reflection cycle and makes some decision (modeled with event d), one of the trigger [guard] / effect statements specified in the AJ state will be executed. All these statements have the same trigger (event d). The guard is specified with a combination of N, isThreat, isOp values. The effect consists of raising a set of events that will trigger a transition from AJ to RJ (or to VJ) in the FM Appreciative system statechart and will be captured and processed in the staechart model of the PhD recruitment. In Table 4, we provide details on decision-making logic implemented in the current model. Note that both the situation assessment (Table 3) and the decision-making logic (Table 4) are validated by the authors, based on their experience as FMs. A detailed empirical study is a part of our future work. Example 1: d[isOp ==1 && isThreat ==1] /raise changeVj - this statement specifies a decision (d) taken under condition that both opportunity and threat are perceived; the behavior will have the following effect: switch to value judgment and reassess the situation. Example 2: d[N1==1 && isOp ==1 && isThreat == 0] /N1 =0;raise defSubj; raise reviewSubj; raise submit; raise resubmit; isOp = 0; raise act This statement specifies a decision (d) taken under the following conditions: an opportunity is perceived for scientific production (N1==1, isOp==1) and no threat is perceived (isThreat==0); this behavior will have (some of) the following effects based on the current active configuration of the PhD recruitment process: definition (or revision) of the subject, submission (or resubmission) of the proposal; resetting the variables (N1=0; isOp = 0) and triggering a transition to RJ (raise act). Edited by S. Kowalski, P. Bednar and I. Bider 95 Proceedings of STPIS'18 10 Table 4. Decision making logic for the FM. Norm Is Opportunity Is Threat Effect: all 1 1 Change value judgment 0 0 Do nothing OR: Change value judgment Act*: N1 1 0 defSubj; reviewSubj; submit; resubmit; Act: defSubj; reviewSubj; submit; resubmit; OR: 0 1 Change value judgment N2 1 0 Act: approveCandidate; defSubj 0 1 Act: reject candidate N3 1 0 Act: defineSubj, reviewSubj; 0 1 Act: reviewSubj; OR: Change value judgment N4 1 0 Act: defSubj; reviewSubj; submit; resubmit; 0 1 Act: Stop, cancel process; OR: Change value judgment 5 Model Analysis and Identification of New Scenarios The statechart model of the PhD recruitment in Fig.1 can be considered as an external view on the PhD recruitment process. In this model we assume that the intentions of the process participants (FM, DS, GS) are explicit: they can be studied, formalized, transformed into requirements and traced in the process model with traditional RE approaches. This kind of model has no provision for expressing the process participants’ appreciation of their situation, appreciation, which is most often tacit and therefore hidden from view. Even though it is hidden, this appreciation determines some of the decisions that the participants may make during the business process and therefore affect its outcome. We propose to extend the “external” process model with an “internal”, cognitive, model, which sheds the light on this appreciation. Following Vickers, we replace the idea that process participants make their decisions within a business process in order to achieve some goal with the idea of them making decision in order to maintain the desired and elude the undesired relationships. This results in what Vickers called the appreciative setting and we may also refer to as the homeostatic state. The statechart model of appreciation provides an insight about the process participants’ intentions and behavior. As a result, new possible process scenarios can be discovered. In our work, the statechart model in Fig. 2 allows us to study how the FM’s appreciation impacts the decisions of a faculty member about recruiting a new PhD candidate. * All the events will be raised; based on the active configuration of the PhD recruitment statechart, some events will be processed and some ignored. ©Copyright held by the author(s) 96 Proceedings of STPIS'18 11 5.1 Design While modeling appreciation, we made the following observations. The appreciative system modeled in Fig.2 is independent from the PhD recruitment process modeled in Fig. 1. This means that a new iteration of the appreciation process can be triggered any time during the PhD recruitment process (e.g., when the FM receives a promotion, a new application). Each regulation cycle in the appreciative statechart can potentially affect the ongoing PhD recruitment process. In the appreciative model the perception of a threat / opportunity is subjective. This means that the outcome of the value judgment (VJ) for the same observed event can vary from one concrete person (FM) to another and from one occurrence of the process to another, depending the capacity of this person to see (or not to see) the bright side of things and/or the tendency to think carefully or to take risks. In our statechart model, this is implemented with: a) a declarative specification of behavior while assessing the situation (in VJ) and b) with a possibility to simulate a reflection (VJàAJàVJà..) cycle of any desired length. Thus the actor can use one or several assessments in arbitrary order and independently from the observation that triggered this assessment (Table 2). For example, receiving an application, the FM can decide to recruit the candidate or not based on the application evaluation only. Alternatively, the FM can (re)assess her academic status, motivation and a prospective research subject. 5.2 Simulation We propose the following use of the complete model: we run the regular scenarios of the PhD recruitment process (as expected by the original, formal, process specification) and we add some “noise” by triggering additional events. These events are mimicking the environment or the social system with which the FM is interacting. The examples of these “noise” events can be: career opportunities, health issues, conflicts within the group, new ideas and inspirations etc., (we include only two of them in the current model.) We show that the socio-technical system where the PhD recruitment takes place is not closed and that the state of mind of the process participants (the FM in our case) can be affected by these events that are not directly related to the process. Using the appreciation model, we show that the results of the FM interacting with her environment can change her perception of the situation and can make the FM act, not act or stop acting in response. The FM can decide to cancel the hiring or to resubmit the proposal at any point in case some norm is threatened (e.g. she judges that the subject is not challenging, the candidate is not qualified or her personal or career situation is threatened). If these aspects are not modeled, the affects on the formal process scenario are seen as “random” or “bad” decisions. These are often referred to as “human error” in more safety-critical or strategic process. We show that these kinds of decisions are not random but driven by the process participants appreciation of their situation, which indeed can be seen and studied as a “human factor”. Edited by S. Kowalski, P. Bednar and I. Bider 97 Proceedings of STPIS'18 12 5.3 Applications and some Why’s Appreciative system modeling can be considered as a process related to the design of organizations. Here we propose to model an organization as an ecosystem of individuals seeking to maintain their homeostatic states through various interactions (or despite of them). The statechart models we propose allow one to learn about the organization, its stakeholders, their explicit and implicit norms and to discover the ground for potential collaborations and conflicts. In this work, we model appreciative systems with statecharts as to help with the process design phase, in parallel with standard RE activities. We do not claim that considering appreciative settings can improve the existing workflow or business process management systems, though we think that it can explain why these systems might not work. Why state-orientation? State-orientation can help in order to explain, predict or analyze decisions that participants must, should or could take during the process but not how they will implement these decisions through activities. We use this paradigm for expressing both a cognitive model of decision-making and a process model. Why the appreciative system model? Vickers’s appreciative system explains human and organizational behavior as norm-holding instead of the prevalent goal- achievement. It focuses the modeler’s attention on the relationships maintained by the participating actors instead of the outcomes they want to achieve. This different perspective can help business process management system designers to \"see\" more scenarios, to predefine more activities and to specify more possible outcomes. For existing business process or workflow management systems, where all activities are predefined, analysis of the appreciative system model can shed some light on situations where the process fails to follow its predefined scenarios and to reach its predefined outcome. 1 and Other models, such as the Case Model Management and Notation (CMMN) (BMM)2 by OMG can be considered as related work. Business Motivation Model These models and notations originate from the enterprise domain, are more specific and thus can be seen as more “practitioners-friendly”. Both CMMN and BMM are outcome oriented, just like BPMN and other process modeling notations. The norm holding paradigm of Vickers’s appreciative system provides a substantially different way of looking at the business process design. Why Statechart? Statechart is a generic notation to reason about state-transition systems. It is well suited for modeling the appreciative system because this model, as defined by Vickers, is inherently state oriented. On the other hand, it is well suited for modeling processes (loosely structured and context-driven processes in particular [6, 10]). Therefore, in our approach we use statechart as a common, domain-independent formalism for extended process modeling. 1 OMG: Case Model Management and Notation, version 1.1, 2016 2 OMG: Business Motivation Model, version 1.3, 2015 ©Copyright held by the author(s) 98 Proceedings of STPIS'18 13 Why Yakindu? YAKINDU SCT provides useful features for system modeling and simulation; its efficiency was shown on the examples of various systems. We believe that this tool can be also useful for modeling and simulating of the appreciative systems. 6 Conclusions In this paper we propose to extend a traditional (goal-oriented) process model with a model of appreciative system of a process participant. Using the appreciative system as a cognitive model for decision-making in interactive processes has triple interest: 1) For process engineers: we take into account not only a process and its context but a social system formed by the process participants and their (personal and professional) contexts. This gives an opportunity to consider more complex interactive scenarios and to anticipate the sources of actions with undesirable (and for some processes, catastrophic) consequences. Safety critical processes, activities associated with risk taking, can benefit from this socio-technical approach for process modeling and analysis; 2) For process participants: the model of appreciation provides an opportunity to think about norms, values, beliefs, reflection cycles and decision making routines (as we experienced while filling in the tables 3-4 for example). Identifying these elements, the process participants can better cope with the stress encountered during their activities, better understand the source and the effects of this stress and can possibly put in place some context and person-specific strategies for stress management. 3) For process managers: understanding of (or at least being aware of) the appreciation of process participants can help the process managers to anticipate and understand the source of conflicting situations during the execution of the interactive processes. The process manager can be better equipped to propose a conflict resolution strategy. Such a strategy may involve initiating a new appreciative cycle in the maintenance state, where participants will be able to change values (VJ) and to find a common vision of the situation. All three cases discussed above lead to discovering new process scenarios based on interactions of the process participants with each other and with their environments. In practice this will be reflected by adding new transitions and states into the formal process model. In our example, we add new transitions corresponding to scenarios where the FM cancels or decides to resubmit the PhD proposal to the statechart model. Considering an information system supporting the PhD recruitment, the new interfaces supporting cancelation and compensation activities has to be integrated into this system. In this work, we consider an appreciative system for one process participant only – the faculty member. Modeling the appreciative systems for all participants would provide us with better understanding of the complex interactions involving trust, Edited by S. Kowalski, P. Bednar and I. Bider 99 Proceedings of STPIS'18 14 conflict emergence and resolution, cooperation, concurrency, etc. This is the subject of our future work.",
  "2019-BRLG": "I. INTRODUCTION\nThe term process comes from Latin and refers to a performed action or something that has been done to achieve\na specific result [1]. No surprise that since Adam Smith\nand Frederick Taylor the de facto point of view on business\nprocesses was activity-centered, where business process are\nrepresented with a set of interrelated and ordered activities or\ntasks.\nToday, in the era of Data, a new point of view on the\nbusiness processes has emerged. For many processes, success\nis not anymore in their predictability and efficiency, but in their\ncapacity to adapt for the situation and to treat each customer’s\nrequest as unique. Such processes require creativity and collaboration. Their execution depends on the data collected for\nthe current process instance or case and on the knowledge of\nan expert handling the case, rather than on the a` priori model.\nThese are knowledge-intensive processes (KIP).\nWe execute KIP to achieve a specific result, however the\nway we achieve it is not any more a universal sequence\nof actions that can be designed, validated and tested before\nexecution. Thus, from the traditional, activity-centered point\nof view, KIP are challenging to automate, to control their\nexecution and improve them.\nVarious communities have grown with the objective to better\nstudy KIP and to share practices. ACM (adaptive case management) [2] is the community of practitioners promoting the\nmethods and tools for adaptive case management as opposed\nto business process management. The WfMC Global Awards\n(Workflow Management Coalition) for Case Management recognizes and focuses upon successful use cases within Adaptive\nCase Management. CMMN (Case Management Modeling and\nNotation) [3][4] is the OMG case modeling standard. It also\ngathers a community of companies around the representation and support of cases, including IBM, Oracle, TIBCO,\nSAP and others. AdaptiveCM (International Workshop on\nAdaptive Case Management) and DEC2H (DECision and\nHybrid approaches to processes) are annual workshops where\nresearchers and practitioners publish and discuss about KIP\nand other non-workflow approaches for process management.\nProblems around automation, collaboration, flexibility, compliance, and structure have been raised. Other communities work\non new approaches, like Speech-Act [5] or AI4BPM (Artificial\nIntelligence for Business Process Management).\nBusiness Process Management (BPM) discipline emerged\nin order to better manage and optimize processes [6]. BPM\nexplores an activity-centered view on the processes, which\nis based on the assumption that any business process can\nbe modeled with a (possibly large) sequence(s) of activities.\nProcess activities according to BPM are interrelated, ordered in\ntime, and known a` priori. The process model can be tested and\nvalidated; the resulting process will then follow (instantiate)\nthis model at each execution. This activity-centered view\nprovides control over processes and ensures compliance “by\ndesign”. The importance and complexity of KIP were exposed\nby the progress of BPM within companies: KIP scenarios can\nbe hardly predefined and compliance for KIP can be ensured\nonly at “run-time”. This makes application of traditional BPM\npractices for KIP difficult if at all possible and limits the\nautomation of KIP.\nStrategy models, deciding important tasks to plan and which\nkey processes should be optimized, rely on organization’s\ngoals [7]. In goals modeling, businesses’ stakeholders are\ninterested in the who, what, and why questions [8]. These\nactivities share common practices and challenges with KIP:\ngoals are the most important concepts to keep in mind;\ninformation processing, collaboration, and decision making are\nregular steps; assure compliance or constraint while executing\na process is important.\nIn this work, we study and analyze the challenges and\npractices in KIP management reported in the research literature. The objective is to highlight recurrent topics in the KIP\nliterature, preferably on recent works, and characteristics that\nare still few or unsupported.\nIn spite the reported advancements, KIP management remains challenging and new research questions are yet to be\nsolved. We outline some perspectives in KIP research. In\nparticular, we discuss an approach to structural and semantic\nanalysis of KIP scenarios. This approach aims to provide\nthe compliance support for KIP by suggesting process fragments fitting the execution context. The discussed solution\ncontributes to the third group of solutions outlined above\n(execution support). It is based on classification techniques\nused in data science (for structural analysis), on ontologies\n(for semantic), and on association rules (for behavior).\nThe paper is organized as follows. Section II presents\nthe BPM context and defines KIP. Section III presents and\ndiscusses the three groups of solutions we identify in the\nliterature: BPM extensions, theoretical foundations for KIP,\nexecution support solutions for KIP. Section IV outlines the\nperspectives in KIP research. Section V presents our conclusions.\nII. BUSINESS PROCESS MANAGEMENT, KNOWLEDGE\nINTENSIVE PROCESSES, AND CASES\nA. Business Process Management\nThe literature defines a business process as a set of interrelated activities that are performed in coordination in an\norganizational and technical environment, in order to achieve\na business goal [9][10].\nBusiness process management is a discipline that includes\nconcepts, methods and techniques to support the design,\nadministration, configuration, enactment, and analysis of business process [10]. BPM seeks to manage and optimize business\nprocesses in order to deliver the best outputs (product, service,\n...) [6].\nBPM recognises the 6 phases of the process lifecycle [6]:\nprocess identification, process discovery/process modeling,\nprocess analysis, process redesign/process improvement, process implementation, process monitoring and controlling.\nBPM uses the activity-centered point of view on the processes, where activities define the states of process entities.\nBPMN [11] (Business Process Model and Notation) and\nEPC [12] (Event-driven Process Chain) are examples of modeling languages, which are widely used by academics and\npractitioners. In these modeling languages, the activities and\nevents are defined and explicitly ordered forming workflows\nthat will be executed.\nOther modeling languages, less restrictive, also exist under\nthe declarative paradigm. These languages propose constraints\nas basis for modeling [13].\nTo support management and optimization of business processes, multiple IT solutions were developed. Process Aware\nInformation Systems (PAIS) are systems that “support operational business processes by combining advances in in-\nformation technology with recent insights from management\nscience” [13]. Workflow Management Systems (WfMS), an\nexample of a PAIS, are systems for process modeling and/or\nautomation. A process represented by a workflow model is the\ncore of a WfMS [6]. Business Process Management Systems\n(BPMS) are the successors of WfMS. They support different\nfeatures for process monitoring and use business intelligence\nfor process improvement [6].\nB. Knowledge Intensive Processes\nProcess automation and optimization with PAIS, exposed a\nspecific type of processes, difficult to manage within the established BPM paradigm. From industrial processes to current\nservice processes, automation encountered the “knowledgeintensive economy” [14]. Since a few decades, knowledge is\nconsidered as one of the most important competitive advantage for companies [15][16][17]. The knowledge management\n(KM) discipline studies “how knowledge is created, developed,\nretained and applied in the workplace and how it enables\norganizational learning and innovation” [18].\nIn the context of business processes, some specific human\nactivities couldn’t easily be modeled with regular process\nmodeling languages, and therefore, were left as dedicated\nactivities or sub-processes [19].\nProcesses where knowledge is a key characteristics are\ncalled knowledge intensive processes (KIP). Compared to\nmanual tasks defined in BPMN 2.0 [11], where a process\nparticipant accomplishes a manual task, knowledge-intensive\nactivities or tasks are characterized by their intensity in\nknowledge and by the requirements to use and transform this\nknowledge within processes. KIP emphasizes the importance\nof the experience or tacit knowledge of process participants\nalso called knowledge workers. The role of knowledge workers\nis to accomplish specific activities that are characterized by\ncomplexity or require creativity.\nKM and KIP open a fairly new research field where many\ndefinitions and characteristics co-exist. The following characteristics of KIP are defined in [14]:\nKIP embed a lot of knowledge: the tasks are complex;\n• the decision making requires a lot of information to be\nprocessed simultaneously.\nKIP require a lot of human collaboration to execute the\n• process.\nKIP have a high degree of uncertainty: the activities might\n• not follow the expected order, and activities that were not\ninitially planned might appear. This makes the structure\nof KIP very loose.\nKIP require creativity: original ideas and improvisation.\n•\nTraditional business processes may have some of these\ncharacteristics too. However, in the case of KIP, all of them\nare typically present, making a process very difficult to model\nand manage within the BPM paradigm.\nConcerning uncertainty, a distinction must be done between\nthe ordering of activities, and the unpredictable activities.\nUnpredictable activities depends on the context of execution and require improvisation based on tacit knowledge.\nFor example, in some unique situations, it is expected that\nknowledge workers improvise or try new actions based on\ntheir assumptions (tacit knowledge).\nSeveral terms are used for KIP in the literature: knowledge intensive business process (KIBP) [20][14] or artful\nprocess [21][22], which highlights the fact that an art is\nrequired to execute them, and it would be extremely difficult\nto transpose it within an IT system. In this article, we will use\nthe term KIP to denote all of those types of processes.\nC. Adaptive Case Management\nAs the KIP field grows and exposes limitations of BPM\nsupport, some attempts to support KIP within IS evolved\naround case paradigm [23]. A community of practitioners,\nknown as ACM, worked specifically on this point of view [2]\nand in what extent cases can handle KIP challenges [24][25].\nFollowing [26], Case Management Process (CMP) are\n“driven by emergent knowledge about the case subject or\nthe environment; largely based on human expertise; highly\nunpredictable; difficult to replicate; hard to analyze and improve as no HOWTOs available” [26]. Cases differs slightly\nfrom KIP because of the point of view adopted. Within the\ncommon points, we can find the unpredictability and nonrepeatability (KIP allows each process instance to differs,\nexactly like each case is unique) from which we can deduce the\nlow or nonexistent structure, and also the knowledge-intensive\nproperty where knowledge workers (KIP) or case managers\n(ACM) must deal with a lot of information and experience\nto take a decision. A mapping between KIP characteristics\nfrom [27] and multiple definitions of case management has\nbeen made in [25]. Major distinctions come from the goaloriented property (each case/instance must have one or more\ngoals that evolve with time) and data-centered property (cases\nare updated with information given by each service or human\nconsulted). [28] also adds boundaries (context and scope of\nthe case) and event-driven (activities are executed following\nevents).\nBeyond ACM, multiple case-oriented applications were\nand are investigated. As mentioned in [25], case handling\ninitialized multiple approaches with the case idea: “Case Management, Adaptive Case Management, Dynamic Case Management, Production Case Management, and Emerging Case\nManagement”. As another example, Case-Based Reasoning\n(CBR) [29] also tries to “... solve(s) problems by relating some\nsimilar previously solved problems (past experiences) stored\nin a case base ...” [29] by using artificial intelligence. CBR\nis using a problem description to propose solutions.\nIII. MOTIVATION, PROBLEMS, AND SOLUTIONS FOR\nKNOWLEDGE INTENSIVE PROCESSES\nWe gather papers in three main steps. First, we search for\nthe terms “weakly structured” “knowledge intensive process”\nmodels qualities in google scholar and relationships “weakly\nstructured” “knowledge models” taxonomy. The extracted\narticles are kept if they are focusing on KIP or KM in their\ntitle and abstract. Second, we searched within AdaptiveCM\nproceedings of 2012, 2015, 2016, and 2018 in order to extract\nACM works proposing practical solutions. The case point of\nview is preferred because some results in the first step are\ndirectly coming from the ACM community or are adopting its\npoint of view, and we chose this point of view for the proposed\nsolution. Finally, extra works from previous researches about\nmodeling languages for KIP are added. Some KIP papers may\nbe missing as the goal of the research evolves from works\ntalking about processes’ without structure or weakly one, to\nKIP and their challenges. We organize these papers in three\ngroups:\nThe first group includes the solutions grounded on BPM\npractices. These works extend BPM in order to support KIP\nand to provide an appropriate level of flexibility and user\nsupport for KIP management.\nIn the second group, we discuss the works that exploit\nnew paradigms for KIP management, study characteristics and\npropose theoretical foundations for KIP. In these works, KIP\nmanagement is considered as a discipline that is an alternative\nto (and not an extension of) BPM.\nThe third group of works reflects the pragmatic views on\nKIP. Here we discuss the works focusing on the practitioners’\nneeds and presenting concrete solutions for KIP execution\nsupport. This group is specifically oriented on already implemented solutions (as opposed to theories and characteristics\nfrom the second group) which are not derived from BPM (as\nopposed to the first group).\nA. Extension of BPM for KIP\nKIP or processes that strongly rely on human decisions, collaboration or knowledge always existed in the organizations.\nBPM develop a theory and practices to help organizations in\noptimizing their processes [2][6][14]. The rapid expansion of\nBPM and process automation, proliferation of BPMN as a\nde-facto standard for process modeling in the beginning of\n2000s revealed that KIP can have only limited support from\nBPM. Van Elst et al. [30] study how KIP can be supported\nby process models and workflow. Authors emphasize that KIP\nare exploratory, non-repetitive in detail, and not completely\nknown a` priori. Authors conclude that by nature, KIP can’t\nbe supported by traditional BPM, yet some BPM practices\nmight improve quality and efficiency of knowledge work, and\neven improve the process modeling activity itself. Herrmann\net al. [31] also recognize flexibility in KIP as a crucial factor\nfor their efficiency. As traditional BPMS and WfMS require\npredefined structures to optimize business productivity, they\nare not suitable for KIP management.\nVarious KIP-supporting solutions based on BPM extensions\nand adaptations are proposed during the last decade:\nVan der Aalst et al. [13] propose a declarative specification\nlanguage for workflows. They use constraints to define the\nspace of process execution scenarios along with activities and\nevents. A declarative specification simplifies the model as less\nscenarios need to be modeled. It also leaves more flexibility\nto the process executor as some activities can be chosen at\nrun-time.\nBPMN 2.0 [11] replaces its predecessor in 2011 and defines\nseveral KIP-friendly concepts. One of them is an ad-hoc subprocess, which allows modeler to define a group of activities\nwithout sequences between them. Other features enable more\nflexible scenario execution: one can specify a sub-process\nwhere activities will be executed as many time as the process executor requires. New notation also allows for explicit\ndependencies between activities and data.\nRiss et al. [32] study how PAIS are deployed to increase\nproductivity at the cost of rigidity. They propose an approach\nthat keeps flexibility and control with a bottom-up process\ndevelopment and evolution. This approach uses knowledge\nworkers’ execution experience in order to make the company’s\nBPM evolving. However, they also state that this approach is\nbased on static models, reducing the ability to adapt to quick\nchanges happening in the real processes or in the regulations.\nShkundina et al. [33] study the challenges a user meets\nduring the knowledge-intensive process execution. The authors\nconclude that a process workflow provides no context for a\nconcrete task that needs to be executed. As a result, a user\nspends a lot of time searching for relevant information or\nguidance in order to make her decisions. The authors propose\nto enhance the workflow tasks with the information about their\ncontext. Some metrics for measuring similarity between tasks\nbased on their execution contexts are proposed.\nMoura et al. [34] state that computing infrastructure is\nnot adequate enough to support user activities within KIP.\nCollaboration support represents a particular challenge. They\npropose an approach and a tool that recommend during design\ntime and execute at run-time the best dedicated communication\nservices for the KIP.\nManfreda et al. [35] perform a case study to find which\nBPM methods can be reused in the case of KIP. Globally, the\nmain BPM phases are compatible with KIP, but, local adaptations are required to support innovation and collaboration\nbetween the great number of participants. They propose to\nintroduce methods dedicated to KIP into BPM phases, and\nworkshops for initial and modelling existing processes phases.\nFor analysis phase, qualitative methods are preferred. In their\nspecific case study, to-be models are not required.\nSallos et al. [36] linked BPM literature with knowledge\nintensive entrepreneurship (KIE) literature, in order to build a\nconceptual framework for business process improvement (BPI)\nin the context of KIE.\nGromoff et al. [37] outline multiple problems around KIP\nmanagement in a survey made on 98 russian companies.\nThe problems concerned the process transparency, missing\nlinks between KIP events from one instance to another one,\nfew knowledge-intensive control making reporting inefficient,\nabsence of knowledge-intensive change management reducing the predictability, and no system approach hindering the\nsupport from the other parts of the organizations. All of\nthese problems motivated the authors to support flexibility\nfor process management in knowledge-intensive perspective.\nHowever, authors do not support the idea that a better process\nformalization will lead to better execution. They propose to\nimprove management of KIP with a self-adaptive system. The\nsystem respects KIP’s requirements for flexibility within the\nideas of process optimization methods and BPMS.\nAnother addition to BPM is Business Rules Management\n(BRM) and business rules management systems (BRMS). Following [38], “rules are explicit constraints on behavior and/or\nprovide support to behavior”. Moreover, they are explicitly\ndeclarative instead of procedural, applied for business (and\nnot for IT), and must be kept within their own repositories in\norder to be “applied across processes and procedures” [38].\nOne objective of the rules is to separate the acceptable business\nactivities from unacceptable ones. BRMS are the repositories\nfor storing and integrating business rules within organizations\nin order to facilitate business processes [39].\nWhereas a significant effort is made to extend the existing\nBPM theory and practices in order to support KIP, other works\nconsider that KIP-supporting solutions do not necessarily have\nto root in BPM or its adaptation.\nB. Theoretical Foundations for KIP\nIn this group, we discuss the works that study KIP characteristics and create the foundations for future artifacts and\nKIP-supporting solutions.\nIs¸ik et al. in [14] study the differences between KIBP\n(KIP) and non-KIBP (regular processes). They list the most\nused or exposed characteristics found in the literature, in\norder to test which criteria are unique to KIP. Three criteria\ndiscriminates the two kinds of processes: KIP usually are\ncomplex, non- or few repeatable, and require creativity ;\nregular processes usually are simple, repeatable, and do not\nrequire a lot of creativity. However, following their study,\nstructure and automation were not discriminating. Precisely,\nthey stated that some KIP, or some parts of them, might be\nautomated.\nKushnareva et al. [40] study limitations of BPMN in the\ncontext of a specific case of crisis management process where\ndecisions cannot lead to predefined activities because of the\nunpredictability of events. The proposed solution is using\nthe statecharts formalism [41] in order to change the point\nof view (from the “HOW it must be done” to “WHAT\nmust be done” [40]). This work is formalized [26] by using\nhigraphs [42] (mathematical graphs).\nHull et al. [43] introduce the Guard-Stage-Milestone (GSM)\nmeta-model (which is also called GSM-ArtiFact) for lifecycle\nmodels. This approach is considered as entity-centric and is\nbased on four key constructs: information model (data, events,\nand milestones concerning the stage), guard (required events\nto enter into a stage), stage bodies (container of sub-stages also\ncomposed of guards and milestones), and milestones (business\ngoal(s) which ends the current stage instance when activated).\nThe execution evolves in response to the events triggered by\nmilestones and detected by guards.\nDos Santos et al. [44] emphasize that KIP contain creative\ntasks, tacit knowledge, unpredictable decisions, and evolves\ndynamically following all of those characteristics. The knowledge manipulation, and how it is still difficult to represent in\nthe process models, is a specific aspect that hinders KIP modeling with traditional methods. Specific knowledge-oriented\ntechniques are able to describe some aspects of KIP, but not\nall of these aspects simultaneously. In order to handle all\nof the useful concepts, KIPO (Knowledge-Intensive Process\nOntology) has been built following the 101 Methodology [45].\nKIPO is an high level ontology based on 5 pre-existing\nontologies: Business Process Ontology (BPO), Collaborative\nOntology (CO), Decision Ontology (DO), Business Rules\nOntology (BRO), Knowledge-Intensive Process Core Ontology\n(KIPCO). Each pre-existing ontology describes a specific\npoint of view, based on criteria gathered from a literature\nreview. KIPO is domain agnostic and its concepts are enriched\nwith stereotypes from Unified Foundational Ontology [46], an\nontology built using philosophical, linguistics, and cognitive\npsychology theories. All of these concepts allow to represent\nnearly any part of a KIP.\nDi Ciccio et al. [27] note that following the research trend\nthere is no holistic system supporting knowledge workers\nand process. Therefore, they acknowledge a missing mapping\nbetween characteristics and system requirements in KIP. The\nmapping would ease the choice of the existing approaches for\npractitioners and researchers following the point of view used\non KIP. The authors gathered 25 requirements induced by 8\ncharacteristics of KIP. The characteristics were derived from a\nliterature review and 3 application scenarios. Following these\ncharacteristics, KIP can be described by: knowledge-driven,\ncollaboration-oriented, unpredictable, emergent, goal-oriented,\nevent-driven, constraint- and rule- driven, non-repeatable. The\nrequirements are organized within 7 classes: data, knowledge\nactions, rules and constraints, goals, processes, knowledge\nworkers, environment. Following authors, these requirements\nand characteristics allow to rethink the process life cycle.\nInstead of the regular design, execute, monitor, analyze, and\nre-design steps, it becomes easier to think about templates\nand fragments to reuse in a design, execution, and adaptation\ncontinuum.\nMarin et al. [25] reuse Di Ciccio et al. [27] KIP characteristics and requirements in order to compare them with current\ncase management definition and test how well CMMN can\nimplement KIP. Case Management definitions mainly support\n7 of 8 characteristics (one of them is slightly supported by\none definition). The characteristics of KIP were classified between 2 categories : modelling (4) and execution (4). CMMN\nsupports all of the 4 modelling characteristics of KIP (2 fully,\n2 partly), making it a good candidate for KIP modeling.\nZensen et al. [47] compare BPMN and CMMN using an\nexample of KIP and provide guidelines for their usage. They\nstudy four characteristics of each modeling language: process structure, routing and control-flow, communications and\nevents, data aspect and data-flow. Whereas BPMN is perfect\nfor repetitive work with few data exchange, few exceptions,\nand strict workflow, CMMN is data-centered and allows for\nmore flexibility in ordering tasks within a workflow. Their\nfindings show that the two languages are complementary: Each\nmodeling language can potentially integrate the other for a\nspecific activity or task.\nHislop et al. [48] define a gap between theory and practice\nin KM field. They propose directions for future research in\nKM field, based on multiple papers. According to the authors,\nKM can benefit from IT in order to increase socialising,\nconnectivity and collaboration in order to create and co-create\nknowledge. Multiple technologies are presented as examples.\nNevertheless, the biggest organizations, or the most modern\nones, are still unable to benefit from those technologies.\nToo much information is available within organizations, but\nmultiple factors (technical and organizational) hinder the possibilities of their usage by knowledge workers.\nAll of these characteristics specific to KIP introduce new\nsolutions, which represent alternatives to BPM and workflowinspired approaches.\nC. Execution Support for KIP\nDi Ciccio et al. [22] study the informal communications\nand processes generated on the fly in certain situations. They\npropose MailofMine, a tool that builds workflow structure\nfrom a collection of mails, in order to represent the artful\nprocesses (KIP) implicitly hidden behind the communications.\nBased on process mining or workflow mining techniques, it\nextracts tasks and presents them graphically with a global\nand a local views. On local view, each task is shown with\nits constraints: time (before/after another task), implication\n(required tasks), repeatability (how many times each may\nbe done). In global view, all the tasks are presented with\ntheir interdependencies or constraints (orders between tasks,\nand how many times they may be achieved). The difference\nbetween local and global view lies on the readability criteria.\nThe local view is more adapted to check the constraints of\none task, and the global view is more adapted to have a main\nview of the whole process. KIP are therefore considered as a\ncollection of constraints.\nMoura et al. [34][49] aim to support the execution of knowledge intensive activities requiring collaboration. KIP usually\nrequire collaboration between multiple actors and services,\nbut authors argue that infrastructures are not able to support\nthese activities. They propose XcuteKIP, a semi-automatic\nrecommendation system of collaborative services for KIP,\nbased on the previously shown KIPO [44] (precisely by using\nthe Collaborative Ontology). Each collaborative activity of a\nKIP is mapped to services, in order to calculate the most\nappropriate collaborative service to run the process and make\npeople communicate adequately.\nIn addition to these original solutions, ACM appeared as a\nnew domain of research. Some particularities of ACM make\nit interesting:\nACM manipulates cases, which can be view as an\n• equivalent of a process instance (each instance/case is\nsomewhat different to another one, following events).\nThese cases can be stored when finished, and reused via\ntemplates [50] to help knowledge worker make better\ndecisions.\nACM uses goals to help knowledge worker to choose the\n• best action to do. Each case might have multiple goals\nor sub-goals, offering multiple choices for each step.\nACM is data-centric. Because of unpredictability, instead\n• of using the activity-centered view where the instance is\nmoving from one state to another one on a predefined\nworkflow model following rules, each case can be seen\nas a file where data are added or modified by knowledge\nworkers and/or resources which generate data.\nThe ACM community studied multiple characteristics of this\npoint of view, and proposed some solutions to handle cases\nwithin information systems.\nHuber et al. [24] study ACM in order to fill the gap left\nbetween BPM and KIP. BPM is typically used to increase\nthe performance of processes by standardizing repetitive work\nwith WfMS. As KIP are unstructured by nature and require\nflexibility, BPM is unable to correctly support them and\nincrease their efficiency. However, ACM is proposed to fill\nthe gap by giving a general structure from previous cases,\nand leave the exact execution open to change. The main\nproblem with this idea, is how to apply ACM in practice\nwithin organizations. Some questions around scaling in large\ncompanies and how implied collaboration would work are\nopened.\nHuber et al. [51] study ACM in the context of Open\nInnovation. Companies are creating more complex products\nand services, requiring cooperation between multiple teams.\nOpen Innovation (OI), a paradigm supporting collaboration,\nincludes creative activities that are unpredictable and highly\nknowledge-intensive. The authors try to use ACM to support\nthese unpredictable and knowledge-intensive activities which\nrequires a lot of collaboration.\nCognini et al. [19] study how to partially structure processes\ndepending on the possible variances. As knowledge work is\nnot predictable, there usually are no predefined structure available to help a knowledge worker to take a decision. Authors\nare using CBR methodology in order to use process fragments\nand propose solutions to knowledge workers. They propose\na “case-based process fragment modelling language named\nBusiness Process Feature Model (BPFM)”, which can be\nused as a “case content representation (modelling language)”.\nBPFM is compared to BPMN, CMMN, and DECLARE [52]\nmodelling languages to expose the required features supported\nor missing. As CBR is used in the execution context, BPFM is\nused to follow the steps/activities realized and the data used.\nTran et al. [53] propose an approach to allow flexibility\nwithin processes while checking its compliance, using logic\nrules in ACM. Therefore, ISIS Papyrus ACM System [54] is\nextended to support compliance checking. During design time,\nthe usual methods of model checking can be applied. However,\nat run-time, as goals evolve, state-based rules and data-based\nrules are used to propose a predefined set of possible goals\nand tasks. New tasks can be proposed from generic templates,\nin this case, the knowledge worker takes the responsibility of\ncomplying with the rules. When tasks are completed, their\nresults are gathered to create new compliance rules to achieve\ngoals.\nTenschert et al. [55] present an approach to detect micro\nprocesses by using speech acts. Instead of monitoring actions,\nthe progress of knowledge work is based on interactions.\nAuthors expect to “bridge the gap between structured, semistructured and ad-hoc processes” by focusing on interactions.\nThe approach allows to create templates that reduces the need\nto seek information within documents, and help knowledge\nworker to automatically fill (partially or fully) data depending\non the current case.\nCzepa et al. [56] discuss how to ease the update of compliance rules for knowledge workers and business users using\nconstraints and ontologies. Machine learning techniques are\nused to learn from past cases, and recommend next actions.\nRoutis et al. [57] propose directions to upgrade CMMNenabled platforms. Authors stated limitations within the platforms. Typically, conditions on data or event are not supported,\nclosed cases are not well exploited when reused, knowledge\nworkers can’t make modification on case model during execution phase. The notion of Case Learning is introduced\nas the procedure to improve case model when the context\nchanges by offering suggestions instead of restrictions from the\nCMMN flow. A first goal would be to implement a method that\ncould transform a CMMN model into a Case Template, and\nvice versa. The second goal is to validate the Case Learning\nprocedure as a useful tool for knowledge workers.\nBider et al. [58] test if patterns could be used in nonworkflow models, precisely with goals patterns within STateoriented Business Process Modeling (SToBPM). SToBPM\nrepresents the state of a process instance within a dimensional\nspace as a vector. Multiple types of data can be embedded\nas parameters of the process instance. Patterns used in this\napproach have characteristics: first, they are not activities/tasks\npatterns with their order like in workflow models ; second, “the\npatterns are rather semantic than syntactic”. This early test\nshown interesting positive results, even if it was very specific\nto SToBPM and goals patterns.\nZasada [59] proposes to derive patterns from legal texts,\nusing content analysis methods, in order to help elicitation of\nfood industry compliance requirements. Patterns are extracted\nfrom legal texts using content analysis in a semi-automated\nprocedure, and are stored as regular expressions. Even if the\nmethod is still not fully automated, the generated rules are a\nmachine readable knowledge that can be reused.\nSid et al. [60] focus on flexibility in KIP by using artificial intelligence planning techniques. They allow knowledge\nworkers choose any granularity level during execution, and\ndynamically change goals. Their solution aims to support the\nknowledge worker in achieving its goals instead of giving him\ntoo much freedom with few control or excessive restrictions\non the given recommendations.\nIV. DISCUSSION\nIn spite of the recent advancements, KIP management\nremains challenging for practitioners and many research ques-\ntions are yet to be solved by researchers. In this section we\nsummaries the challenges and outline some perspectives of the\nfuture research in KIP.\nThe reported literature acknowledges various aspects of\nKIP that make them different from regular business processes\nand, thus, challenging to automate and manage with already\nestablished methods and tools [32][34].\nBased on our study, the following questions still need to\nbe addressed by researchers and practitioners in the future.\nWe illustrate these questions with an example of a meeting\ndeciding on the orientation to take for a company. A meeting\ncan be seen as a KIP as it implies a lot of collaboration\nbetween multiple participants, creativity is required to bring\nnew solutions, decisions are often made, and each meeting is\nunique.\nA. How to manage efficiently and effectively the information\nand knowledge that is manipulated (i.e., created, used, updated) by KIP?\nAccording to [48], many organizations struggle with the\namount of information they have to manage. Lack of information and knowledge management practices and tools, and lack\nof integration between knowledge management and process\nmanagement have a negative impact on KIP.\nWhereas ACM proposes some solutions that address\nknowledge-intensity “by design” [51], BPM solutions require\nan extension [35].\nIn our example, recording each participants’ speeches and\ndocuments used would help to see how a decision has been\nmade in the case where the team has to change. However, the\nhuge amount of texts, documents, and meetings are hindering\ntheir usage: few people have time to hear or read again everything. One solution is to extend documents and transcriptions\nby adding relevant tags on them (which document has been\nused to take which decision). Another solution would be to\nfollow Adaptive Open Innovation [51] phases instead.\nB. How to support collaboration and context-specific decision\nmaking for KIP?\nCollaboration between knowledge workers participating in\nKIP remain challenging [34]. According to [32], the organizations are often preoccupied with process standardization\nand control. Supporting collaboration and creativity is not a\npriority for them. An excess of control or formalization has a\nnegative impact on knowledge workers and KIP [37]. [22]\noffers help for informal communications, but solutions for\ncollaboration support in highly-controlled environments are\nyet to be developed.\nIn the illustrative example, meetings are usually made\naround a table using pencils and papers, and optionally with\nvideo-conference for distant people. Forcing people to use\na precise software instead of their own tools might reduce\ntheir creativity and freedom, therefore breaking the goal of\nthe meeting: presenting and sharing creative ideas in order to\ntake decision. As there are no known solutions to combine\nstandardization and creativity, only homemade adaptations\nmay work.\nC. How to integrate context information in KIP design?\nA perspective to design and build a KIP supporting system\nbased on some “a` priori” model (by analogy with BPMS\nand WfMS) is attractive: model simulation, validation and\nverification enable compliance “by design” and could save a\nlot of efforts in implementation. KIP however are contextor case-sensitive: each of their execution scenario is unique.\nCompared to workflow execution, KIP execution can be considered “unpredictable” as its execution scenario is driven (a)\nby a context (a current situation) and (b) by a tacit knowledge\nof an expert (or group of experts) involved.\nA traditional workflow model does not provide enough flexibility to support KIP. Within current solutions, the workflow\ncan be partially skipped or organised as flexible ad-hoc subprocess [11], however a new activity cannot be added to the\nworkflow at run-time if not planned in advance.\nDeclarative modeling languages propose a partial solution:\nif the activity does not violate predefined constraints, it can\nbe executed [53].\nIn our example, each meeting has precise objectives. Even\nif the main phases are globally known (agenda’s presentation,\nand discussion of each topic), the knowledge brought by each\nperson might change the agenda based on the internal and\nexternal context of the company. Foreseeing these information\nwhile building the agenda is difficult as the situation evolves\ncontinuously based on the talks and decisions. Constraints,\nlike prioritization and time limits, avoid unending talks on\nunexpected and useless topics. Sharing the agenda and main\ninformation before the meeting also reduce off topic talks.\nD. How to support compliance in KIP?\nFrequent changes in regulations and rules have to be followed by the processes and have to be reflected by corresponding PAIS [56][61]. BRM brought solutions for compliance by\nmanaging rules within specific transverse repositories using\ndedicated languages [62]. Rules and declarative approaches\nallow to create a frame where knowledge workers are free\nas long as they stay inside. For KIP, automated compliance\nchecking is a challenge: due to context-sensitivity, compliance\nchecking in KIP needs to be done at run-time, when some\nactivity is created and/or instantiated according to the execution context. Decisions about the next activity to execute are\noften made by a knowledge worker in KIP [40]. Thus, it is\nthe responsibility of a knowledge worker to ensure that this\nactivity comply with the regulations and norms. A possibility\nto analyze an activity instance, its compliance and its potential\nimpact on the process scenario would be of a great value for\nknowledge workers [53][56].\nThe lack of formalization in regulations and norms also\nmakes compliance checking in KIP challenging. These regulations and norms are often specified as an unstructured text,\nwhich is hard to access and to use for knowledge workers\nduring the process execution [61]. Formalizing regulations and\nintegrating them into PAIS can improve the situation and help\nthe knowledge worker to ensure the compliance [59].\nIn the example, the order of the agenda might be very\nimportant. It is the responsibility of each participant to present\nrelevant ideas. However, it is difficult to prevent the discussions to move on another topic, except if the agenda is\nregularly reminded. In some emergency cases, it might be\nrequired to change of topics.\nE. How to support flexibility in KIP scenario?\nFlexibility in KIP is studied in multiple\nworks [32][31][61][37][53][51][47]. Some works tried\nto offer some solutions for managing flexibility in BPM,\nlike [11]. However, the paradox of organizations expecting\nto standardize their processes with BPM, and respecting\nregulations, while giving enough freedom of action to their\nknowledge worker is still unsolved. KIP are overlapping\nmultiple fields of research, like BPM and KM, allowing\nmultiple points of views. In the illustrative example, flexibility\nis available by the nature of a meeting: people are talking\nfollowing simple rules (people raise their hands before having\ntheir speaking slot). If everybody is in accordance to, an\nunexpected topic can be discussed even if not on the agenda.\nHowever, if no agreement is found, it becomes difficult to\nadd a topic.\nTwo points of view on KIP are prevalent today: an activitycentered view where the process is specified with a sequence\nof activities, and a data-centered view, where the process is\nspecified with the data (also referred to as a “case”) it requires\nor manipulates.\nUsing the activity-centered view, the process designer can\nexplore the process (KIP) execution logs in order to analyze\nthe process steps and identify the process structure [22]. The\nactivity-centered view highlights structure, and focuses on the\norder of activities. Here, some high level structures [24] or\npatterns [19] can be explored.\nWithin the data-centered view, a KIP seeks to achieve one\nor multiple goals through collaboration between participants.\nKIP can be managed using a document (a case folder). Each\nparticipant uses an information from the case folder and makes\ndecisions based on it. Participants can execute activities for the\ncase based on the available inputs and required outputs [2].\nThis point of view ensures more flexibility in decisions and\nprocess scenarios compared to workflow (or activity-centered\nview).\nThe two points of view are complimentary: the activitycentered view can be used to provide a global “roadmap”\nfor a KIP - a sequence of steps to achieve the goal(s) - at\ndesign time. This roadmap is not hindering local flexibility for\ncomplex steps. These steps (or subprocesses) can be detailed\nusing the data-centered view at run time. Conversely, patterns\nfor some complex (or highly prescriptive) activities can be\ndefined at design time, using the activity-centered paradigm.\nAt run-time, a knowledge worker can choose a pattern or\na combination of patterns when appropriate, maintaining the\nglobal flexibility.\nMultiple ideas of patterns applied to KIP or ACM are\ndiscussed. In [59], compliance patterns are studied within legal\ntexts and transformed into regular expressions in order to\ncheck compliance. [58] introduces goal patterns for SToBPM.\n[19] represented cases with CBR and BPFM in order to help\nknowledge workers by proposing solutions based on process\nfragments.\nUsing activity-centered point of view on the illustrative\nexample would give a list of topics to talk about. In the strict\nvision, it would be impossible to talk about something else\nthan the current topic. As a meeting is by nature flexible,\napplying the data-centered point of view allows to change\nof topic following the information given by each participant.\nSome topics might be linked because of their impact on the\ncompany, they must therefore be addressed together and in\na precise order. In this case, the linked topics added in the\nagenda can be seen as fragments of patterns.\nF. How to explore and reuse process fragments in KIP?\nThe idea of studying past cases in order to identify and\nreuse fitting fragments in the next instances (like in CBR)\nof KIP is highlighted in [27][19] and seems to have a great\npotential. A possible way to help knowledge workers could be\nto provide patterns of past solutions, and check compliance of\nan instance compared to a reference, by using a combination of\nmathematical technique and ontologies. We expect to compare\nthe logs of activities of current instance of KIP, with the previous finished KIP or reference from regulations and standards,\nsimilarly to what CBR does when searching the most close\nsolution to a problem.\nThe illustrative example is based on an agenda and topics.\nIn order to build the next agenda on specific topics already addressed in the past, it might be useful to reuse past fragments.\nIf these topics were not added in the initial agenda, but during\nthe unexpected deviation of talks, it might be useful to reuse\nthis order in the new agenda as a reminder of the context to\nevery participant.\nIn [9], several methods are combined to evaluate the similarity of business process models. Similarly, three techniques\ncan be combined in order to check the structure, the semantic,\nand the behavior of process fragments. In order to compare\nthe structure of patterns, we propose to use Formal Concept\nAnalysis (FCA) [63]. FCA is a method for data structuring\nthat is based on deriving a concept hierarchy from a collection of objects and their properties [64][65]. This method is\napplied for knowledge discovery in databases (KDD) [66]. The\nsemantic comparison is expected to be done using concepts\nfrom BabelNet [67] or semantic distance from WordNet [68].\nBehavior comparison can be done using association rules, like\nA Priori [69]. Association rules generate rules describing in\nwhich conditions some objects might appear, based on traces\nof execution.\nWe expect to show first how FCA and semantic analysis\ncan generate interesting fragments, then how association rules\ncan propose an order of assembly. The first contribution will\nextract fragments of interest for the business based on its\ndomain ontology. The second contribution will propose to the\nknowledge worker how to combine these fragments.\nV. CONCLUSION\nKnowledge-intensive process management represents a\nchallenge for modern organizations: KIP are often complex\nand context-specific; they require flexibility; they involve\ncollaboration and rely on creativity and tacit knowledge of\nparticipants. Product design, change management, problem\nsolving, diagnostics, customer relationship management, education, apprenticeship - are some examples of KIP.\nStrategic modeling activities share common practices and\nchallenges with KIP: first priority on goals; information\nprocessing, collaboration, decision making as usual steps;\nimportance of compliance or constraint while executing a\nprocess. Contributions from the KIP domain bring answers\nand practices to strategic modeling.\nIn this article we provided an overview of the current work\non KIP. We examined several groups of contributions:\nthe works defining KIP management solutions as an\n• extension of BPM solutions. These works use the activitycentered point of view on KIP and add new concepts and\nsemantics in order to increase the run-time flexibility of\nprocesses and improve the context-awareness.\nthe works developing theoretical foundations for KIP\n• and considering KIP management as an alternative to\nBPM. These works explore alternative, non-workflow approaches to process management (data-centered, artifactcentered, etc).\nthe works proposing pragmatic solutions for KIP execu• tion support. These works are driven by the practitioners\nneeds and focused on implementation and practical value\non the first place.\nIn spite of significant efforts of researchers and practitioners\nin the domain, more research is still required in order to\nenhance PAIS with KIP management capabilities: some major\nchallenges concern run-time flexibility, standardization and\ncompliance support. Run-time flexibility refers to a capacity\nof a PAIS to add, skip or modify process tasks at run-time.\nStandardization and compliance support refers to a capacity\nof a PAIS to help knowledge workers to respect regulations.\nContext-specific guidance for knowledge workers, automatic\nidentification of compliant and non-compliant activities at runtime are possible solutions.\nPatterns in KIP and process execution context are studied\nin the literature. Identifying patterns in previous activities\nand resolved cases/instances can be helpful for developing\nrecommendations for knowledge workers. For example, a\ncorresponding solution can help a knowledge worker to identify an activity that fits the situation and, at the same time,\ncomply with the norms and regulations defined for current\nKIP instance. We are going to develop this idea in the future\nwork.",
  "2019-RRW": "1 Introduction Business process models treat all participating actors alike, as deterministic machines. Human actors are seen as no different than automats producing results based on predefined inputs. The psychological and social dimensions that characterize human behavior are not represented in these models. In this article, we explore how these dimensions may be added to a traditional business process model. We use Vickers’ appreciative system [1], [2], [3] as a model of human behavior. Vickers created this model to show how people made sense of themselves and their environment. Vickers was feeling that models based on goal achievement did not give a good description of the richness of human interpretation of complicated situations. * Corresponding author © 2019 I. Rychkova et al. This is an open access article licensed under the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0). Reference: I. Rychkova, G. Regev, and A. Wegmann, “Extending Business Process Models with Appreciation,” Complex Systems Informatics and Modeling Quarterly, CSIMQ, no. 18, pp. 23–46, 2019. Available: https://doi.org/10.7250/csimq.2019- 18.02 Additional information. Author’s ORCID iD: I. Rychkova – https://orcid.org/0000-0002-1100-0116. PII S225599221900104X. Received: 1 January 2018. Accepted: 8 February 2019. Available online: 30 April 2019. We explain homeostasis [4] and Vickers’ appreciative system, and show how these concepts can be applied to business process modeling. We use the example of a recruitment process in which graduate student candidates are evaluated to be hired as doctoral students. We use the term PhD as an abbreviation of “doctoral student.” We use statecharts to model the process as defined by the university. This process includes the Doctoral School, a Faculty Member and a Graduate Student (a candidate). We first create a model that we call operational. It shows the standard process model that is usually drawn to automate behavior. We then discuss that the operational model captures the “technical” perspective of the process keeping the “social” perspective of the process participants implicit. We make this social perspective explicit with Vickers’ appreciative system in what we call the appreciative model. This model exposes the social norms that govern the behavior of the process actors. This is a preliminary work that extends our previous publication at STPIS’18 workshop [5]. With this work we would like to show that it is feasible to model an appreciative system with statecharts, to link it to a business process model, and therefore to enlarge the technical scope of business process modeling with a social perspective. In Section 2 we introduce the example of the doctoral student recruitment process and provide the basic concepts of statecharts. In Section 3 we present the operational model of the doctoral student recruitment process as defined in the university. In Section 4 we present Cannon’s framework of homeostasis and Vickers’ appreciative system and propose the formalization of an appreciative systems with Harel’s statecharts. We extend the operational model of the doctoral student recruitment process with an appreciative model in Section 5. In Section 6, we discuss the interest and some challenges of formalizing the appreciative systems. In Section 7 we address the future work and conclusions. 2 The Running Example and Introduction to State Charts This section describes the principle of state chart models that we will use to describe the operational model as well as the appreciative model. It also introduces the example we will use to illustrate the concepts we propose to include in process modeling. 2.1 Example: The PhD Recruitment Process In this work we use an example of a PhD recruitment process of a University. The process involves three actors: a faculty member (FM) – a professor leading a research group and recruiting a doctoral student for the team; a graduate student (GS) searching for a PhD position; and a doctoral school (DS) that preselects the PhD candidates and manages them until their graduation. This process is a simplified view of the PhD recruitment processes experienced by the authors of this article. It omits other actors (e.g. the HR department or the dean of the university who might be also involved). In order to be recruited as a doctoral student by the university, a GS  has to be accepted by the DS and  has to be approved by a prospective thesis supervisor (a FM). The GS can start the process either by sending an application to the DS or by directly contacting the FM whose team she/he aims to join. In the first case, once accepted, GS needs to apply for the positions with different FMs. In the second case, once approved, the GS needs to apply to the DS for evaluation. The DS receives and evaluates the GS' applications by examining their academic background and track of records. If the DS accepts the GS candidate, the candidate needs to search for a thesis supervisor by sending applications to various FMs. If no supervisor is found within some predefined time frame, the GS cannot be recruited and the process stops. 24 The FM evaluates GS applications mainly focusing on the candidate’s motivation and skills related to the specific research topic and can approve or reject the GS. If the FM approves the graduate student candidate, the latter must also be accepted by the DS. If DS rejects the graduate student candidate, the GS cannot be recruited and the process stops. This double stage process is designed in order to guarantee the excellence of graduate student candidates, evaluate their capacity to integrate in a university doctoral school program and in a specific research team and, also to successfully finish their training by obtaining the PhD degree. 2.2 Statecharts and YAKINDU SCT Statecharts. We model the PhD recruitment process using statecharts defined by Harel [6]. Compared to a workflow paradigm widely used for process modeling, we specify the process with a set of states and transitions between states. This allows us to omit specification of concrete activities associated with recruitment but to focus on the process goals and milestones. Thus, statecharts provide a uniform modeling notation to reason about the operational (business) process as specified between the process participants and about the cognitive process associated with decision-making of each participant. Statecharts formalism. The statecharts formalism specifies a hierarchical state machine (HSM) that extends classical finite state machine (FSM) [7] by providing:  Hierarchy (or depth) – the possibility to model states at multiple hierarchical levels, with the notion of abstraction/refinement between levels;  Orthogonality – the possibility to model concurrent or independent submachines within one state machine;  Broadcast communication – the possibility to synchronize multiple concurrent submachines via events. As for FSM, states and transitions are central concepts in statecharts. State specifies a state of the modeled system. A state can have a behavior that describes which actions are taken under which conditions. Transitions between states are triggered by events. If a transition t takes place only if an event e occurs – e is called a triggering event of the transition t. Triggering events can be complemented with a guard conditions: e[c]. In this case, we say that a transition t takes place when e occurs and c holds. During an execution of a state machine, a state can be active or passive. Due to a hierarchical structure, where several (sub)states can be embedded in a (parent) state, a multiple states of a statecharts can be activated at a given moment of execution. These states are called an active configuration of the state machine. Compared to a FSM transition, a statechart transition can be seen as a change from one active configuration to another. We illustrate this formalism and the notion of model execution on the examples in the following sections. Yakindu. We use YAKINDU Statechart Tools (YAKINDU SCT) [8] for modeling, simulation and analysis of the process. YAKINDU is a modular toolkit for developing, simulating, and generating executable finite-state machines (FSM). YAKINDU statecharts are organized in regions. Hence it is possible to organize multiple state machines in different regions and to run them concurrently. Regions contain states and transitions. A state, in turn, can contain one or more regions, turning it into a composite state (contains one region) or an orthogonal state (contains two or more regions). A state can define behavior in the form of one or several trigger [guard] / effect statements. These statements provide a declarative (non-prescriptive) specification of behavior as no execution order is applied. Triggers are events that can be complemented with a guard condition (guard). The effect will only be executed if the trigger occurs and if the guard condition holds. The effect can include one or several actions such as assigning a value to a variable, raising an event, calling a function. 25 We simulate the statechart models with Simulation view in YAKINDU STC. The framework allows us to manually raise events, to inspect and modify variables of a running simulation and to observe the model’s behavior as a sequence of active configurations triggered. We use this simulation tool to play and discover different process scenarios. More advanced features for model simulation are available in YAKINDU. They are out of the scope for this article. 3 The Operational Model of the PhD Recruitment Process The PhD recruitment process, as defined by the University, provides the details on the steps that must be taken starting from the point when a graduate student (GS) applies for a PhD. This application can be done either by sending an application to the doctoral school of the university (DS) or by contacting a professor (FM). The PhD recruitment process terminates with a recruitment or rejection of the GS. In this section we model the process of the PhD recruitment described above with statecharts using YAKINDU STC. We explain the model and how it can be simulated with YAKINDU. We call this process model “operational” as opposed to the “social” process model discussed in the next sections. 3.1 Using statecharts for process modeling We model the process as collaboration between its three participants. We use statecharts to describe the role of each participant in the process. Each statechart contains states and transitions describing the behavior of the corresponding participant and is illustrated in: Figure 1 (Graduate Student), Figure 2 (Doctoral School), and Figure 3 (Faculty Member). The process model (Figure 5) shows collaboration between participants and is organized in three independent regions. Process participants exhibit concurrent behavior and communicate via messages. The statecharts diagram in Figure 5 is also a statechart and can be used as a part of a more complex diagram. This creates a hierarchy of statecharts. Hierarchical statecharts are successfully applied for modeling complex real-time systems [6] and thus we consider that they can also be applicable for real business processes. Figure 1. Statecharts model of behavior of the Graduate Student (GS) within the formal PhD recruitment process defined by the University. 26 The Graduate Student “GS” statechart (Figure 1) describes a behavior of a graduate student who applies for a PhD position. The statechart is specified with five states: Start_Recruitment (the default starting state), Applied to DS, Applied to FM, Rejected and Recruited states. State transitions (depicted by the arrows between states) are triggered when their corresponding triggering conditions are satisfied. The triggering conditions are modeled with “condition[guard]/effect” statements. The condition is specified with a boolean expression or an event raised by some statechart (another process participant in our case). It can be guarded by an invariant. The effect describes what will happen along with the transition (e.g. an event can be raised or an operation can be executed) and is optional. The triggering conditions are shown next to their corresponding transitions (the arrows) in the model. For instance, the transition  “Applied_to_DS Rejected” in GS (Figure 1) is triggered when the ds_reject event is raised (no guard is specified). We model the events with a prefix indicating the statechart raising this event: ds_ corresponds to the DS statechart (represented in Figure 2).  The transition “Applied_to_DS Applied_to_FM” in GS (Figure 1) refers to the case when the graduate student, once accepted by the DS, has to apply to a faculty member (or FM). The statement on the transition ds_accept[active(FM.Start_Recruitment)]/raise gs_applyFM reads as follows: when the DS accepts the graduate student candidate (ds_accept event is received) and the graduate student candidate is not yet reviewed by a FM (the FM statechart is in its starting state: active(FM.Start_Recruitment)), the graduate student applies to a FM (the event gs_applyFM is raised). This event triggers a transition in the FM state chart (Figure 3).  The transition “Applied_to_FM Applied_to_DS” in GS (Figure 1) refers to the opposite case – when the graduate student gets approved by a FM (i.e. fm_approved is received) while not yet applied to the DS. The Doctoral School “DS” statechart (Figure 2) describes a behavior of the doctoral school. The statechart is specified with five states: Start_Recruitment (the default starting state), Candidate_Review, Accepted, Rejected and Recruitment.  The transition “Candidate_Review Accepted” in Figure 2 refers to the case when the DS accepts a graduate student while the candidate is not yet approved by a faculty member (thus the DS waits for the decision to terminate the process). The statement on the transition ds_accept[!active(FM.Approved)] expresses this behavior. Figure 2. Statechart model of behavior of the Doctoral School (DS) within the formal PhD recruitment process defined by the University 27  The transition “Accepted Recruitment” in Figure 2 refers to the case when for a graduate student accepted by the DS (being in the state Accepted), the DS eventually receives an approval from a FM. From the point of view of the doctoral school, the recruitment conditions are fulfilled, the procedures can be now finalized and the GS can be notified on recruitment. The statement on the transition fm_approve/raise notify_GSrecruited expresses this behavior. Figure 3. Statechart model of behavior of the Faculty Member (FM) within the operational PhD recruitment process defined by the University The Faculty Member “FM” statechart (Figure 3) describes a behavior of the faculty member. The statechart is specified with five states: Start_Recruitment (the default starting state), Candidate_Preselection, Approved, Rejected, and Recruitment. The transition logic is similar to one described above for the DS. 3.2 Interaction Between Process Participants The DS, FM, GS statecharts are running concurrently: each statechart represents an actor who reacts on the events raised by the other actors (statecharts) in the model. In Figure 4 we present a list of events defined for the operational PhD recruitment process. internal: // operational process events // user events (manuall): event gs_applyDS event gs_applyFM event fm_approve event fm_reject event ds_accept event ds_reject // simulated events: event notify_GSrecruited Figure 4. Events defined for the Operational PhD recruitment process All the events except of the last one are specified with the prefix (ds_, fm_ or gs_) that identifies the “sender” of the event. The last event “notify_GSrecruited” can be raised by either the DS or FM, when the conditions for recruitment are fulfilled. 28 Events represent the observed results of some activities during the process. During the simulation, all the events specified in the model are broadcasted and every statechart in the model “receives” them or “sees” them occur. However, it is a modeler’s choice to specify which events will be ignored and on which events the statechart will react and how. The modeler can specify a behavior that can be triggered (a) upon receiving some event when entering, exiting or resting at some state or (b) when a transition between states is triggered. In the model, the corresponding behavior statements and their triggering conditions can be shown on state transitions (see the examples above) or inside the states (this will be explained in the next section). 3.3 Operational Model Simulation Model simulation in YAKINDU allows for visualizing different process scenarios. During the simulation, the simulation window shows the current active state of each statechart and the last triggered transitions (they are highlighted in color in Figure 5 (a)). This is called “current active configuration” of the statechart. The simulation window also provides the list of events (Figure 5 (b)) that can be raised during the simulation. In the simple simulation mode, the user clicks on these events, one at a time, simulating their occurrence and the statechart reacts by changing the active configuration. As discussed in Section 3.2, some events can be raised by statecharts as a part of their behavior expression (specified by the “raise” keyword). The simulation terminates when some final state or final configuration is reached. More complex simulation can be done with YAKINDU, when different scenarios can be preconfigured and played automatically or driven by some program at the backend. In our example, it is the user who simulates the event occurrence and can decide upon their order. Note that events can have an effect in current active configuration only if at least one behavior statement describing the reaction on the event is specified in this configuration. Conversely, if no behavior that describes a reaction on an event is specified – the event will be ignored by the statechart. For instance, consider the current active configuration as shown in Figure 5. If the user clicks “notify_GSrecruited” event from the list – its occurrence will have no effect and the active configuration will not change. Clicking “ds_reject,” however, will immediately trigger transitions in both DS and GS statecharts and the active configuration will change. In our example, the user plays the roles of different process participants and thus has to respect some logic in order of events that can be raised by these participants. Thus, when the model simulation starts, the user plays the role of the graduate student (GS) by applying either to the DS or to the FM; next it has to simulate the corresponding decision of the DS or FM and so on. Figure 5 (a) illustrates the configuration of the statechart after the user raises gs_applyDS event from the simulation window (by clicking gs_applyDS): here the GS statechart passes from Start_Recruitment to Applied to DS state, DS passes from Start_Recruitment to Candidate_Review and FM remains in the Start_Recruitment state. From this configuration, the next step will be to “simulate” the decision made by DS on the application by raising ds_accept or ds_reject events (i.e. the user needs to choose and click on the corresponding event in the list illustrated in Figure 5 (b)). Here the user plays the role of the doctoral school (DS) in the process. While occurrence of major events in our model is simulated by the user, some events are  automatically raised: the transition “Accepted Recruitment” of the DS statechart in Figure 5 (a) is specified with the expression fm_approve/raise notify_GSrecruited. Here the fm_approve event need to be raised by the user (clicking in the simulation window), but the notify_GSrecruited event will be raised automatically as a part of the behavior expression specified for the transition. 29 Example of scenarios to play with this model: gs_applyDS – ds_accept – fm_approve – notify_GSrecruited – here the graduate student candidate applies to the doctoral school, gets accepted, applies to a faculty member, gets approved and eventually recruited. gs_applyFM – fm_approve – ds_reject – here the graduate student candidate applies to a faculty member, gets approved, applies to the doctoral school, but gets rejected. gs_applyDS – gs_applyFM – fm_approve – ds_approve – notify_GSrecruited – here the graduate student candidate applies to the doctoral school and to a faculty member (not waiting for being accepted), gets accepted, gets approved and eventually recruited. Other variants of scenarios are possible. (b) (a) Figure 5. Simulation of the operational model with YAKINDU: (a) represents the three concurrent state machines and (b) shows the events sent by the person running the simulation (i.e. the actual decisions taken by the three actors). 3.4 Discussion on the Process Efficiency – Implicit Gap The graduate student candidate is recruited when two conditions are fulfilled: FM approves the candidate and DS accepts the candidate. It is known from practice that the DS evaluates the academic background of the graduate student candidate. FM evaluates GS' applications mostly focusing on candidate’s motivation and skills related to the specific research topic. This double stages process is designed to ensure the graduate student candidate excellence both on the university level and on the level of the research team. An assumption is made that an academic excellence of a GS as defined by the doctoral school is a necessary condition for successful integration of the GS in the research team and completion of the PhD training. Nevertheless, some faculty members do struggle to hire a PhD student 30 within this process. This creates a gap (a conflict) in the organization. The gap is implicit in the model. The concrete evaluation (or how the DS and FM are making their respective decisions about graduate student candidate excellence) is implicit in the model. In order to understand the gap and to eventually improve the process, we propose to examine the appreciative systems (as proposed by Vickers) of the FM and DS. These appreciative systems will be formally represented in one appreciative model per actor. In the next section, we discuss the theories of homeostasis [4] and appreciative system [1], [2], [3]. We then model the appreciative systems of the DS and FM with statecharts, creating a social model of the PhD recruitment process. Compared to the operational model discussed above, the social model is focused on the cognitive aspect of the recruitment: it shows how this process is perceived by the FM and DS and how the decisions regarding the graduate student candidate excellence are made by these participants. 4 From the Technical Perspective to the Social Perspective of the Business Process: Appreciative Systems 4.1 Homeostasis and Appreciative Systems A business process is often associated with achievement of some or several well-defined goals. This can be seen as the direct implementation of Cybernetics as defined by Rosenblueth, Wiener and Bigelow [9]. Rosenblueth was a collaborator of Cannon [10] who, in the 1920s, coined the term Homeostasis in order to explain how an animal body maintains the steady states that are the basis of its survival [4], [11]. Cannon explained that living organisms somehow found a way to maintain steady states even though they are made of unstable internal elements and live in an unstable external environment. It is the maintenance of these more or less stable internal states that maintain a living being’s identity and therefore its survival. In Cannon’s work, as explained by [12], there is no goal to be achieved, just the maintenance of steady states. Rosenblueth, Wiener and Bigelow simplified Cannon’s work and defined teleological, purposeful, behavior as achieving a well-defined final state through the use of a negative feedback mechanism [10]. Whereas the early work in Cybernetics involved the study of man-made systems, it was very quickly applied to socio-technical systems. Business process management is one such example. The goals that are to be achieved by business processes are the modern-day descendants of this early teleological work. Writing from a social perspective, Vickers [1], [2], [3] took the work created in Cybernetics [9] and re-expanded it with maintenance in mind, but this time writing about the maintenance of relationships instead of states. The maintenance of a relationship is, in fact, the maintenance of a relationship in a given state. Vickers wrote about attaining, maintaining and eluding relationships [11]. Maintaining and eluding a relationship can both be seen as keeping it in a specific state, either close or distant. In Vickers’ work these relationships are maintained with respect to norms, states that remain more or less the same over time, just like Homeostasis is the maintenance of (quasi) stable states. For Vickers, norm-holding was very different from what he termed goal-seeking (goal-seeking is known today as goal achievement in Requirements Engineering [13], [14]). Achievement goals are met once and for all, and determine a well-defined end point. For example, to accept or reject a PhD candidate are two end states of the PhD recruitment process. Norm holding defines an on- going activity of matching the current state of affairs with the relevant desired state [11]. Accepting or rejecting a PhD candidates are only steps in the overall on-going activities of a DS and FM in which these structures must be kept alive by regulating mutual relationships. In Requirements Engineering, the concept of maintenance goal was suggested to represent this on- 31 going activity [13], [14]. The concept of maintenance goals was defined earlier on in [15] and [16]. The desired state can be described with a set of steady states called norms. Each norm is associated with a threshold, (e.g. a human body temperature is maintained at approximately 37°C, with an associated threshold of a fraction of degree). An action is taken whenever the actual state is sensed to be outside of the threshold. This well-known process of regulation has no beginning or end. It operates as long as the subject survives and is responsible for its survival. This results in a homeostatic state [4]. Unlike the animal or machine, in which the norms are often pre-defined by either evolution or design, the norms that are held by an organization are the result of the on-going process of appreciation. They are therefore generated and maintained by the same process they govern and should not be considered as immutable or given by a designer [3]. Vickers distinguished two “segments” in the regulation process. In the first segment, the current state is compared with the norm. The resulting information is fed to the second segment in which an action is taken to correct the state if it is outside of the threshold associated with the norm [3]. Vickers notes that the second segment has received much more attention in system engineering and problem solving than the first segment. In policy-making, he observes, the major difficulty is in the first segment. In this segment the challenge is to define the criteria for capturing the current state and evaluating it against multiple potentially inconsistent norms [3]. It is this attention to the first segment that most distinguishes norm-holding from goal-seeking. Vickers conceived the appreciative system, a model of policy-making as norm-holding, to give more attention to the first segment. Vickers’ appreciative system is composed of 3 distinct, but interrelated elements [2], [3], [17]: Reality Judgments (RJ), Value Judgments (VJ) and Action Judgments (AJ). Reality judgements and value judgments can be seen as comprising the first segment. Action judgments can be seen as belonging to the second segment. Reality judgments correspond to what people perceive of their situation. Value judgments correspond to the selection of the norms relevant to these reality judgments and how the actor compares the reality judgments with these norms, resulting in a realization that the situation is acceptable or not. Vickers further detailed the stages of the Value Judgment component as [17]:  Attaching a Reality Judgment to an existing category and thereby defining the relevant norm (Matching).  Evaluating the Reality Judgment (the state of affairs) on present and future relations with the help of the norm (Weighing).  Creating a new category for future exercises of the appreciative system (Innovating). Action judgment is prompted if the situation is unacceptable in order to bring it to a more acceptable form. The action can be directed inwards (in order to change the “inside” of the system: norms, value judgments, reality judgments), outwards (in order to change the “outside” of the system, it’s environment) and can be “not to change anything.” Thus, action judgments correspond to the choice of relevant behavior (activities) within the selected response strategy. Vickers describes the overall functioning of the appreciative system as a process of optimizing and balancing [3], [17]. Optimizing is geared toward what Vickers calls functional (or service) relationships. It attempts to keep the relationships with the actor’s external entities in a relatively good state (akin to Simon’s satisficing concept) [1]. Balancing tries to prevent the depletion of its resources; which Vickers calls metabolic relationships. The maintenance of the requisite functional relationships is dependent on the metabolic relationships and vice versa. Thus, the appreciative system continually tries to provide as good a service as possible to its external stakeholders (optimizing), with its available or obtainable resources (balancing the inputs and outputs) [3]. The repeated exercise of the appreciative system leads to what Vickers calls readiness in each of its components: Readiness to see (related to RJ), Readiness to value (related to VJ) and 32 Readiness to act (related to AJ). At any given moment this readiness defines a specific state of the appreciative system, which Vickers calls the appreciative setting [1]. 4.2 Modeling Appreciative Systems In this section we describe a generic appreciative model that shows how the concepts defined by Vickers (i.e. norm, threshold, reality judgment, value judgment, and action judgment) can be modeled with statecharts for a generic actor. In the next section, we use this generic model to define appreciative systems for the faculty members and the doctoral school actors in our PhD recruitment process. The Appreciation model, is illustrated in Figure 6, consists of one composed state called A_Setting. A_Setting represents the appreciative setting of the actor and can be associated with one norm or the set of norms N the actor seeks to maintain (i.e. keep unchanged). Each norm N is associated with a tolerance threshold. When this threshold is crossed, the system is not in its homeostatic state and has to respond to regain it by acting on the environment or on its appreciative setting. We represent Reality Judgments (RJ), Value Judgments (VJ) and Action Judgments (AJ) as sub-states of the A-Setting state. The regulation process is captured with states transitions. For each of the states, we define a behavior that will be triggered upon entering, exiting or while remaining in this state with a list of trigger[guard]/effect statements. Figure 7 depicts the list of variables and events that can be captured or raised by the appreciative model. These events and variables are used in the trigger, guard or effect expressions of the behavior specification. Step 1: Actor’s Reality Judgment RJ is the default starting state for the regulative process. In the RJ state, the actor captures the events e (Figure 6) raised by the environment (i.e. another actor) and transforms them into reality judgments. We do not show the details of this transformation. These reality judgments are further used in the VJ state for the situation assessment, a transition toVj (to the VJ state) is triggered. Step 2: Actor’s Value Judgment In the VJ state, the actor evaluates the situation by selecting relevant norms to compare with the reality judgments r1, r2, and concluding if their corresponding thresholds are crossed or not. Thus, each ri can take a value of “OK” or “NOK.” Still in Figure 6, we model value judgments with the events: r1isOK, r1isNOK, r2isOK, r2isNOK. Simulating the value judgment in YAKINDU, r1isOK will be raised when r1 is considered within the threshold and r1isNOK will be raised otherwise. In the current model, value judgments must be raised by the user from the simulator view. Potentially this can be automated with YAKINDU. Each value judgment automatically triggers a transition to the AJ state (toAj event is raised automatically, see Figure 7). In order to make more than one value judgment, the user will need to raise the addVj event in the AJ state. 33 Figure 6. A generic appreciative model Optimizing and balancing internal: event toVj //-automatic, raised when RJ is created event toAj //-automatic, raised when VJ is made event addVj //-raised by the used to make a new VJ event decision //-raised by the user when decision making is finished // response types - automatic, based on optimizing-balancing event noChange event actSeize event actAvoid event changeVj //value judgments - manual, raised by the user event r1isOK event r1isNOK //Optimizing-balancing var OP: integer var TH: integer //observations (inputs) –user events (manual) event e //actions affecting the reality (outputs) – simulated events event action1 event action2 event action3 event action4 Figure 7. Events and variables used in the Appreciation statechart We define optimizing-balancing as a function F(r1,...) that transforms any combination of value judgments {.. riOK, rjNOK, ..} into a pair {OP, TH}, where OP indicates how much the actor perceives the overall situation as an opportunity (something we wish to seize or take) and TH indicates how much the actor perceives this situation as a threat (something that we wish to avoid). This perception of the situation as a whole defines the type of response the actor eventually produces. In Figure 8, we illustrate optimizing-balancing. Once a value judgment is made, we use the table of optimizing-balancing that indicates “how much” this value judgment is an opportunity and how much it is a threat. Summing up the obtained scores for all value judgments made about the situation we obtain OP and TH respectively. We call these the “optimizing-balancing factors.” 34 Figure 8. Optimizing-balancing We distinguish several response types based on the total OP-TH scores. When OP ≠ TH – the response will be directed outwards (in order to change the system’s environment, producing some impact that can be captured by external actors). In particular, when OP > TH – the actor will act to seize what it sees as an opportunity; when OP < TH – the actor will act to avoid what it sees as a threat. When OP == TH == 0 – the observed situation does represent neither a threat nor an opportunity and the response will be neutral (“not to change anything”). When OP == TH ≠ 0 – the situation is unclear for the actor, and the response will be directed inwards (in order to change the appreciative setting, i.e. norms, value judgments, reality judgments, optimizing and balancing factors). In the example above, if we change the optimizing-balancing factors for r1OK from [2,0] to [3,0] , we will obtain OP = 3 and TH = 2 for the situation. This will result in OP > TH– the act- seize response type. In the appreciative model in Figure 6, the optimizing-balancing is shown in the VJ state: r1isOK / OP = OP+2; raise toAj r1isNOK/TH = TH +3; raise toAj r2isOK / raise toAj r2isNOK/TH= TH+2; raise toAj Raise toAj stands for a transition to the AJ state. To reflect the real cognitive process of evaluation, we assume that any number of value judgments can be considered. During simulation, addVj events can be raised many times in order to mimic a human process of appreciation: one individual may undertake a thorough reflection VJAJVJ..., cycle with various assessments concurrently increasing and decreasing OP and TH, until the moment she is satisfied with her assessment. And conversely, an individual can make a hasty conclusion, making a judgment based on one parameter only and passing to action: VJAJ Step 3: Actor’s Action Judgment Action judgments correspond to the choice of relevant behavior (concrete actions) corresponding to the selected response type. In the statechart in Figure 6 we model it with the following statements: decide[OP == TH && OP>0] / OP=0; TH=0; raise changeVj decide[OP>TH] /raise action1; raise action2; OP= 0; TH=0; raise actSeize decide[OP<TH] /raise action3; raise action4; OP= 0; TH=0; raise actAvoid decide[OP==TH && OP ==0] /raise noChange 35 During the simulation, when in the AJ state, the user raises the event decide that stands for executing a response. We use OP and TH variables in a guard expression to select the right response that matches the guard. changeVj response brings the statechart back to the VJ state and resets OP and TH values. – here we simulate the change of value judgments. In the current model we can illustrate only this type of “inward” changes or changes in appreciative settings during the simulation since the other settings cannot be changed “on the fly” and require modification of the statechart. actSeize and actAvoid responses correspond to the “outward” response, where some actions (e.g. action1, action2, ...) will be executed and TH and OP will be reset. These actions define how exactly the actor will interact with the environment (i.e. other actors) in order to seize the opportunity or to avoid the threat respectively. This simulates a change in the environment that results in a change in RJ. The limitations of our model make it a closed-loop but in reality the action is made on the environment and then sampled again in the RJ. The Appreciation statechart will be set to its initial RJ state for the next cycle of the regulative process. noChange response resets the Appreciation statechart bringing it to its initial RJ state without any change (to the environment or to the appreciative settings). 5 Extending the Operational Model with the Model of Appreciative Systems for the PhD Recruitment Process We extend the model of the operational PhD recruitment process presented in Section 3 with the appreciative models of the Faculty Member (FM) and Doctoral School (DS). We examine the reasons for the issue observed in the operational process – the fact that some faculty members struggle in order to recruit a PhD student within this process. To formalize the regulative processes that support decision making for PhD recruitment, we define two statecharts: DS_ (Figure 9) and FM_ (Figure 11). These statecharts represent the appreciative systems of the Doctoral School (DS) and the Faculty Member (FM) respectively. They instantiate the model explained in the previous section. We add these statecharts to the initial model available (in Figure 1) as two separate regions. This forms the extended model that is illustrated in Figure 13. We present the list of events defined for DS and FM appreciative system models in Figure 14. The events defined for the operational model (Figure 4) remain unchanged. However, now we can distinguish between the events raised in the operational model and used as observations in the appreciative systems (gs_applyDS, gs_applyFM). The same applies to the responses raised in the appreciative models (Figure 3) and used as decisions in the operational model (ds_accept, ds_reject, fm_approve, fm_reject). The appreciative models explain how these observations are transformed into responses. 5.1 Norms and Thresholds of the DS and FM We begin by identifying the norms that affect the decision to accept a graduate student candidate by the DS and the decision to approve the graduate student candidate by the FM. For the doctoral school (DS) the norm can be formulated as follows: DS_Norm = “To ensure strong background of a graduate student candidate in the research areas defined by the school “ We define one aggregate variable v that corresponds to this norm and allows for assessing the graduate student candidate excellence. As an example, this variable can be the graduate student candidate’s GPA (Grade Point Average), or a score for some specific modules (Science, Mathematics, etc). The norm can then be expressed as GPA = 5. We assume that the threshold associated with this norm is 0.2, meaning that in some special cases a graduate student candidate with a minimum of 4.8 can be admitted as well. The threshold can be expressed as follows: GPA > 4.8 36 For the faculty member (FM) the norm is to ensure team cohesion. This rather abstract norm can be lead to two more concrete norms: FM_Norm 1 = “motivated graduate student candidate” and FM_Norm 2 = “Graduate student candidate with relevant skill set.” For the FM, we define two variables that correspond to this norm: the graduate student candidate’s motivation for a particular subject defined by the FM and the skills developed in the relevant area. The thresholds can be expressed as follows: “the graduate student candidate expresses interest in the subject (e.g. in her motivation letter or interview)”; “the graduate student candidate has at least 2 years of experience working in the area.” In the next sections, we explain this transformation of observations into responses in detail for the DS and FM. 5.2 Statechart Model of the DS Appreciative System The statechart model of the DS appreciative system is shown in Figure 9. Step 1: DS’s Reality Judgment In the RJ state, we define an event (an observation) that triggers the regulating process. For the DS, it is the gs_applyDS event raised in the operational model (Figure 13). This corresponds to the GS application for the doctoral school. Once this event is captured in the RJ (i.e. a graduate student candidate application is received), the relative reality judgments are created (analogy of ri in Section 4.2). In the case of DS, this corresponds to the graduate student candidate’s GPA (e.g. based on her track of academic records). – Figure 9. DS_ the Appreciation statechart for the DS Step 2: DS’s Value Judgment We model two events that correspond to the positive and negative evaluation of the graduate student candidate with respect to the threshold specified above: isGpaOK is raised when the graduate student candidate’s GPA is within the threshold (>4.8) and isGpaNOK is raised when this threshold is crossed. Optimizing-balancing In order to understand how value judgments will be transformed into a response, it is important to refer to the context of the PhD recruitment process. We model this process based on the 37 assumption that the DS is searching for graduate student candidates. In other terms, there exists a norm related to a (desired) number of PhD students defined by the DS, and, regarding this norm, we are below some threshold (ex: #PhD > x). This is not shown in the model but embedded into the context – this is the reason why the university runs the PhD recruitment process and the DS participates in this process on the first place. Figure 10 illustrates how the evaluation of a graduate student candidate’s application (i.e. GPA) leads to a decision by the DS (i.e. optimizing-balancing): Figure 10. Optimizing-balancing for the appreciation statechart of the DS Here the graduate student candidates with GPA = 4.8 and higher are considered as an opportunity for the doctoral school to maintain the norm regarding the desired number of PhD students. Whereas the graduate student candidates with GPA < 4.8 represent a threat as, if recruited, they might fail to complete the PhD program. In the statechart in Figure 10 this optimizing-balancing is specified with the corresponding statements: DS_.isGpaOK / DS_.OP = DS_.OP +1; raise DS_.toAj DS_.isGpaNOK /DS_.TH = DS_.TH +1; raise DS_.toAj Step 3: DS’s Action Judgment In AJ, we define the responses that will be triggered based on the optimizing-balancing above: DS_.decision[DS_.OP> DS_.TH] /raise ds_accept; DS_.OP = 0; DS_.TH = 0; raise DS_.actSeize DS_.decision[DS_.OP < DS_.TH] /raise ds_reject; DS_.OP = 0; DS_.TH = 0; raise DS_.actAvoid In case of opportunity, the graduate student candidate will be accepted (raise ds_accept); and in case of threat the graduate student candidate will be rejected (raise ds_reject). Other response types (i.e. noChange or changeVj) are not defined for this model, since the optimizing and balancing is simple and based on one value judgment only. 5.3 Statechart Model of FM Appreciative System The appreciative system of FM is presented in Figure 11. Step 1: FM’s Reality Judgment In the RJ state, we define an event (an observation) that triggers the regulating process. For the FM, it is the gs_applyFM event raised in the operational model (Figure 10, Figure 11). This corresponds to the GS application sent directly to the faculty member. Once this event is captured in the RJ, the relative reality judgments are created (analogy of ri in Section 4). In the case of FM, this corresponds to the graduate student candidate’s Motivation (based on her motivation letter) and Skill (based on her CV). 38 Step 2: FM’s Value Judgment We model four events that correspond to the evaluation results of these variables: isMotivOK is raised when the graduate student candidate does express the interest to the subject in her motivation letter (see Section 5.1) and isMotivNOK in the opposite case. Idem, isSkillOK is raised if the graduate student candidate’s CV shows 2 or more years of experience in the relevant area and isSkillNOK is raised otherwise. Figure 11. FM – the appreciation statechart for the FM Optimizing-balancing As above, it is important to refer to the context of the PhD recruitment process. We make an assumption that the faculty member (FM) is interested to recruit a PhD student in her team. Thus, there exists a norm related to a (desired) number of PhD students in the team defined by the FM, and, regarding this norm, we are below some threshold (ex: #PhDTeam > y). This is not shown in the model but embedded into the context and this is the reason why the FM is participating in the PhD recruitment process on the first place. Figure 12 illustrates how the evaluation of a graduate student candidate’s application (i.e. Skill and Motivation) leads to a response by the FM (i.e. optimizing-balancing): Figure 12. Optimizing – balancing for the appreciation statechart of the FM We model a FM who gives a clear priority to the graduate student candidate skills: isSkillOK contributes 3 for the opportunity and isSkillNOK contributes 4 to threat. Motivation, in contrast, 39 is necessary but not sufficient condition for the graduate student candidate to be approved: a good motivation alone does represent neither threat nor opportunity for the FM. This is represented by an o-b factor for isMotivOK = [0, 0]; and the o-b factor for isMotivNOK = [0, 2]. In the statechart in Figure 11 this optimizing-balancing is specified with the corresponding statements: FM_.isMotivOK/ raise FM_.toAj FM_.isMotivNOK/ FM_.TH = FM_.TH +2; raise FM_.toAj FM_.isSkillOK /FM_.OP = FM_.OP +3; raise FM_.toAj FM_.isSkillNOK /FM_.TH = FM_.TH +4; raise FM_.toAj The actor may choose to evaluate two variables or only one prior to define a response. Example: the FM receives the application and evaluates the motivation first. If motivation is not convincing (isMotivNOK) – the optimizing-balancing will terminate with OP = 0, TH = 2 and the response will be to avoid the threat. However, if the FM proceeds with evaluation of skills (i.e. adds another value judgment) and finds them satisfactory (isSkillOK) – the optimizing-balancing result will be: OP = 3; TH = 2 and the response will be to seize the opportunity. The optimizing-balancing factors affect the way decisions are made. Further details on this subject, however, are out of the scope of this article. Step 3: FM’s Action Judgment In AJ, we define the responses that will be triggered based on the optimizing-balancing above: FM_.decision[FM_.OP == FM_.TH && FM_.OP>0] / FM_.OP = 0; FM_.TH = 0; raise FM_.changeVj FM_.decision[FM_.OP > FM_.TH] /raise fm_approve; FM_.OP = 0; FM_.TH = 0; raise FM_.actSeize FM_.decision[FM_.OP < FM_.TH] /raise fm_reject; FM_.OP = 0; FM_.TH = 0; raise FM_.actAvoid FM_.decision[FM_.OP == FM_.TH && FM_.OP==0] /raise FM_.noChange In case of opportunity, the graduate student candidate will be approved by the FM (raise fm_approve); and in case of threat the graduate student candidate will be rejected (raise fm_reject). The example above shows that the decision depends on the valuation process. We find it important to highlight the complexity of the valuation process and its subjectiveness since it is related to the social nature of the appreciation as a whole. We will study this process in our future work. 5.4 Appreciative and Operational Models Working Together Figure 13 illustrates the three actors’ operational model and appreciative model of the PhD recruitment process. The operational model can be considered as the technical perspective and the appreciative model as the social perspective. The appreciative models illustrate how the decisions regarding accepting, approving and rejecting graduate student candidates are taken and how they are related to the social norm held by the corresponding process actors. Extending the operational model with the appreciative model allows one to separate the decision-making process from the decision results observed in the process and to reason about the process efficiency. 40 isGpaOK isGpaNOK gs_applyFM, gs_applyDS Operational Appreciative isMotivOK ds_accept, ds_reject isMotivNOK isSkillOK fm_approve, fm_reject isSkillNOK Figure 13. Extended model of the PhD recruitment process. Interactions between the operational model and the appreciative model are illustrated with bold colored arrows showing the corresponding “in” and “out” events. 5.5 Model Simulation Simulating the extended model in YAKINDU allows for visualizing the process scenarios as in the operational process model. As above, some of these events need to be raised by the user, using the simulation interface, and some are automatically raised by the statechart. The DS_ and FM_ statecharts (representing the social perspective in Figure 13) capture the two events raised in the operational process: gs_applyFM and gs_applyDS. These events trigger the regulative processes at FM_ and DS_ respectively as explained in Sections 5.2 and 5.3. During the regulative process, DS_ and FM_ make value judgments isGpaOK/NOK, isSkillOK/NOK, isMotivOK/NOK respectively. In the operational perspective DS, FM and GS statecharts capture four events raised by the DS_ and FM_ in response: ds_accept, ds_reject, fm_approve, fm_reject. These events affect the formal process the same way as in the models in Figure 1, Figure 2 and Figure 3. Figure 15 – (a) illustrates the configuration of the statechart that corresponds to the configuration in Figure 5 (a) – after the user triggers gs_applyDS event from the simulation window. The configurations of the DS, FM and GS statecharts that correspond to the operating model of the PhD recruitment process do not change. The FM_ statechart (bottom-right) is in the starting state (RJ). The DS_ statechart (top-right) is in the VJ state: gs-applyDS triggered this value judgment. From this configuration, the next step can be to “simulate” the appreciative system of DS (DS_). To do so, the user needs to “evaluate” the graduate student candidate application and to raise isCandOK or isCandNOK event from the DS_ interface of the simulator window (Figure 15 (b)). 41 interface DS_: in event toVj //-automatic, raised when RJ is created in event toAj //-automatic, raised when VJ is made in event addVj //-raised by the user to make a new VJ in event decision //-raised by the user when decision making is finished // response types - automatic, based on optimizing-balancing in event noChange in event actSeize in event actAvoid in event changeVj //value judgments - manual, raised by the user in event isGpaOK in event isGpaNOK //Optimizing-balancing var OP: integer var TH: integer interface FM_: in event toVj //-automatic, raised when RJ is created in event toAj //-automatic, raised when VJ is made in event addVj //-raised by the user to make a new VJ in event decision //-raised by the user when decision making is finished // response types - automatic, based on optimizing-balancing in event noChange in event actSeize in event actAvoid in event changeVj //value judgments - manual, raised by the user in event isMotivOK in event isMotivNOK in event isSkillOK in event isSkillNOK //Optimizing-balancing - set up automatically var OP: integer var TH: integer internal: //Process events: interaction between the Operational and Social process perspectives event gs_applyDS //- From Operational (GS) to Social (DS_); raised by the user event gs_applyFM //- From Operational (GS) to Social (FM_); raised by the user event fm_approve //- From Social (FM_) to Operational; (raised automatically) event fm_reject //- From Social (FM_) to Operational; (raised automatically) event ds_accept //- From Social (DS_) to Operational; (raised automatically) event ds_reject //- From Social (DS_) to Operational; (raised automatically) event notify_GSrecruited //- raised automatically These events represent the actual Figure 14. The list of events from the extended process model. judgmen ts made in the mind of the actor. During the simulation, the corresponding events have to be triggered manually. The decision about accepting or rejecting the graduate student candidate will be selected automatically by the simulator from the list of statements defined in the AJ state of DS_. Once the DS_ statechart is in the AJ state, the user can raise the “decision” event from the DS_ interface of the simulator window. The decision will be executed automatically based on the optimizing and balancing result, as explained in the previous section. Thus, ds_accept or ds_reject events will be raised automatically. The statecharts representing the operational process will react upon it accordingly: in the first case, gs_applyFM will be raised and the evaluation of the graduate student candidate by FM will be simulated; in the second case the graduate student candidate will be rejected and the process will stop. 42 DS_ interface (b) (a) Appreciative- Operational: interaction Figure 15. Simulation of the extended process model with YAKINDU 5.6 PhD Recruitment Efficiency: Discussion – Explaining the Gap The VJ states of the appreciative systems of FM and DS show that the overall decision depends on the evaluation of three variables: graduate student candidate background (by DS), her motivation and skills (by the FM). The PhD recruitment process efficiency is related to the capacity to provide equal opportunities for the university professors to recruit doctoral students. Thus, technically, all FM should have the same chances to recruit a PhD student within this process. Considering this extended model, we can see that this is only possible if the variables measured in VJ states by DS and by FM are independent. The fact that some professors cannot recruit the PhD students through this process can be explained by existing (implicit) dependencies between these variables. We illustrate these dependencies with the following examples: Example 1: The student with a strong technical background often finds the subjects offered by the FM “too soft.” In the model we would observe this as follows: the evaluation isGpaOK made by DS in the VJ state of DS_ will often correlate during the simulation with the evaluation isMotivNOK made by FM in the VJ state of FM_ resulting in TH=2 for a given graduate student candidate. (Unless the skills of the candidate are good – she will not be approved by the FM.) Thus, the FM will rarely approve the graduate student candidates accepted by the DS. Example 2: The students with a strong technical background most of the time have insufficient skills for the subject as defined by the FM (i.e. the subject requires skills in social or management sciences). In the model we would observe this as follows: the evaluation isGpaOK made by DS in the VJ state of DS_ will often correlate with the evaluation isSkillNOK made by FM in the VJ state (TH=3) for a given graduate student candidate. Thus, the FM will rarely approve the graduate student candidates accepted by DS. 43 Example 3: The student with a strong motivation and skills for the subject often has insufficient technical background as defined by the DS. In the model we would observe this as follows: the evaluation isSkillOK and isMotivOK made by FM in the VJ state of FM_ will often correlate with the evaluation isGpaNOK made by DS in the VJ state of DS_ for a given graduate student candidate. Thus the graduate student candidates approved by the FM will be often rejected by the DS. The examples above describe the specific situations that are nevertheless common for professors working on transversal subjects. They show that the norms the process participants hold do not fit all participants. DS_Norm =“To ensure strong background of a graduate student candidate in the research areas defined by the school” FM_Norm =“To ensure team cohesion through recruiting motivated graduate student candidates with relevant skill set” Ensuring strong background may run contrary to the cohesion of the FM team (while possibly increasing the background of all graduate students). We can observe here the balancing act between the interest of the individual (i.e. the faculty member) and the group (i.e. the doctoral school). This is the essence of social systems, creating and preserving social groups through social norms. 6 Discussion on the Formalization of Social Systems In this article we present how we can model the social perspective and the technical perspective of a business process. One challenge with the social perspective is that humans or living beings, can adapt and have intuitions. These are very challenging to formalize, and probably – luckily – yet impossible. The model presented in this article can be used to simulate predefined scenarios. We model appreciative settings that do not change and we rely on an environment that is stable. Few thoughts on the limitations of our approach are as follows: On reality judgments: We do not discuss how the actor perceives the environment (reality judgment), uses her intuition and how she imagines disrupting the environment (value judgment and action judgment). This is out of the scope for our current work but is the key to model more sophisticated ways to adapt. On value judgments: We model the regulative process where the actor will need to change her appreciative setting (either reality judgment, or value judgment or optimizing-balancing factors). During the simulation, we demonstrate that the actor can decide to add a new value judgment or to change the existing one (“change her mind”). However, the current model does not explain how appreciative setting can be changed. In the current version, changing of optimizing-balancing factors and reality judgments requires model redesign and cannot be done at run time. On action judgments: In any social group, there will be numerous relationships between appreciative systems and their underlying norms. In a similar way, the choice of our response (in action judgment) may require an interaction with more than one operational process. Another appreciative system can model this choice. Different operational processes can be chosen or constructed following the response strategy. The response strategy (e.g. act and seize or act and avoid) is based on the actor’s understanding and anticipation of the environment. However, we do not show how the actor is gaining this knowledge. On formalizing social behavior: The model we have shown is a mechanistic simplification of a much more sophisticated way in which social groups behave. Still, it allows rationalizing our behavior instead of just ignoring all social and psychological aspects. On statecharts and YAKINDU: state-transition paradigm to reason about both technical and social perspectives of a business process. This paradigm is well known for modeling systems 44 behavior, whereas process models most commonly follow the activity-oriented paradigm, representing a process as a predetermined sequence of activities. Within the state-oriented paradigm, the process scenario is specified with as a sequence of events; the concrete activities that will produce these events remain implicit. Examples of state-oriented modeling formalisms include state machines in UML [18], generic state-transition systems or state machines, such as FSM [7] or Petri Nets [19], and statecharts by D. Harel [6] created for the specification and analysis of complex discrete-event systems. Harel’s statechart and the YAKINDU statecharts modeling tool are proposed for modeling scenarios for decision making support for knowledge- intensive, weakly structured processes in [20], [21]. 7 Conclusion In this article we show how to extend business process modeling by using appreciative systems. We define an operational model (the statechart equivalent of a BPMN model) coupled with an appreciative model (statechart equivalent of the mindset of participants). Using the appreciative system in the context of BPMN has triple interest: 1. For process engineers: we take into account not only a process and its context, but a social system formed by the process participants and their (personal and professional) contexts. This gives an opportunity to discover more complex scenarios and to anticipate the sources of actions with undesirable (and for some processes, catastrophic) consequences. Safety critical processes, activities associated with risk taking, can benefit from this socio-technical approach for process modeling and analysis. 2. For process participants: the appreciative model provides an opportunity to think about what people perceive (reality judgment) their values (value judgment) and their response strategy (action judgment). Identifying these elements, the process participants can better cope with the stress encountered during their activities. 3. For process managers: understanding of (or at least being aware of) the appreciative systems of process participants can help the process managers to anticipate and understand the source of conflicting situations. The process manager can be better equipped to propose a conflict resolution strategy. We have specifically shown that the social perspective can explain the problems related to the operational processes in the organizations and, potentially, can help anticipate issues due to conflicts of norms, and to resolve observed problems by showing elements to take into considerations when redesigning these operational processes. Our work can be placed in the mid-way between the social sciences and engineering. In the social sciences, the problems of cognition, decision-making and social interactions of an individual are addressed in a much more elaborated manner than we attempt in this article. Engineering, in contrast, tends to address the same issues in a much more pragmatic and, sometimes, oversimplified manner. Our goal is to find an appropriate level of complexity and to bring the values of the social sciences to engineering. This article illustrates how actors can maintain their stability with an appreciative setting that does not evolve (e.g. no new judgments). The goal of the appreciative model is mostly pedagogical, nevertheless we assume that it brings value to process engineers, process participants and managers by making them aware of the mechanisms at stake.",
  "2020-KRNRW": "1 Introduction Research in Information Systems (IS) Engineering has resulted in so many meth- ods, ontologies, theories, models, or languages, that much effort has been ex- pended in trying to reconcile them. The trend is somewhat to try to reach a sin- gle true correct ultimate view over a socio-technical system. Inescapably, though, every attempt at reconciliation creates yet another artifact (e.g., method, ontol- ogy, language). IS engineering researchers are schooled mostly in the predom- inant positivist tradition where a method’s ontology must represent reality as closely as possible. This is similar to the way the law of gravity is a true repre- sentation of reality on earth, and to the way its value must be defined as closely as possible to match observations that are the same regardless of the observers’ culture and context. This objective observation of reality applies poorly to the socio-technical organizations where IS research is or should be conducted [6]. In a field where an objective reality is not shared among the researchers, the quest for a common ontology is futile. As engineers, we perceive complexity as a phenomenon that has to be broken down into smaller pieces that then have to be weaved back together, or as Jackson points out; “Having divided to conquer, we must then reunite to rule” [27]. This engineering tradition has made its way into IS research, for example, through design with viewpoints [32, 44], hierarchies of ontologies with domain, upper, core, and foundational ontologies [5, 18], model- driven system design (with UML, for instance), and business and IT alignment with the help of Enterprise Architecture methods [65]. These efforts point in the same direction: that we should analyze a system from many points of view but then synthesize a single one that represents the single true comprehensive view of the system under consideration. Some researchers trace these tendencies to the days when the models of computer systems ultimately had to be represented in machine code as a single source of truth [27]. We challenge the assumption that in the context of IS Engineering for socio- technical systems it is necessary, or even possible, to reach a single representa- tional format (methodology) that can unite all perspectives. Agreeing to disagree seems to be a better path. The basic assumption of the interpretivists is that all ontologies, methods, and theories are valid and useful in their given context because they are the product of a point of view of an individual or a group of people. From an interpretivist perspective, it is impossible to introduce a point of view that will invalidate, disprove, generalize, replace, or subjugate the oth- ers, as it will be yet another point of view that has no more (and no less) value than any other, except for the individual or the interpretation context from which it originates. Instead of describing how to map, merge, and reconcile to a sin- gle point of view, we analyze these efforts through the interpretivist research paradigm to show a different perspective as to why we, as method designers and method users, do so. We define reconciliation as an agreement and a shared understanding that might only exist momentarily then disappears as the people’s world views, uniquely shaped by their experiences, begin to diverge again. Once an agree- ment is achieved, it is likely to dissolve as time goes by unless it is maintained, just like any system subject to external and internal change. This maintenance is important because in organizations (i.e., socio-technical systems) a lack of any agreement will lead to chaos and to the eventual demise of the organization as a single entity maintaining its identity. Therefore, organizations strive to pre- vent major disagreements from happening, by repeated and frequent exercises of reconciliation, explicit or implicit. We propose a set of heuristics inspired by our experience in reconciling the modeling and design methodologies created by our own research group and other methods [54]. These heuristics are based on systems-thinking principles and are independent of our methods. The main take-away is that to reconcile different points of view, it is useful to go beyond the immediate ontology and to under- stand the differences in all epistemology, axiology, and ontology, thus forming a trilogy that together forms a world view. The structure of the paper is the following. In Section 2, we review existing literature to understand better the reconciliation domain. Then, we present the set of heuristics in Section 3. We illustrate the use of the heuristics in Section 4 with an example. In Section 5, we discuss our findings; and in Section 6, we reflect on the limitations of our research findings. We conclude in Section 7. 2 Problem Statement We inquire into different fields of study that propose their points of view about what is to be conceptualized, hence, modeled conceptually within the process of analysis and design of IS. The term point of view refers to different concepts (e.g., ontology, methodology, framework, model, language) in the different fields. We use the term point of view to avoid terminology confusion due to overloading constructs that are already in use in the literature. As a result, we reconcile our point of view with others by introducing yet one more point of view. First, we look into the early work on ontology from the field of artificial intelligence for knowledge representation and sharing that defines ontology as “a specification of a “conceptualization”” [15, 16]. Later Guarino and Giaretta clarified the definition of ontology for it to also be a “synonym of conceptualiza- tion.” [17]. Most attempts to reconcile ontologies, however, assume the former definition by Gruber because of the implicit assumption that there is just one possible conceptualization [24]. The single-conceptualization assumption leads to the goal of explicitly reaching a single specification/ontology. There are two main approaches to reconciliation: (1) refinement and abstrac- tion and (2) alignment (sometimes called matching [12]). With the help of refine- ment and abstraction, models of different levels of detail can be (de)composed into more or less detailed ones, with the help of formally defined semantics [1]. Refinement has been used in multiple studies and is one of the main principles in computer science: for example, going between high-level specifications and for- mally verifiable specifications [28], value models to business process models [26], and user stories and backlog items [40]. The refinement relationship is a seman- tic one and can rarely be fully automated. Alignment is used, for example, in ontology mapping for web services and semantic web data representations [43, 11]. Alignment deals with semantic heterogeneity and with semantically related entities in ontologies [12], recently extended towards the term semantic interop- erability [19]. For an illustrative example of reconciliation in ontology-based IS research (without it being called reconciliation), we take the work by Nardi et al. [42] who propose an ontological solution for the service domain. The ontology they propose, UFO-S, is a reference ontology for services [42]. UFO-S is based on the Unified Foundational Ontology (UFO) [18, 20–22]. UFO has three modules: (1) UFO-A that covers endurants (objects) [18], (2) UFO-B that covers events (perdurants) [20, 21], and (3) UFO-C that covers social entities [20, 22]. UFO-S, on its own, is built in multiple parts for the different phases of a service life- cycle: service offer, service negotiation, and service delivery. UFO-S is a reference ontology. It is not as general as a foundational ontology and not as specific as a domain ontology. Hence, even using UFO-S means that for a domain of application (or an interpretation context), a modeler would have to introduce another conceptualization for their particular case. Ontological work is not the only example where the phenomenon of reconcilia- tion between methodologies, theories, ontologies, conceptualizations on different abstraction levels exists. Zachman and later Sowa and Zachman proposed an overarching framework (in essence, a matrix model) that describes an IS archi- tecture in terms of the fundamental questions (what, how, where, who, when, and why) and discipline-dependent views [65, 49]. The Zachman framework was initially thought of as representing everything there is to represent about an organization and its IS. In the subsequent 30 years, we saw the development of numerous enterprise methods and frameworks (e.g., TOGAF [25]) that led to a “jungle of enterprise architecture frameworks” [48]. The creators of these later methods and frameworks introduce points of view that, in their intended con- text, are as valid and useful as Zachman’s. This shows that whatever framework that is supposed to describe everything will be superseded by others. We believe that the assumed problem that research communities might be trying to solve is the lack of a single methodology. There is an implicit belief among researchers that there must exist such a single point of view and that it is the ultimate one. Here, we put forward the idea that having all these points of view is not a problem to be solved per se. On the contrary, it shows that there is no established status quo rather mostly disparate schools of thought. These different opinions are valid and valuable for us to be able to express the nearly unlimited points of view that exist when we design an IS. In his seminal work on the nature of scientific revolutions, Kuhn observed that in the preface of a “scientific status quo” before everyone in a domain agrees on something (shared understanding), everyone has to define their own universe and to start from the beginning because there is no common ground to be building upon [34]. We strongly believe that method designers and method users will always strive to reach an agreement. Hence, we propose some heuristics for guiding the inevitable reconciliation efforts that will continue to occur in the academic and the industry domains of IS Engineering. 3 Systems-Thinking Heuristics for Reconciliation The point of departure for our heuristics is interpretivism. Interpretivism is a philosophical paradigm that regards meaning as an emergent property of the relationship between an observer and their reality [39, 6]. The use of interpre- tivist methods in the IS research domain has been usually used and discussed in juxtaposition to the positivism paradigm with its core belief in an objec- tive, observer-independent reality [50, 6]. The relationship of the observer with a reality out there helps interpretivists avoid the trap of solipsism, where every observer has their own reality with no connection with other observers [39]. The shared reality between observers helps them to create a shared meaning, which is a social construction. According to Weick, in a socially constructed world, the conceptualizations we hold (“the map”) creates the reality we see (“the ter- ritory”) [59]. Hence, co-constructing their conceptualizations makes sense to a group of people who share a similar experience. In our conceptualization (which is simply another point of view), ontology is the most visible part of the observer’s worldview that is called systems philos- ophy in [54, 46]. The other parts are epistemology and axiology. Epistemology roots the knowledge held by the observer about their reality, the hidden part of the conceptualization. Axiology is the choices the observer makes (explicitly or implicitly) about which entities to observe in their reality and those that will be included in their ontology. To attempt to reconcile ontologies at the ontological level is like trying to mix sugar and tea at room temperature. They do not merge well. One needs to heat the tea first. To reconcile ontologies, we need to under- stand the epistemology and axiology of the people who define the ontologies and try to understand their similarities and differences: what they agree and what they disagree on. This is usually called social construction. Heuristic 1. Reconciliation as a Process of Accepting Change. To reconcile points of view is to change their creators’ minds at the epistemological and axiological levels. We believe that one of the most difficult endeavors is to change people’s minds about deeply held beliefs. If it is possible at all, it usually takes time. For example, according to Haldane, there are four stages of acceptance (of a scientific theory): “(1) this is worthless nonsense, (2) this is an interesting, but perverse, point of view, (3) this is true, but quite unimportant, (4) I always said so” [23]. Heuristic 2. Just Enough Change. A corollary of the previous heuristic is that change must come in at just the right amount, not too little and not too much. If there is too little change, nobody will notice that a reconciliation has taken place. If there is too much change, the identity of the reconciled points of view will be lost for the observers. In some cases, the best course of action is to take a moderate approach to change. Or it can be best to take the most conser- vative option with an absolute resistance to change. And in some other cases for a system it can be best to reach out to high entropy states that disintegrates the identity. The latter option should not be neglected in consideration. In practice it happens as frequently as the former two. Heuristic 3. Requisite Variety. Requisite variety is a heuristic for study- ing the responses of a system to existing or future threats [2, 62, 47, 41]. Weick [60] shows that, for effective action in a situation with high ambiguity, it is necessary to maintain as many different points of view as necessary in order to “to grasp the variations in an ongoing flow of events.” All of them are valid for a context and all of them are necessary to maintain a requisite variety. For reconciliation, this means that researchers need to suspend their willingness to reduce the vari- ety in the points of view they seek to reconcile, until they have made sure that this variety is not needed in the domain they describe. Heuristic 4. Understanding the Philosophy of Each Ontology Cre- ator. As ontology is only the visible part of the world view of its creator, it is useful to instantiate a process of social construction in order to explore each creator’s epistemology and axiology. Going to the philosophical foundations, epistemology and axiology, enables us to see the source of our differences and to potentially reach a consensus. Staying on the level of only ontology lacks se- mantics and prevents us from understanding what it actually means to agree or disagree. Staying on the level of only epistemology lacks syntax and a concrete form that we can act upon. Let us take an example and ask ourselves, “Is a tomato a fruit or a vegetable?” The tomato, as a sign, can be related to either depending on the classification we use. A way to understand which classification to use, with the use of epis- temology, a representation can be connected to the contexts → “I’m at home, mom told me tomatoes are a vegetable”, “I’m at school, the teacher told us that tomatoes are a fruit.” With the use of axiology, the observer can chose the “right or good” context, once this context is identified → “For dinner, we don’t put tomatoes in the fruit salad.”, “On the test, I should mark tomatoes as a fruit.” Hence, in our work, ontology is used in the broad sense to signify the multiple pomodoro1 ways with which we can represent the given concepts (tomato and are two ways of naming a round red plant). Epistemology enables us to relate the conceptualization to contexts. And axiology enables us to reason about ethical choices (e.g., about good and bad, beautiful and ugly) as well as about moral values. These definitions are inline with the systemic paradigm as proposed by [4] and used in our own work [54, 46]. Heuristic 5. Practicality. In practice, there are reconciliation techniques (alignment, refinement) that have their trade-offs, we can understand each and apply whichever makes sense. Both refinement and alignment are well-recognized ways to reconcile ontologies and models. We can achieve alignment through in- troducing a new entity (fruit) that a “reconciled” point of view includes because it has some basic properties (attributes) that two or more other entities from different models (e.g., apple, tomato) have in common. With the help of general- ization, this alignment will give us one more point of view that departs from the specific context of the other points of view. The resulting models will not con- tradict in the cases where they do not share interpretation contexts. In the cases where these models share interpretation contexts, conflicts of interpretations are possible. In case of conflicts, generalizations of this sort will resolve the conflicts on a more generic level of interpretation, but on the specific levels the conflicts will remain. For example, if we use an algorithm to use an instance of a type Fruit in our fruit salad, yet a tomato is treated as a fruit at home, there might 1We could interpret pomodoro as pomo d’oro, meaning a golden apple. Thus, the tomato becomes a golden apple, if we only look at the representation (ontology) of methods. We anecdotally call this heuristic the “Golden Tomato” heuristic. be a conflict as the generalization abstracted away the context of interpretation (tomato is a fruit in class but not a fruit at home.) Heuristic 6. Duration of an Agreement. Nothing lasts forever, but some things last longer than others. We need to make sure to know what is being insti- tutionalized/cemented in our systems through automation. Some reconciliations persist longer than others. For example, an agreement to map the ID field from Database 1 to the field PersonalID from Database 2 could be done on a white board and could be stable only for a few hours while the discussion continues. Or it could be a longer-lasting reconciliation that has been institutionalized by scripting the mapping between these fields. In both cases, there is a reconcilia- tion, yet the level of automation is different. For implementing an IT system, we need to be able to come up with long-lasting agreements that we could codify in a specification, and then in code, thus, we could express in a formal verifiable form what is to be built (verification). Still, to ensure the validation of the sys- tem, we should not forget that the agreement is not final, and that the process is continuous. 4 Illustrative Example To develop a specification of what an information system (in a typical project, for example) would do, we can investigate the settings in which the system will operate and can reach an agreement from various stakeholders about the opera- tions that the system should support [66]. The views of people and what the as-is situation in the initial steps of the requirements process are usually a subject of analysis and design, with methods that apply to motivation, goals, sociology, psychology, etc. To express the IT specification, these views are taken as input in a requirements process that, at the end, yields a more precise description of the functionality of the IT system. The format of this specification is var- ied and of varying degrees of formalism: informal, semi-formal (UML diagrams, semi-structured specifications, user stories), formal e.g., (design-by-contract, for- mally verifiable specifications). To conduct a (socio-technical) system analysis and design, IS practitioners use different methods and tools. We use, for an example, a conference submission information system. We scope the example to only highlight certain specificities of the methods in use. We illustrate the use of our heuristics for relating a UML use case diagram (UCD) and user stories. The choice of these two methods is dictated by their widespread use, hence, by the fact our readers are likely to be familiar with the methods. The two models can be used in many methodologies to communicate between different stakeholders hence are versatile and applicable in many contexts. We also select these two because of the nature of their differences to also emphasize the variety of mediums in IS methods: a UCD is pictorial and a user story is text-based. A UML UCD models the functionalities of a system and the actors who use these functionalities [13]. We use the basic version of a UCD. We explore how to relate a UCD with a user story. A user story is a semi-structured way of expressing system requirements that originated in the practice of agile methods. A user story usually follows the following format: “As a user>>, I <<type of so that <<reason>>” [9]. User stories employ the vocabulary of the <<action>> system’s users/customers and have to be further refined into concrete technical specifications (known as Backlog Items). Figure 1 depicts a UCD that includes three actors: (1) reviewers, (2) authors, and (3) PC chairs. These three actors have a common ancestor actor: a user. The conference system has three functionalities (download a paper, assign papers to reviewers, upload a paper) that the actors use and two functionalities that the user has (register, login). The PC chairs and the reviewers share the ‘paper assignment’ functionality, whereas the authors use only the ‘upload a paper’ functionality. To construct a user story that is aligned or based on the UCD, we can relate only the ontology elements and create a user story such as: “As an author, I can upload a paper in the conference system.” However, without any interpretation and context, the first two discrepancies become apparent. (1) The author is related to a user. Does this mean that a user can upload a paper as well? It is not possible to show inheritance of an object in user stories. (2) In the UCD, there is no mention of why the author, or any other actor, would like to use any functionality. It is not possible to show the intentions of actors in use cases. If we are to meaningfully relate the two or to use them as complementary to each other, then the need for Heuristic 4 to use the episte- mology, axiology, ontology of methods is in place. Some authors have proposed ways to map the two models with the use of alignment and refinement Heuristic 5 [51] or annotated and extended one of the two [8, 52]. To illustrate Heuristic 6, the duration of the reconciliation proposed by our basic example is short-lived, whereas the rules coded in a computer-aided tool could last longer [51]. These works do not invalidate the original models, they give one more point of view (Heuristic 3 ) that can be used when we deem it to be more appropriate for the context of interpretation (for example, Dalpiaz and Sturm found user stories to be better fit for deriving conceptual models than use cases [10]). Moreover, this continuous generation of knowledge around UCDs and user stories is the process of reconciliation, which continues to occur (Heuristic 1 ). To illustrate our heuristics, we also reflect on a meta-level about our choice of points of view. The heuristic that we can highlight here is heuristic 3 on requisite variety that there are many points of view, many methods and models, download a paper reviewer register assign papers to reviewers user login PC chair upload a paper author Fig. 1. Use case diagram of a conference management information system that we could have used in our analysis. For example, only under the name UML are there various diagrams such as use case, sequence, class, responsibility, state diagrams [13]. All of them have their use and can complement others, or can be used by themselves. This can also be said for the other methods that we could have selected to “reconcile” the UML UCD with, for example a value-based method [14]. The other heuristic we would like to exemplify here is heuristic 1 on reconciliation being a process. In the initial iterations of our work, we (the authors) selected to relate methods that we created in our research. Yet, we decided to use better-known methods for the design and analysis of IS in order to introduce less changes (heuristic 2 on just enough change) and to focus mainly on the change that our heuristics could represent as a new point of view that the reader could potentially find easier to reconcile with. 5 Discussion Why these heuristics? The heuristics we describe here are tightly coupled with our experience of reconciling the methods we, as a research group, have created for the past 20 years and with our quests to relate to others [56, 36, 55, 31] and to connect different perspectives within our own methods [57, 46, 30]. However, throughout the literature search and given our understanding of the field of IS Engineering, we have seen the efforts of connecting different points of view being repeated as patterns. The reformulation of the problem of reconciliation as an ongoing practice is a new idea we put forward. Yet, much academic work has already mentioned their different methods being valid only in particular cases. Moreover, true to intepretivism, we believe we are also offering a point of view with our set of heuristics and not simply the set. We have seen other such heuristics, or principles (cf. [29, 64]). A future avenue for exploration is to consider the different sets of heuristics that exist in other contexts in the domain of IS and to reconcile the reconciliation heuristics. And even though no one set is exhaustive or all of its constituent principles are valid in every context, we subscribe to the view expressed by Klein and Myers that the “systematic consideration [of the set of principles] is likely to improve the quality of future interpretive field research in IS (especially that of a hermeneutic nature)” [29]. “Technology is neither good nor bad, nor is it neutral” [33]. One more method, with its corresponding ontology, epistemology, and axiology, is neither good nor bad, nor is it neutral. It is created for a context: as an aca- demic endeavor, with an objective to communicate or to share one’s experience with the others (i.e., researchers, students), or as a more pragmatic attempt to create a common ground with a group of people (experts from various domains) for discussion and analysis of a situation, or to find an agreement on a particular problem/solution. Ontology reconciliation will continue to take place. The ques- tion is, Are we, as researchers, cognizant that, by introducing more conceptual work, we are reconciling our point of view with the points of view of others thus creating a new point of view by interpreting through our own philosophy and our context of interpretation? more2? Are all points of view valid, yet some just a bit Any method- ology, with its corresponding methods, models, theories, and artifacts may (but, being subjected to fallacies, not necessarily has to) be valid and useful within a particular context where it is defined and used. Outside of that context ev- erything is possible: its validity and usefulness can theoretically be anywhere within the range “absolute–limited–nil”. Let us use Boltzmann’s entropy as an analogy: S = k · ln(W ), where W is statistical weight, and k is the number of possible state configurations within a statistically described thermodynamic system. If we now apply this analogy to methodologies, then we can say that within a social system we can find a set of these. Any of the methods defines a set of conceptual configurations that rely on valid conceptual states within the social system. Hence, any one from the set contributes to the statistical weight. A conceptually rich method will define a rich set of states and a conceptually poor method will define a poor set of states. Every state contributes to the sys- tem’s statistical weight. However, the contributions of each to the overall social system’s entropy might be unequal. The contributions are probabilistic. Applying the analogy of Boltzmann’s entropy, we might say that a partic- ular methodology, or a point of view, regardless of how conceptually rich/poor and probabilistically frequent/rare it is, contributes to the statistical weight of conceptualizations of the social system where it belongs. A contribution of a par- ticular methodology to the social system’s entropy depends on the probability of reaching the states of its conceptualizations within interpretation contexts of this social system. The more frequent the states are, the lesser the contribution to the system’s entropy is. A Word on the Context of Interpretation. A particular interpretation context defines its corresponding particular state of the system’s conceptual con- figuration. Any methodology that describes or models the system can be eval- uated with regard to how useful/useless it is for a description of this particular conceptual configuration. A richer ontology (with a higher language expressive- ness [27]) has a larger probability of being useful. An ideal ontology would cover all conceptual states of the system, even the high-entropy states. All ontologies are equally valid, as there is no one point of view which is superior or inferior to the other (for this there needs to be an objective observer of observers, or the so-called super observer, who does not exist [61]). If “all ontologies” is an un- limited set, then it is logically impossible to define a context where all of them will have the same degree of validity and usefulness. Hence, reconciliation or any effort to make an overarching ontology within a methodology is yet another point of view. Although it tends to reduce disorder (conflicts, variety, inconsis- tencies) by enabling actors to express their current beliefs regarding the state of the observed system, it cannot cancel, improve, or rule on the rest. An ontology can be perceived as independent of context. In this case, an ontology has (1) no dependency on a context to which it could be potentially applied in an attempt to describe the context, and (2) no dependency on an 2“All animals are equal, but some animals are more equal than others.” from Animal Farm: A Fairy Story, 1945 by George Orwell observer who could try to apply the ontology in order to describe some context. An interpretation context is a conceptualizable state perceived as existing or designed to exist within a system that is being modeled by someone (by an ob- server, by a modeler, by a group of people, etc). Multiple contexts can exist for a given system. Interpretation contexts are (1) dependent on observers/modelers, and (2) contain conceptualizable representations of some entities within the sys- tem, these representations can be attempted to be described by one or several ontologies. Different ontologies can be either more or less useful in their attempts to describe some representations within a particular interpretation context. The potential for a success or a failure of a methodology within a given interpretation context does not depend on a modeler who applies it rather on only the concep- tual richness or poorness of this methodology, with regard to the representation needs required by the context. If we seek to represent the state of a system, a data-flow diagram might not be the most informative [27]. Knowledge Representation for AI/ML: the Wave of Automation. There is a trend to connect data that have been generated independently by different sources to enable interoperability and uniformization of formats to ease data sharing [63]. Our set of heuristics could be classified towards the wave of semantic interoperability efforts [19]. Here we pause to pose the question, Should we aim for interoperability and all this uniformization of data formats? Enabling more analytics to be done on the data that we can connect, given that we have shown that irreversibly some of the context of generation of these data is lost in the (model-to-model) translation, might lead to unpredictable consequences. The advancement in semantic interoperability enables data from different sources to be cross-referenced hence to build representations of individuals and groups that could be seemingly labeled as context-rich, even though the context in which the data were generated is decoupled from any information system. There is an implicit assumption that it is better to allow these uses and to enable more automation through the use of AI/ML-powered systems that use these rich data sets, because humans are perceived to be the weak link in any system [3]. Yet, before we understand the technologies labeled as AI on”3 and ML, we should “tread softly because we tread uncharted territory of technology than can be employed for the automation of decision making that optimizes predominantly profitability of enterprises [35]. And even if we succeed with automating the human out of the process of translating between models and methods in the context of IS Engineering, the next question is, How and who will handle the mistakes that such automation would eventually lead to? According to Bainbridge, the human who would have to take control over the failure would have to be specialized and highly trained even more than the people whose tasks are being automated [3]. 3“Tread softly because you tread on my dreams.” from Aedh Wishes for the Cloths of Heaven, 1899 by William Butler Yeats 6 Limitations Reusability of our Findings. Positivist research aims to create objective, generalizable knowledge (e.g., laws of physics) that is absolute and can be reused independently from the context. In this work, we propose a set of heuristics that stem from the interpretivistic research paradigm. In other words, they are a product of our interpretation of our own experience in reconciling methodologies for design and analysis of IS. We reflect on the limitations of the interpretivistic research paradigm and pose the question, Is the validity of our findings in general an oxymoron [45]. We argue that the whole idea of IS research results as being general (or context-independent) should be taken with caution. Once the results are pre- sented as general, the researchers, who rely on the results or implement them in the context of their particular socio-technical system, can be absolved of any responsibility. Interpretivism, in contrast, makes the researchers and their view an integrated part of the research and its findings. Thus, it is the responsibility of a researcher to choose and reuse all or part of our heuristics in their context. Therefore, our findings are reusable, but not absolutely or objectively, they are subjectively reusable. Generalizability and Reliability of our Findings. One could argue that, if responsibility is in researcher’s hands, then what about the reliability or gen- eralizability of the results they produce? They would be inevitably biased. Ac- cording to the positivist research paradigm, the researcher is independent from the research; they provide objective observations / measures that guarantee the objectiveness of results. Any researcher, by reproducing the same experiment, should obtain the same results. This implies reliability. According to the inter- pretivistic methods, the researcher is a social actor, a part of a socio-technical system they study, and it is through their observations that the system to be studied emerges and its identity is created. Although, the observations are obvi- ously biased. We argue that the socio-technical system is a product of the biases of its actors. They are not a threat rather a part of the system’s identity and hence have to be explicitly taken into consideration [53]. A possible contradiction that could be found is between our interpretivistic approach and the very nature of Systems Thinking. Interpretivism shies away from generalizability, whereas Systems Thinking is an inter-discipline that con- nects other disciplines through general principles [61]. However, any general Sys- tems Thinking principle (or, in our vocabulary, heuristic) is a subject of inter- pretation and contextualization. Hence, we see the interplay between generally applicable principles and their context of application as being integral to con- structing a Systems Thinking body of knowledge, and that it can be applied throughout. Validity of our Findings. In positivism, the created knowledge is absolute and can be validated (or invalidated) analytically (by deduction) or through ex- periments with the use of falsifiable hypothesis. For example, the laws of physics are absolute. In qualitative interpretivistic research, the validity of knowledge can be demonstrated only within a given frame of reference [7]. This frame of reference labeled by some transactional validity defines “research as an interac- tive process between the researcher, the researched, and the collected data that is aimed at achieving a relatively higher level of accuracy and the consensus by means of revisiting facts, feelings, experiences, and values or beliefs collected and interpreted” [7]. Once the frame of reference changes, the knowledge can be invalidated. In the positivist paradigm, such a frame of reference is taken for granted by researchers as “something everyone agrees upon” hence is often omitted (implicit). This creates an illusion of an absolute or objective validity. In interpretivism, the frame of reference, the context or the socio-technical system, is a part of the research, a variable of the equation. It cannot be omitted, as we cannot claim that “everyone shares it”. Interpretivism leaves a researcher no choice but to explicitly mention their frame of reference (and to identify a com- munity that shares this frame of reference). Only within this frame of reference and for this community will the produced knowledge be valid. Bottom line: For some researchers, our findings could potentially be valid, but not “absolutely”; they are valid only within a given frame of reference. In the grand scheme of research pursuits, studies such as ours are natural precursors to a potentially better understanding of the field, that then through the accumulation of a critical mass of knowledge in the domain of systems design, these studies can be re-used in practice [58, 37]. Any academic pursuit that investigates a new or understudied phenomenon goes through stages of understanding: from chaos to heuristics to algorithms [38]. 7 Conclusion and Future Work In this paper, we have presented a set of heuristics for the reconciliation of methodologies for design and analysis in the domain of IS Engineering. We have presented some current literature on conceptual and ontological work as well as Enterprise Architecture to illustrate how different domains already accommodate various methods and models. We have put forward the idea of reconciliation as a recurrent practice in the context of IS scholarly and industry works in order to find place for the knowledge we generate. Our heuristics build on the notions of interpretivism, entropy, and well-known principles of computer systems design such as abstraction, refinement and alignment. We have explored the futility of reconciliation solely on the level of ontology and have proposed a way to look at differences on a philosophical level that includes epistemology, axiology, and ontology; but never only one. We have illustrated the use of our heuristics with the help of an example modeled with a use case diagram and user stories. We plan to explore and categorize further the epistemological principles that help us understand differences and points of intersection better, as well as to extend the reconciliation towards research artifacts. For the future, we will inquire into the reconciliation process and heuristics on the level of method users, as opposed to the level of method designers, whose perspective we explored in the current paper.",
  "2023-KRHS": "1 Introduction The supply chain is one of the most important economic systems[13] because it enables the production and delivery of goods and services to customers. In [15], the authors define supply chain as “the network of organizations involved, through upstream and downstream linkages, in the di↵erent processes and activities that produce value in the form of products and services delivered to the ultimate con- sumer”. Trust is a vital aspect with far-reaching consequences across various do- mains. In supply chains, trust plays a key role in shaping relationships between stakeholders: timely identification and elimination of trust issues is crucial for suc- cessful collaborations. Gambetta[14] defines trust as “the expectation that another person (or institution) will perform actions that are beneficial or at least not detri- mental, to us regardless of our capacity to monitor those actions”. Following this definition, a trust issue can be defined as a lack of trustor’s belief that another party (trustee), for one reason or another, will actually meet these expectations. In modern society, where interpersonal or inter-organizational relations are of- ten mediated by technology, trust becomes multidimensional: Mayer [27] defines 2 E. Kiomba Kambilo et al. trust between social entities (individuals or organizations), McKnight[10] speci- fies trust between humans and technology (Artificial Intelligence, Business intel- ligence), Andrew[31] discusses trust between humans depending on technology, Pietrzak et. al. [32] addresses digital trust as a determinant of interpersonal and inter-organizational relationships in the digital world. As a result, trust issues re- lated to digital security, privacy of data, process transparency and performance gain a lot of attention. Today, blockchain is considered a de facto technology to address trust issues in the supply chain. Blockchain is a distributed ledger system supported by a network of peers, each of whom maintains a copy of the ledger[28]. Blockchain is particularly attractive in supply chains due to its hacker-proof architecture and cryptographic algorithms(aspects), such as consensus algorithms that allow to verify and validate transactions on the network. In the context of SCM, this means that all parties involved in the supply chain can trust that the data recorded on the blockchain is accurate and has been agreed upon by the network. Additionally, blockchain can control access to information through smart contracts by defining specific conditions that must be met in order for the information to be accessed. These self-executing contracts, which are tamper resistant and traceable, can monitor the activity of each participant based on hash and signatures of each transaction[34]. Implementing blockchain-based solutions in SCM, organizations aim to address trust issues. However the success of this endeavor is contingent on the architectural model and implementation of blockchain technology. Software patterns provide a blueprint that software engineers can follow to solve a specific problem in a structured and e\u0000cient manner. Bushman[4] defines software patterns as “a function-form relationship that occurs in a context where the function is described in terms of unresolved trade-o↵s or forces in the problem domain. The form is a structure described in the solution domain that achieves a good and acceptable equilibrium among those forces.” Software patterns focus on capturing and systematizing successful experiences and techniques used in software development. Blockchain software patterns are discussed in the literature [35][39]. To the best of our knowledge, there is a lack of research exploring blockchain software patterns focusing on trust. In this paper, we investigate how trust issues expressed in the supply chain domain (problem domain) can be e\u0000ciently addressed by blockchain technology (solution domain) using specific blockchain software patterns. In this work, we develop the following contributions: 1) We construct a taxon- omy of trust issues in Supply Chain Management(SCM) based on the analysis of 18 research publications in the domain. 2) Following the guidelines of requirements engineering, we propose a technique for translating trust issues into trust require- ments. 3) We define the mapping between the formulated trust requirements and the blockchain software patterns. This mapping can guide decision-making in the design of trustworthy solutions in SCM. The paper is structured as follows: In Section 2, we provide the background on blockchain technology, software patterns, supply chain, and discusses the concept of trust. In Section 3, we define the taxonomy of trust issues in SCM based on related literature, than we translate these issues into trust requirements. In section Title Suppressed Due to Excessive Length 3 4, we discuss how the trust requirements from the previous section can be met by the specific blockchain software patterns. We illustrate our findings on the example of Section 5. This example also serves as a preliminary validation of our findings. In Section 6 we present our conclusions. 2 Background 2.1 Trust In social sciences, trust is defined as “the willingness of one party (trustor) to be vulnerable to the actions of another party (trustee), based on the expectation that the other party will perform the expected action”[27]. Social trust reflects (subjec- tive) trustor’s beliefs that the trustee has suitable at- tributes for performing as expected in a specific situation. These attributes include ability, benevolence and integrity [14]. Zheng [40] stated that social trust is a product of experiences and perceived trustworthiness. Advances in technology, such as Artificial Intelligence (AI) and robotics have led to the need for organizations to establish processes to regulate trust in technol- ogy [9]. Trust in technology can be defined as trustor’s confidence in technology (trustee) to accomplish the task at hand. In [10], the authors present three essen- tial elements that can help build trust in technology: reliability, functionality, and helpfulness. Digital trust emerges in interpersonal or inter-organizational relations where technology plays a role of mediator. Je↵rey(2020) defines digital trust “as the confidence users have in the ability of people, technology, and processes to cre- world”1. ate a secure digital Trust in technology is a precursor to digital trust, as people must trust technology before using it between them. 2.2 Blockchain in Supply Chain Management Supply chain management (SCM) aims to ensure that goods and services are de- livered to consumers promptly, cost e↵ectively, and e\u0000ciently[15]. In SCM, trust plays an important role. Tradelens[19] is one example of blockchain in SCM. It provides transparency, e\u0000ciency, and accountability in global trade by digitizing and streamlining the flow of information and documents among supply chain par- ticipants. Another example of practical applications is traceability that can be provided through a blockchain solution [21]. Blockchain is a decentralized, distributed ledger technology widely recognized as a critical enabler for the secure, transparent, and immutable tracking of transac- tions [36]. Blockchain has several intrinsic features that make it relevant for supply chain management. It can create a decentralized and tamper-proof ledger of all transactions that occur throughout the supply chain. The ledger could track the movement of goods from their origin to their final destination, providing complete transparency and traceability. It can also help increase supply chain data’s integrity by enabling monitoring and auditing of all transactions. Smart contracts can be 1 https://www.techtarget.com/whatis/definition/digital-trust 4 E. Kiomba Kambilo et al. used to automate specific processes within the supply chain. This can improve e\u0000- ciency and reduce the risk of human errors. Finally, blockchain can be used for the real-time identification of goods, particularly for perishable goods with a limited shelf life. 2.3 Software patterns Pattern-based design is widely adopted by the software engineering community since the mid-1990s. The resulting software patterns describe recurring designs used in software development [4]. A software pattern is considered as “a function- form relation that occurs in a context, where the function is described in problem domain terms as a group of unresolved trade-o↵s or forces, and the form is a structure described in solution domain terms that achieve a good and acceptable equilibrium among those forces.”[4] According to [39], software patterns play a vital role in addressing trust issues. Blockchain software pattern is a repeatable design solution to a recurring problem in blockchain development [35]. In SCM, blockchain software patterns can provide a systematic way of tackling trust-related concerns, such as ensuring data authen- ticity and integrity, promoting transparency and accountability, and maintaining the privacy and security of the system. In [35], authors identify a set of 120 unique patterns. 104 of them have been classified as design patterns, 3 of them as architectural patterns, and 14 as idioms. These blockchain software patterns come from a range of fields, including agri- culture and industries and address a number of generic issues. In this work, we review the software patterns in [35] and identify twelve patterns that can be used to address the specific trust issues in SCM. 3 From Trust Issues to Trust Requirements in SCM A trust issue refers to a challenge, problem, or disagreement that a↵ects the level of trust between individuals or parties[22]. They can be grounded on explicit evidence (frauds, contract violations, bad user experience) or on implicit beliefs. They are subjective and hard to grasp. In order to be explicitly analyzed and addressed by the software solutions, trust issues need to be translated into requirements. A requirement is a statement which translates or expresses a need and its associated constraints and conditions[1]. In this section, we define a taxonomy of trust issues based on our analysis of related literature and translate these issues into trust requirements. 3.1 Taxonomy of Trust Issues in SCM The work in [30] presents a methodology for building a taxonomy and discusses problems associated with taxonomy development. To establish our taxonomy, we define the following research protocol: Title Suppressed Due to Excessive Length 5 1 Identification: We conducted the search for primary research publications in the two major databases: Scopus and Google Scholar. We used the following key words: supply chain, trust, issues, requirements(38 papers for Scopus and 52 google scholar). 2 Selection: We selected the articles on the literature that met the following criteria : C1: Evoke the issues related to social, digital trust or trust in technology C2: Propose a solution that addresses trust issues or aims to improve trust in SCM. We identified (non-systematically) 18 research studies published between 2018 and 2022 (we filter by date to ensure the research is consistent and avoid irrelevant or outdated papers.) by screening abstracts and full texts. 3 Extraction: We extracted two types of text evidences (a) evidence evoking (explicitly and implicitly) trust issues and (b) evidence on the proposed technological solutions, indicating technological, architectural, design choices. 4 Synthesis: The extracted data was revised and discussed by several researchers (authors of this paper) to reduce the interpretation bias. Eventually the 21 extracted trust issues were grouped into 7 categories to form a taxonomy (Fig 1). We applied the protocol to identify and formulate trust issues in all selected sources. Here is an example: In [37], the authors highlight the di\u0000culties faced in Supply Chain Management in verifying the authenticity of goods and conducting investigations into illegal activities. Three main issues are identified as “insu\u0000cient auditing”, “opacity - lack of transparency” and “lack of oversight”. Another study by FranCasino[6] sheds light on the problem of a single point of failure in traditional databases, which can compromise privacy and tamper-proofing. We identify issues of “storage” and “privacy,” etc. Figure 1 groups trust issues extracted in the literature into seven categories, “Traceability” focuses on the challenge of tracking and monitoring products in real time along the supply chain. “Cost Control” addresses the stakeholders’ concern about reducing costs associated with blockchain transactions(cost limit and cost reduction). It is essential to minimize the cost of these transactions to ensure their practicality. “Lack of Auditability” highlights the di\u0000culties in auditing blockchain transactions, and stakeholders need to be able to audit them at any time. “Se- curity” emphasizes the importance of keeping data and transactions confidential, secure, and tamper-proof. “Data Governance” concerns the users’ control over data shared with other institutions and the need to anticipate scalability to avoid addi- tional fees. “Lack of Accountability” highlights the responsibility of stakeholders to be accountable for their actions, as it is essential for everyone to take responsibility for their decisions and actions. The final category, “Acceptance” that concerns the user acceptance. 3.2 Translating Trust Issues Into Trust Requirements. Trust issues are subjective and sometimes stem from stakeholders’ intentions. Defining requirements starts with understanding the stakeholders’ intentions, needs, 6 E. Kiomba Kambilo et al. Fig. 1. Taxonomy of Trust Issues goals, or objectives, as outlined in ISO[1]. In requirements engineering, a require- ment is defined as a statement which identifies an operational, functional or design features or constraint of the product or process, which is unambiguous, testable or measurable, and necessary for the product or process to be accepted by consumers or internal quality assurance guidelines[1]. A set of explicit, clearly stated require- ments facilitates communication between stakeholders: it justifies technological and design decisions and provides a basis for solution validation. When expressed in natural language, the statement of requirements should include a subject (e.g., system, software, etc.), an active verb and other elements necessary to specify the information content of the requirement. The guidelines for writing requirements are specified by ISO/IEC standard[1]. Transforming trust issues into trust requirements involves thoroughly analyzing the needs and expectations of all stakeholders and examining current systems and practices. The subjective needs of stakeholders are then transformed into objective needs or objectives. Requirements Engineering(RE) as a mediator between the acquirer and supplier domains, establishing and maintaining the requirements for the desired system, software, or service[33]. RE covers the discovery, elicitation, development, analysis, determination of verification methods, validation, communication, documentation, and management of requirements[1]. It is a crucial part of the software development Title Suppressed Due to Excessive Length 7 process and involves stakeholder collaboration to guarantee that the end product fulfills stakeholders’ needs and adheres to project constraints. In this paper, we adapt the RE process and follow the ISO/IEC standard [1] for transforming trust issues into trust requirements. Our process consists of the following steps: (1) Elicitation of user trust issues: In this step, the evidences of trust issues has to be gathered. Various techniques defined in the fields of requirements en- gineering and knowledge management can be used to collect the empirical data, including interviews, case studies, workshops, action research, etc. The outcome is a collection of trust issues expressed by end users or stakeholders. In this work, the data about trust issues has been collected through the literature review. For instance, in [17], an evidence of lack of accountability (I1) is expressed as follows: “it is important to listen to the interactions and responsibility between the suppliers and the OEM to maintain the transparency between the di↵erent vendors”. In [29], the same issue is expressed as follows: “The use of account- ability and incentive structures to punish and encourage dishonest or trustworthy individuals was a strategy to increase trust and confidence in the data”. (2) Analysis: The purpose of this step is to analyze each expressed trust issue in order to identify a subject of trust (actor, system, process, technological com- ponent), an object of trust (e.g., data, activity, function, etc.) and an expected relationship that must be established between the former and the latter in order to mitigate the issue. For example: Lack of accountability (I1) issue addresses a business partner in the supply chain (the subject) and the transaction data (the object). The issue expresses a trustor’s belief that, in case of dispute, the partner can avoid responsibility for his actions unless the formal proof of such actions is provided. To mitigate the issue, the transaction has to be non-reputable (the re- lationship). In the field, such analysis has to be conducted iteratively, confirming and validating the results with users. (3) Specification of requirements: In this step, the requirements are documented based on the analysis from the previous step and following the recommendations from [1]. The outcome of this step is a formalized requirement specification. Ex- ample: For the Lack of accountability (I1) issue, we formalize the corresponding trust requirement as follows: “System must guarantee non-repudiation of data”. The taxonomy of 21 trust issues and their corresponding trust requirements defined following the process above is presented Table 1. This taxonomy provides a decision-making support for requirements engineers and designers and guides the design of the prospective trustworthy SCM solution. The proposed process for translating issues into requirements can potentially support designers in identifying new issues and requirements. 8 E. Kiomba Kambilo et al. Table 1. Taxonomy & Mapping trust issues into trust Requirements emaN noitacfiitnedi ecnatpeccA ytilibareporetnI ytilaitnedfinoC ytilibisnopseR ytilibatnuocca foorp-repmaT dezilartneceD ycnerapsnarT tnemeriuqeR lortnoc ytilibaecarT gnirotinoM ecnedfinoC ytilibalacS ytilibaileR ataD ytirgetnI ytiruceS ycavirP egarotS tiduA ekaF tsoC resU T-R tnerapsnart noitacinummoc stnemeriuqer sredlohekats sessecorp .IS ytilaitnedfinoc dna ro atad lanretxe .atad seludom gnitidua .noitcafsitas reeP-ot-reeP atad lla tsurT elbadaer noitcasnart fo rotinom fo foorp-repmat ytilibaliava ytirgetni fo fo noitazilitu atad noitaiduper ecnedfinoc egarots ot rehto etatilicaf noisurtni sssecirosecorp ni emit ycavirp seussi ot nigiro resU fo htiw atad hcae tresni atad ytilibissop gnikcart rof laer hgiH a desilartneced ot yb tsurT erahs atad atad atad atad on troppus fo non deetnaraug deetnaraug no ot selfi hcum noitacfiicepS ecart a a elbailer atad elbixefl eetnaraug eetnaraug eetnaraug eetnaraug eetnaraug eetnaraug eetnaraug eetnaraug eetnaraug dna gol eht .enoyreve tsoc dooG a kcart evah evas evas evig eb eb eb eb eb t’nseod sdeen tsum tsum tsum tsum tsum tsum tsum tsum tsum tsum tsum tsum tsum tsum tsum tsum tsum tsum tsum tnemeriuqeR rof rennam metsyS metsyS metsyS metsyS metsyS metsyS metsyS metsyS metsyS metsyS metsyS metsyS metsyS metsyS metsyS metsyS metsyS metsyS metsyS metsyS metsyS ]8[ ]8[]22[]42[]5[]7[ ]73[]52[]92[]61[ ]73[]62[]83[]81[ ]42[]52[]11[]61[ ]22[]42[]21[]73[ ]8[]42[]61[]81[ ]73[]3[]83[]81[ ]73[]3[]83[]11[ ]8[]22[]42[]73[ secnerefeR ]6[]71[]3[]11[ ]71[]5[]7[]62[ ]5[]7[]73[]62[ ]2[]6[]42[]21[ ]6[]71[]42[]5[ ]2[]6[]21[]7[ ]2[]22[]71[ ]2[]21[]21[ ]2[]3[ ]8[]2[]22[ ]92[]61[ ]22[]11[ ]92[]61[ ]71[]7[ ]8[]22[ ]5[]3[ ]6[]3[ ]92[ ]52[ ]62[ -acinummoc )41I(noitacfiitnedi seussI )61I(ecnatpecca )6I(noitcasnart )4I(ytilibareporetni )51I(ytirgetni )1I( )11I(ytilibisnopser )02I(foorp-repmat )9I(ytilaitnedfinoc )12I(egarots )5I(ycnerapsnart )21I(ytilibaecart )71I(gnirotinom )91I(ytilibalacs )8I(ecnedfinoc ytilibatnuocca tsurT )7I(ytilibailer )3I( )01I(ycavirp )2I(gnitidua tcerid ytiruces fo )81I(atad doog resU atad fo ymonoxaT TR tsoc fo )31I(noit fo fo fo fo fo fo fo fo fo fo fo fo fo fo fo fo fo fo eussI hgiH kcaL kcaL kcaL kcaL kcaL kcaL kcaL kcaL kcaL kcaL kcaL kcaL kcaL kcaL kcaL kcaL kcaL kcaL kcaL ekaF Title Suppressed Due to Excessive Length 9 4 Use of Blockchain Software Patterns for Meeting Trust Requirements in SCM. Blockchain is often considered a de facto trust enabler. We argue that, while of- fering a number of key features, stock blockchain may not provide a complete solution for the specific trust requirements in a given context. In this section, we discuss the current limitations of stock blockchain solutions and propose the use of blockchain software patterns to e\u0000ciently address the specific trust requirements in SCM. According to the literature, intrinsic features of blockchain(public) technology address a number of requirements in SCM including trustworthiness. However, this technology also has limitations that can overshadow the benefits and have a negative impact on trust. These limitations have to be taken into account when making design decisions. Challenges related to privacy, scalability, trust and inter- operability are some examples relevant to the SCM domain. These challenges can be e\u0000ciently addressed using specific blockchain software patterns. Here are some examples. Blockchain is not suitable for storing and managing large amounts of data. Keeping images and other large data sets in blockchain can be expensive due to the high cost of transaction fees. This undermines the performance and credibility of the solution and negatively impacts user’s trust in this technology. To over- come this, o↵-chain data storage (patterns) such as the Interplanetary File System (IPFS) can be implemented in conjunction with blockchain. Using blockchain to store hashes of the data and o↵-chain storage to store extensive data can reduce transaction costs and add scalability to the system. Along the same lines, if a blockchain solution is not designed with security in mind, it can be vulnerable to cyberattacks or other forms of malicious activity. This can result in the loss of funds or the compromise of sensitive information. Ad- ditionally, if the consensus mechanism used in the blockchain is not well adapted, it could lead to issues with trust in the network. For privacy, if a blockchain is not designed with privacy in mind, it can lead to the exposure of sensitive in- formation because ledger is public for all stakeholders. The use of encryption-on chain(patterns) data is recommended to address these challenges. Table 2 presents the mapping between the trust requirements in SCM, the key blockchain features discussed in Section 2.2 that are recurrently used to meet these requirements, and the blockchain software patterns from [35], which we identify to complement the features. We consider three cases: CASE 1 blockchain features provide a complete solution for specific requirements. For example: transparency, as the transparent ledger o↵ered by blockchain inher- ently meets this requirement without the need for additional patterns (indicated ￿ with a symbol in BC features column in the table). CASE 2 : blockchain features are insu\u0000cient to meet a requirement and blockchain software patterns are proposed. For example, a public blockchain’s (transparency) cannot ensure data privacy. In this case, a private blockchain or encryption on- chain data patterns can encrypt data during transit to maintain privacy on the 10 E. Kiomba Kambilo et al. − blockchain (indicated with a symbol in BC software patterns column in the table). Table 2. Mapping of Trust requirements on BC features & Patterns ￿: ￿: Req.,−: Satisfied Req., partially satisfied Unsatisfied Req. Requirements Blockchain Blockchain Software Features Patterns [35] Accountability ￿Accountability ￿ Identifier Registry Audit − ￿ Audit Interoperability − ￿ Contract Observer Transparency − ￿ Transparency Cost control ￿Automating ￿ Minimize On-Chain data, Flyweight, O↵-chain data storage Reliability − ￿ Immutability Privacy − ￿ Encryption on-chain Responsibility − ￿ Identifier Registry Traceability − ￿ Traceability Decentralized − ￿ Decentralized RT identification − ￿ RT identification Integrity ￿ ￿ Hash, Integrity Hash secret Monitoring ￿ Monitoring ￿ Event Log, Publisher-Subscriber Scalability − ￿ State channel[50%], o↵-chain data storage[50%] Tamper-proof ￿ Tamper-proof ￿ Embedded permission Storage − ￿ O↵-chain data storage, Limit- Storage CASE 3 : blockchain features o↵er only a partial solution and can be comple- mented by using software patterns to meet a requirement. For example, the au- tomation of information systems in blockchain can lead to increased costs if the data transit is extensive in terms of storage. To address this issue, minimize On- chain data or using Flyweight patterns can limit the data size in each transaction ￿ and reduce costs (indicated with a symbol in the table). Implementing blockchain solutions by using software patterns can help to im- prove scalability, good storage, privacy and trust in the supply chain management Title Suppressed Due to Excessive Length 11 environment. In Table 2, we present the mapping of 16 out of 21 trust require- ments in SCM . According to our analysis, Security, Confidence, Confidentiality, Fake data, and User acceptance requirements are not fully met by the current blockchain solutions (features and/or patterns). These complex problems require more research and development and need to be addressed by the blockchain com- munity in the future. 5 Illustrative Example and Discussion To illustrate our proposed mapping of trust issues / requirements into specific blockchain software patterns and to provide the initial validation of this mapping, we consider an example from [7]. This paper discusses the use of blockchain for supporting traceability in a food supply chain for food safety risk management and compliance. It describes in details the design and implementation and validates the approach. 5.1 Running the process on the illustrative example Following the process defined in Section 3.2 , we identified the following trust issues from this case [7] and mapped them on our taxonomy in Table 1: Lack of Interoperability (I4): According to the case, “The aim is to develop an interoperable, autonomous systems”, where heterogeneous stakeholders can collaborate. High cost of transactions(I6): Stakeholders are preoccupied by the transaction costs: “Ipfs help to store large amount of data to reduce cost transac- tion”. Lack of Scalability(I19):“We must use decentralized storage such as IPFS to guarantee the integrity of the information”, stakeholders need systems with scal- ability of data. Lack of Traceability(I12): “Traceability-related information is not shared between participants, since they have their own traceability mechanisms and inevitably store their unique traceability records” Lack of Data integrity and privacy (I15, I10): “It is essential to guarantee the privacy of the transactions and the involved actors by using SC”. We map the identified trust issues on trust requirements. For exemple: “Stake- holders need to be reassured of the source of provenance and the authenticity of the products.” correspond to Traceability requirement in Table 1. We analyzed the architecture and patterns proposed by the case authors and compared them to our recommendation based on the mapping in Table 2. Table 3 shows the comparison results. 5.2 Results In this example, we were able to identify the trust issues from the case text and to map them on our taxonomy. Though the patterns indicated by the case authors are not expressed explicitly, we were able to match them with the patterns pro- vided by [35]. The authors in the case use IPFS to store vast amounts of data, which generates a hash secret on a smart contract address to reduce transaction 12 E. Kiomba Kambilo et al. oricess Table 3. Blockchain features and Patterns resolving trust issues ￿: ￿: Req.,−: Satisfied Req., partially satisfied Unsatisfied Req. Requirements BC intrinsic fea- Patterns (ex- BC software patterns (our pro- (extracted from tures tracted from the posal) the case) case) Interoperability Contract Observer − − Cost control O↵-chain data Limit Storage ￿ storage Privacy Encryption On- Encryption On-chain − chain Traceability − − ￿ Integrity Hash Secret Hash Secret ￿ Storage O↵-chain data O↵-chain data storage, Limit − storage storage Off-chain data storage. costs and enable The authors are also concerned with ensuring the privacy and traceability of data through encryption and the use of (Encryption On-chain Hash secret). hash & The patterns used in the case correspond to our recommended blockchain software patterns for the following four requirements: Privacy, Integrity, Storage Limit and Cost control. For the storage and Cost c requirement, we propose the storage Off-chain pattern to set a gas limit for transactions in addition to the data Storage, already identified by the case authors. The Traceability requirement does not require a specific pattern as it is di- rectly provided by blockchain. This corresponds to our mapping in Table 2. The Interoperability requirement is not addressed in the paper. We propose the fol- Contract lowing blockchain software patterns to meet these requirements: the observer pattern to guarantee interoperability and confirm that data written to Identifier registry the blockchain comes from a trustworthy source, and the pattern to track transactions and hold stakeholders accountable in the event of any problems. In this example, we used a mapping proposed in the previous section to extend the solution proposed by the authors with two specific blockchain software pat- terns. This proposal completes the solution by addressing more trust requirements. 5.3 Discussion Software patterns provide developers with technical best practices. However the trust implications of the patterns are not explicit. Our literature analysis shows that trust issues are not explicitly addressed in the design of the SCM solutions. Trustworthiness is often taken for granted by the mere use of a blockchain and cannot by validated. Title Suppressed Due to Excessive Length 13 The blockchain often meets trust requirements through a goal-oriented ap- proach, which many researchers have explored. For example, authors in [23] focused on studying and identifying trust requirements in blockchain systems and created a trust engineering taxonomy to meet blockchain systems’ trust requirements and goals. However, they did not provide evidence of how their taxonomy can be used or how to meet trust requirements using goals. In [20], a goal-oriented approach for business process reengineering is discussed. Here trustworthiness concerns are explicitly represented as (soft) goals and mapped to the relevant trust-enhancing features of blockchain, supporting business process reengineering. These works use a goal-oriented approach to focus on requirements and how specific blockchain features meet trust requirements. The uniqueness of our approach resides in combining specific blockchain pat- terns from [35] and intrinsic blockchain features to address trust issues and to create trustworthy solutions in the field of SCM. We make a first attempt to create a taxonomy of trust issues and trust require- ments and define their design implications for blockchain solutions. This taxonomy will guide organizations to create trustworthy solutions in SCM. Despite the promising results obtained from our study, it is important to note that there are several limitations and opportunities for improvement that should be addressed. These include: – We identified trust issues using a sample of 18 research articles on SCM. We plan to conduct Systematic Literature Review to validate our findings and to extend our taxonomy. – We limited our study to SCM, and more work is required to generalize this approach to other domains. – Both trust issues and pattern were often not explicit in the literature, we had to rely on our expertise and interpretation to extract them. Our main e↵ort is to promote standardization and the use of a common language (taxonomy of trust issues and requirements) to alleviate the interpretation bias in the future. For the practical and e↵ective use of of the proposed taxonomy and mapping, we plan to establish a recommendation system application where developers can select the issues encountered as input and have access to the blockchain software patterns they can leverage to ensure a design that inspires trust for collaboration. 6 Conclusion While blockchain technology is often considered as the de facto trust enabler, some limitations persist. These limitations has to be systematically addressed by improved design practices. In this article we consider blockchain software design patterns to address the trust issues in SCM domain. First, we provided an overview and developed a taxonomy of trust issues based on literature in the supply chain. Following the recommendations from requirement engineering, we translated the trust issues into explicit trust requirements. We examined the existing solutions that address these trust requirements in the literature and identified their limita- tions: We argue that the use of intrinsic features of blockchain often provides only 14 E. Kiomba Kambilo et al. a partial solution for trust issues in SCM. To complete this solution, we propose the use of blockchain software patterns. We identified 12 patterns from the blockchain software pattern literature that can support the trust requirements in SCM and evaluated our proposal on one example from the literature. This preliminary evaluation shows the relevance of the trust issues taxonomy defined in this work. The proposed blockchain software patterns extend the solution from the case, demonstrating the potential interest and added value of our mapping. This work aims at helping enterprises to better understand their trust-related requirements and to improve the design of their SCM systems.",
  "2023-RG": "1 Introduction Trust is a social construct that emerges from relationships and interactions between individuals or groups. It involves a willingness to rely on others based on perceived ability, integrity, and benevolence [1], [2] and is influenced by factors such as past experience, reputation, and social norms. Digital technologies enable novel models of social and business interactions, where trust becomes a critical design consideration for information systems. The impact of trust on system design is twofold: firstly, modern technologies act as mediators in interactions between individuals * Corresponding author © 2023 Irina Rychkova and Marwa Ghriba. This is an open access article licensed under the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0). Reference: I. Rychkova and M. Ghriba, “Trustworthiness Requirements in Information Systems Design: Lessons Learned from the Blockchain Community,” Complex Systems Informatics and Modeling Quarterly, CSIMQ, no. 35, pp. 67–91, 2023. Available: https://doi.org/10.7250/csimq.2023-35.03 Additional information. Author’s ORCID iD: I. Rychkova – https://orcid.org/0000-0002-1100-0116. PII S225599222300194X. Received: 14 May 2023. Accepted: 28 June 2023. Available online: 31 July 2023. and organizations, with the expectation of increasing trust between them; secondly, these technologies themselves must be trusted by users to provide them with a positive experience [3]. In the technological domain, trust is often connotated with security, reliability, and usability of digital systems or platforms. The extended ISO 27000 definition for trust [4] includes the CIA- triad (confidentiality, integrity, availability) as well as authenticity, accountability, non- repudiation, and reliability. Within this conceptualization, trust is often established through technological mechanisms, algorithms, and automated processes, and can be objectively assessed. The gap between the social and technology-centric definitions of trust arises due to the challenges of translating the subjective, context-dependent nature of social trust into objective, measurable terms that can be addressed by technical mechanisms. To bridge this gap, it is important to recognize the multidimensional nature of trust and consider the social and cultural contexts in which technological systems are developed and used. Three forms of trust are widely recognized in the literature: social trust, digital trust, and trust in technology. Social (or interpersonal) trust is defined as the subjective probability that an entity – a trustee – has the required capacity and willingness to perform an action that is beneficial or at least not detrimental to another entity – a trustor – in a specific context [1]. Compared to social trust, digital trust defines relationships between entities in the digital world. It is the measure of confidence that a trustor has in the trustee's ability to protect data and privacy of individuals [5]. Trust in technology is another form of trust that reflects trustor’s beliefs that a specific technology has the attributes necessary to perform as expected in a given situation where negative consequences are possible [6], [7]. Social, digital, and trust towards technology are intrinsic to organizations and have important implications in organizational decision-making and technology adoption [6], [8]–[11]. They need to be explicitly addressed in the design of technological solutions. During the past decade, the blockchain community provided a substantial contribution to the body of knowledge on the design and development of trustworthy information systems [12]–[14]. Blockchain technology fosters digital trust through reliable and efficient information sharing [15]. In the blockchain literature, trust is mainly connotated with specific technical properties such as decentralization, transparency, traceability, data integrity, etc. [11], [16]–[18]. Many of these properties are granted by the fundamental features of the blockchain technology itself [19]. While digital trust provides a foundation for secure and reliable digital interactions, it may not fully capture the complexities of social trust that arise from human relationships, emotions, and cultural factors. Moreover, different architectural and design choices, consensus mechanisms, and governance structures impact the level of trust and confidence that users have in the blockchain solutions [8]. Therefore, the broader scope and implications of trust in blockchain solution design need to be studied. In this work, we investigate how social trust, digital trust, and trust in technology are addressed in the blockchain literature. We follow the guidelines for systematic literature review (SLR) defined by Kitchenham et al. in [20] and review primary research studies that focus on trust conceptualization, trustworthy system design, and acceptance in blockchain. With this study, we intend to make the following contributions: • Descriptive overview of current research in information systems engineering and blockchain that addresses trust issues and trustworthiness requirements. • Definition and classification of trustworthiness requirements extracted from primary research studies in blockchain. • Qualitative analysis of trustworthiness requirements. Grounded on the lessons learned from the blockchain community, this work addresses a broader audience. First, it will help organizational stakeholders to better understand their trustworthiness requirements and to assess potential value of technological (in particular, blockchain) solutions to meet these requirements. Second, it will address technology professionals and researchers, helping them to align their design decisions with a social context. We formulate the identified trustworthiness requirements in technology-neutral language and make them reusable in different 68 problem and solution domains. To bridge the gap between technological and social domains, we associate the defined requirements with the three trustworthiness factors from social science: ability, benevolence, and integrity [2]. In order to effectively address the concerns of various experts involved in solution design, we identify each trustworthiness requirement with its corresponding type of trust. Additionally, we propose a mapping of these requirements on three abstraction levels: strategic, operational, and IT, to facilitate their expression in different organizational contexts. Our final intended contribution is: • Identification of key challenges and directions for future work that would lead to improved alignment between organizational requirements and technological solutions for resolving trust issues. The remainder of this article is organized as follows: Section 2 presents the background for this study and analyses the related works. Section 3 presents our research method. Section 4 reports on the results of this literature review with respect to the defined research questions. In Section 5, we discuss our findings and provide directions for future research. Finally, in Section 6, we conclude our article. 2 Fundamentals and Related Work 2.1 Trust and Trustworthiness In the research literature on trust, the act of trust is often represented as a relationship between a subject (the trustor) and an object of trust (the trustee) [21], [22]. Outcome of trust is defined as an interaction between trustor and trustee and is characterized by the resulting experience (negative or positive). Antecedents of trust refer to the factors that influence trustor’s willingness to trust and include factors related to the subject (trustor’s propensity to trust), to the object (trustworthiness of the trustee) and to the environment where interaction between the subject and the object takes place (e.g., institutional trust) [2], [6], [23], [24]. In this study, we consider trustor’s propensity to trust and institutional trust as invariant for a given interaction. Our primary focus is on trustworthiness factors, which are associated with the expected attributes of trustee. Trustor perceives the trustworthiness of a trustee by collecting information on that particular trustee. This perception can evolve based on the trust outcomes (good or bad experience) [1], [2]. Trustors expectations about trustworthiness of a trustee can be formulated as trustworthiness requirements (TwR). TwR can be met by incorporating certain attributes, features, or properties by the trustee, whether a social entity or a technological solution. Whereas researchers in social sciences study trust as relationships between social entities (individuals, groups or organizations), in information systems research, trust is considered as a socio-technical concept. It can be defined as a relationship between social entities and technological components (e.g., information systems, applications, infrastructure, etc.), in which a technological component can be either an object (trustee) or a subject (trustor) [25]. To address this complex nature of the concept, we consider three types of trust: social trust (trust between social entities), trust in technology (trust between a social entity – a trustor, and a technological component – a trustee) and digital trust (trust between social entities where technological components play the role of mediator and act “on behalf of” a trustor or a trustee). Depending on whether the trustee is a social entity or an IT object, the trustor needs to consider different trustworthiness factors prior to engage into interaction with this trustee. Below, we provide a brief overview of social trust, trust in technology and digital trust and their trustworthiness factors. We summarize the presentation in Table 1. 69 Table 1. Overview of trust types Trust antecedents (factors Type of Trust Trustor (subject) Trustee (object) Outcome of trustworthiness) Ability, benevolence, Interaction / Social Trust Org. / Individual Org. / Individual integrity collaboration Functionality, Trust in Org. / Individual IT object helpfulness, usefulness, Acceptance, use Technology reliability Org. / Individual IT object Privacy, security, Interaction / transaction Digital Trust IT object Org. / Individual transparency, traceability, in digital environment control IT object IT object Social Trust Social trust is a precondition of collaboration. It is described by a situation in which an individual or an organization (trustor) is willing to rely on the chosen actions of another individuals (trustee). Gambetta [1] defines trust as a level of the subjective probability with which the trustor assesses that the trustee will perform a particular action. Mayer, Davis and Schoorman define trust antecedents and outcomes in their integrative model of organizational trust [2]. The authors define the trust for a trustee as “a function of the trustee’s perceived ability, benevolence, and integrity and of the trustor’s propensity to trust.” Whereas propensity to trust is an intrinsic characteristic of a trustor, ability, integrity and benevolence are the factors of (perceived) trustworthiness that characterize a trustee and thus can be evaluated. According to [2], ability defines a group of skills, competencies, and characteristics that enable a trustee to have influence within some specific domain; benevolence defines the extent to which a trustee is believed to want to do good to the trustor, aside from an egocentric profit motive; integrity refers to trustee’s moral quality of being sincere, honest, and her capacity and willingness to adhere to some rules/principles. Social trust is used as the basis for decision-making in diverse contexts, including enterprise strategy, governance of operations and technology [26]. Trust in Technology Trust in technology is described by a situation in which an individual user or an organization (trustor) is willing to rely on technology (trustee) to accomplish a specific task [6]. Trust in technology reflects trustor’s beliefs that a specific technology (IT object) has the attributes necessary to perform as expected in a given situation in which negative consequences are possible. The trustworthiness factors in trust in technology include functionality, helpfulness, reliability and credibility of information [6], [7]. According to Sutcliffe [3], needs for trust in technology can be fulfilled by solutions’ usability, functionality, aesthetics. He also highlights that trust can be facilitated via customizability and adaptability. In [27], [28], trustworthiness of technology is associated with (perceived) usefulness, ease of use, enjoyment and value (quality/price ratio). In [29] trustworthiness of software is associated with transparency, verifiability and compliance of the development process. Trust in technology is an antecedent of technology acceptance and use [28], [30], [31]. To improve the acceptance, user’s expectations about trustworthiness of technology need to be explicitly formulated as respective (trustworthiness) requirements and considered in technology design. Digital Trust Digital trust is a precondition for social and business interactions in a digital environment. In these interactions, technology (IT object) plays the role of a mediator and can impersonate a trustor or a trustee. Digital trust reflects trustor’s beliefs that trustee (a social entity or an IT object) has the attributes necessary to support secured digital interactions [5]. Trustworthiness factors in digital trust include (perceived) privacy, security, transparency, traceability, and control [19], [32]. According to [3], the role of technology as a trust mediator can be also fulfilled by increasing accessibility of information, transparency of processes, communication of intent and identity. 70 2.2 Trustworthiness Requirements in Software and Systems Engineering In systems engineering, trustworthiness of a particular system or component means “to be worthy of being trusted” to fulfil some specific requirements [33]. ISO/IEC 25010 Standard [34] addresses systems and software quality requirements and defines trust as a degree to which a user or other stakeholder has confidence that a product or system will behave as intended. Whereas these definitions provide some reference to the social context where the system is used, the implications of trust are not explored much further. Subjectivity, context sensitivity and emergence are characteristics of trustworthiness that make it difficult to capture and formalize in product design. Consider an example of a mobile phone: if we examine the trustworthiness factors that influence a user’s decision to buy, to use for specific purposes and to rely upon this product, we discover that for different users and contexts of use these factors will not be the same. For an elderly person, the antecedents of trust will include usability and helpfulness; for people with active lifestyle they will include robustness and cost efficiency; for professionals they will include performance, resilience, data security and so on. Further, stakeholders’ “needs for trust” can run into conflict with other needs and impact the requirements. For instance, using a cloud storage is attractive because of its high accessibility and low price, however this raises trust concerns related to resilience and possibility of unauthorized access to your personal or business data. Trustworthiness requirements considered in this work provide a ground for reasoning about such conflicts and their resolution. In requirements engineering, a requirement is defined as a statement which identifies an operational, functional or design characteristic or constraint of the product or process, which is unambiguous, testable or measurable, and necessary for the product or process to be accepted by consumers or internal quality assurance guidelines [35] A set of explicit, clearly stated requirements facilitates communication . between stakeholders: it justifies technological and design decisions and provides a basis for solution validation. When expressed in natural language, the statement of requirement should include a subject (e.g., system, software, etc.), an active verb and other elements necessary to specify the requirement. The guidelines for writing requirements are specified by ISO/IEC standard [35]. In software engineering and systems engineering, two types of requirements are widely used during the product design: functional requirements (FRs) define a function of a system or its component; non- functional requirements (NFRs) define the properties and specify the criteria according to which the system’s functioning can be judged or evaluated. In other terms, FRs define what the system has to do, while NFRs define how it should do it. Whereas FRs and NFRs focus on measurable product quality, phenomena related to people and social context where the product is developed, deployed and used require not less attention in RE. In [36] a taxonomy of ‘soft’ requirements is introduced. Soft requirements are a linguistic concept that addresses a wide range of phenomena related to people, organizations and society: values, attitudes, motivations, emotions. SR are extending NFR/soft goals and can be refined by FR/hard goals and met by some properties of a (software) solution. However they also address social concepts, including trust, that do not always require technological solution. Compared to FR and NFR, SR may be implicit, with their influences subtle and difficult to anticipate at design time [36]. Trust is associated in SR with the aspects of the social system where a technological system is used – its context. Context of use SR influence user requirements (FR and other SR) as well as product qualities (FR and NFR). In this work, we focus on trustworthiness requirements (TwR) that can be associated with FR and NFR contributing to trust between the entities (e.g., a user and a software product) in a given socio- technical system (see Figure 1). We define trustworthiness requirement as a statement made by a trustor about the expected trustworthiness of a trustee. This statement has to clearly express an operational, functional, design or other characteristic, which, according to trustor’s believes, positively impacts trustworthiness of this trustee and interaction between the two. The nature of trustor, trustee, relationship between them as well as trustworthiness factors can vary depending on the type of trust (see Table 1). Similar to SR from [36], TwR for a given product can be emergent and can vary depending on characteristics of a trustor (e.g., her propensity to trust, perception of risk) and the context where the interaction with the product will take place; if explicitly defined, TwR can change (extend) the set of product requirements (see TwR’ in Figure 1). 71 Whereas trustworthiness is considered as an important factor of product satisfaction [34] and recognized as an antecedent of adoption, little research explicitly addresses trust and trustworthiness as a part of system requirements. In [37] the role of trustworthiness in the software development lifecycle is examined. Trustworthiness requirements are derived from user trust concerns and include usability, availability, reliability, transparency. Once users’ trust concerns are elicited during a requirement engineering process, they are translated into trustworthiness requirements and mapped onto specific features / properties of the (prospective) technological solutions. Figure 1. Trustworthiness requirements (TwR) compared to FR and NFR. If explicitly defined, TwR can change (extend) the set of product requirements (see TwR’) 2.3 Trust and Trustworthiness in Blockchain In practice, digital trust solutions consist in implementing a set of control mechanisms, which intend to modify the feasible set of alternatives (undesirable scenarios) that can be realized within a trustor-trustee interaction in a digital environment. Paraphrasing Gambetta [1], digital trust can be seen as “a device for coping with the freedom of others”, which increases the probability that the other party will not (be given an opportunity to) act in a harmful way. Blockchain technology enforces digital trust by providing a set of such control mechanisms as intrinsic features. Blockchain technology has emerged as a potential solution to cope with mistrust in traditional (centralized) institutions and online intermediaries in general [8]. Blockchain can be defined as a distributed database that allows its users to transact in a public and pseudonymous setup without the reliance on an intermediary or central authority [38]. According to [39], trust is the most influential factor driving interest in the blockchain. In blockchain, trust is not placed into the (social) entities participating in an interaction, but into specific properties of the technology. Belotti et. Al. [19] point out, “Whenever trust cannot be laid on a set of network nodes, it is better to have confidence in a protocol (i.e., a set of rules) ... that punishes or makes unfeasible any violation and thus guarantees the correct functioning of a system”. Decentralized architecture, use of cryptography, distributed consensus protocols and smart contracts are fundamental features of blockchain that enable immutability, integrity, auditability and transparency of transactions. These properties are recurrently associated with digital trust in the literature [11], [16]–[18] . However, with a sheer use of blockchain, trust is not granted. While blockchain platforms support digital trust building between parties they do not remove the requirements for social (inter- personal) trust in organizations [8], [40]. Moreover, alleviating some social factors of trustworthiness, blockchain introduces their digital counterparts. For instance, independence from a central authority comes at the cost of privacy; distributed consensus and trustful transactions come at the cost of performance and interoperability etc. Depending on the industry sector and the use case, privacy, security, scalability, interoperability, performance are considered the main challenges and strongly impact blockchain adoption and trust in blockchain technology [19], [41], [42]. To meet these challenges, the blockchain technology trustworthiness has to be examined against the trustworthiness requirements in each particular use case. 2.4 Related Works Trustworthiness of blockchain solutions is widely discussed in the related secondary studies. In [42], the authors evaluate the potential of blockchain against traditional databases in four domain 72 areas, including required trust assumptions, context requirements, performance characteristics and required consensus mechanisms. The literature review in [43] examines how trustworthiness of data provider (“the oracle”) is addressed by blockchain solutions. In [44], the authors evaluate blockchain solutions and discuss requirements and considerations related to trust for identity management in healthcare domain. The literature review in [10] examines the barriers to blockchain adoption and highlights the issues related to high computing power requirements and implementation costs. In [45], the overview of trust-free sharing services in the financial sector is presented. Authors highlight security aspects as main trustworthiness factors. The work in [46] identifies key factors and non-functional requirements related to adoption of blockchain solutions in construction industry 4.0. These factors include information trustworthiness, transparency, traceability, and immutability. In [41], security, privacy, latency and computational cost are also identified as the main technical challenges of blockchain, however only privacy is explicitly related to trustworthiness of blockchain. Whereas many studies discuss technical challenges in blockchain and their impact on the blockchain adoption, only a few relate these challenges with trust (social trust or trust in technology). In [47] the role and the multi-perspective view on trust in the context of the sharing economy and blockchain technology is examined. The authors identify trust in peers, trust in platform and trust in other targets (including products) and put forward the social antecedents of trust (ability, integrity, benevolence). In [48] a goal-oriented approach for business process , reengineering is discussed. Here trustworthiness concerns are explicitly represented as (soft) goals and mapped to the relevant trust-enhancing features of blockchain, supporting business process reengineering. In this work, we capture the issues and requirements expressed in the blockchain literature (FR, NFR, process requirements, etc.) that are associated with trust and trustworthiness and formulate them as non- blockchain-agnostic TwR. These requirements are intended to serve as a knowledge base and to facilitate the mapping between trust issues expressed by both technical and non-technical stakeholders in organizations and trust-enabling features of solutions, rationalizing technological and design choices. 3 Research Method The research method used in this study follows the guidelines for systematic literature review (SLR) from [20]. The methodology consists of the following steps that will be further explained in this section: definition of the research questions, definition of the search strategy, primary source selection, data extraction, analysis and synthesis. 3.1 Research Questions This study aims to examine the existing research on design, development and acceptance of blockchain solutions and to provide a comprehensive overview of the organizational requirements related to social trust, digital trust and trust in technology that drive these solutions. We formulate the following research questions for this study: RQ1: What are the contributions of primary studies? RQ 2: How trust is defined in primary studies? RQ 3: What are the trustworthiness requirements used by primary studies? RQ 4: What types of trust are addressed by the identified requirements? RQ 5: At what abstraction levels the trustworthiness requirements are defined? 3.2 Search Strategy and Selection Process The flow diagram adopted from PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) [49] presents an overview of the source selection process in Figure 2. 73 Figure 2. PRISMA flow diagram for selection process 1: Identification. To identify the initial set of records, we selected the following databases: Scopus, ACM digital library, IEEE. Using the PICOC criteria [20], we define the search terms forming the following search string: “blockchain” AND (“trust” OR “trustworthy” OR “trusted”) AND “requirement” AND (“architecture” OR “design” OR “development” OR “engineering” OR “adoption”). We limit the publication year (PUBYEAR < 2023) to obtain a consistent set of publications that will not be affected by more recent apparitions. We conducted an automated search in the selected databases and identified 793 records in total. After removing duplicates and non-primary sources (e.g., proceedings, books etc.) we kept 415 records for screening. 2: Screening Relevant Publications. We screened titles and abstracts of identified records and eliminated publications based on the following exclusion criteria: EC1: A study is unavailable for retrieval EC2: A study is not a peer-reviewed publication EC3: A study is not primary research EC4: A study does not focus on requirements EC5: Issues or requirements motivating the solution are not related to trust or trustworthiness. We kept 78 records for the full text assessment. 3: Eligibility Assessment. We examined the full text of the preselected publications for eligibility based on the exclusion criteria EC3-EC5. The full text assessment was executed by the two authors independently; the results were compared and the conflicts were resolved. 48 records were kept for the final data set. 4: Snowballing and Final Data Set. We conducted backward and forward citation analysis of the eligible publications from the previous step (a so-called “snowballing” technique) using other databases (e.g., Google scholar) and identified a set of 26 records which was reinjected into the process. These records were screened and assessed for eligibility following the steps above. 18 records have been eliminated. The selection process resulted in a final data set of 56 articles ([7], [11], [16]–[18], [30], [40], [49]–[97]). The overview of the selected studies is presented in Table 2. In our further analysis, we use consecutive enumeration of the studies: S1-S56. 74 Table 2. Overview of the selected studies l a : t c s i f u g n o n l F r o a o o / l n E T l c d a i h i e o t o t i e R c m n t c c t t d a f i c e i c c r a t o i c r o : : u r i a a o o a a r t i l i n h e o r l d f c f e i m s p t e p l i i i p t e u s a t v s o b m t t f e u p b h o y e r e r e D r e r i M O Y A D A R A D A B E T T L T P I Vehicular model D, S, S1 2022 [49] * * TECH T, S Networking algorithm TT proposal S2 2022 [50] Healthcare * * evidence TECH S, OP, T S, TT principles approach S3 2022 [51] Education * * * IM OP, T S, TT prototype approach S4 2022 [52] SCM * * * IM OP, T D architecture S5 2022 [53] EAI * * approach IM OP, T S Healthcare, S6 2022 [54] * * architecture IM T D IoT architecture, S7 2022 [55] SCM * * * IM OP, S D framework evidence / S8 2022 [40] SCM * * SOC ST, T S, TT proposal Electronic evidence / S9 2022 [56] * * IM ST, T S, TT Voting proposal protocol, S10 2022 [57] IoT * IM T D algorithm S11 2022 [58] Other * * framework IM T D system S12 2021 [59] AI * * * IM OP, T TT model approach S13 2021 [60] Healthcare * * * IM S S, D prototype S14 2021 [11] Banking * * other IM OP, S TT architecture prototype D, TT, S15 2021 [61] Education * * * IM S, OP, T model S framework D, S, S16 2021 [62] Healthcare * * other IM OP, T, S TT framework Healthcare S, TT, S17 2021 [63] * * * architecture SOC S Industry 4.0 D system dApps framework S, D, S18 2021 [64] * SOC S, OP, T BPM model TT approach S, D, S19 2021 [65] SCM * * * conceptual IM S, OP, T TT model approach S, D, S20 2021 [66] BPM * * * SOC S, OP, T prototype TT SCM model TT, D, S21 2021 [67] * * * IM OP, T IoT algorithm S SCM framework S22 2021 [68] * IM S, OP, T D, S IoT model Vehicular framework, S23 2021 [69] * * IM T D Networking architecture S24 2021 [70] Other * * architecture IM T D S25 2021 [71] IoT * framework IM T D evidence S26 2020 [18] Healthcare * * SOC S, OP, T S, D principles S, D, S27 2020 [72] EA * other OP TT S, D, S28 2020 [16] Other * evidence TECH OP, T TT 75 Table 2 continued l a : t c s i f u g n o n l F r o a o o / l n E T l c d a i h i e o t o t i e R c m n t c c t t d a f i c e i c c r a t o i c r o : : u r i a a o o a a r t i l i n h e o r l d f c f e i m s p t e p l i i i p t e u s a t v s o b m t t f e u p b h o y e r e r e D r e r i M O Y A D A R A D A B E T T L T P I proposal S29 2020 [73] Banking * * TECH T D architecture S30 2020 [74] Education * * architecture TECH OP, T D Vehicular architecture S31 2020 [75] * * * TECH T TT, D Networking concept evidence S, D, S32 2020 [76] BPM * * SOC S, OP, T taxonomy TT evidence S, D, S33 2020 [77] BPM * * SOC S, OP, T method TT S34 2020 [7] MIS * * model SOC S, OP, T TT Edge S35 2020 [17] * * architecture IM T D Computing architecture evidence S36 2020 [78] Other * * IM S, OP, T TT, D technology blueprint S37 2020 [79] Healthcare * model IM S, OP, T D framework S38 2020 [80] BPM * * * tool IM S, OP S, D process architecture Healthcare S39 2020 [81] * * * proposal IM T, OP S, D EA approach evidence S40 2020 [82] BPM * * TECH S,OP, T S, D approach S41 2020 [30] Other * framework IM OP S, TT S42 2019 [83] SCM * * other TECH OP TT framework S43 2019 [84] Other * * TECH S, OP, T D principles proposal S44 2019 [85] Healthcare * * IM OP, T S, D protocol Electronic S45 2019 [86] * proposal IM S S Voting S, D, S46 2019 [87] Healthcare * proposal IM OP, T TT IoT S, TT, S47 2019 [88] * * framework SOC S, OP, T Industry 4.0 D approach S48 2019 [89] BPM * * IM S, OP, T S, D proposal S, D, S49 2019 [90] BPM * * model SOC OP, T TT framework S50 2019 [91] Cloud * * TECH T D proposal S51 2018 [92] BPM * proposal TECH OP S, D S52 2018 [93] Healthcare * proposal TECH T D BPM architecture S53 2018 [94] * * IM OP, T S, D MAS proposal protocol S54 2018 [95] SCM * * * IM T D model BPM proposal S55 2018 [96] * * IM T D Industry 4.0 framework S56 2017 [97] Other * * tool TECH T D 76 3.3 Data Extraction, Analysis and Synthesis Table 3 defines the data items that have been systematically extracted from the selected sources. Each data item is defined in connection with one of the research questions. Year of publication and Application domain provide a descriptive information about our data set and are not explored any further in this study. Overview of the extracted data is presented in Table 2. The results of our data analysis are presented in the next section. Table 3. Data extraction Form Data Item Value RQ Year of publication NUM -- Application Domain Text -- Quantitative analysis: {Empirical, Artifact, Theoretical, Methodological, Dataset, Survey, Contribution type RQ1 Opinion} Research outcome Text RQ1 Qualitative analysis: Trust: definition {IM, SOC, TECH}; IM = implicit; SOC = social; TECH = technical RQ2 Trust Issue(s) Text RQ3-5 Trustworthiness Text RQ3-5 Requirement(s) Type of trust {S, D, TT}; S = Social, D = Digital, TT = trust in technology RQ4 addressed Level of abstraction {S, OP, T}; S = Strategic, OP = Operational, T = Technical RQ5 For the analysis of contributions, we used the guidelines of Wobbrock [98] to code the contribution types and research outcomes. For the qualitative analysis of trustworthiness requirements, first, we examined which underlying theories of trust are used in the studies and to which definition of trust (social or technology-centric) the studies adhere. Next, we extracted text evidences of trust issues and trustworthiness requirements. Further, the extracted data was coded by the authors following both semantic and latent approaches [99]. We defined the codes for our trustworthiness requirements based on the trustworthiness properties specified by the related secondary studies (Table 4). The codes most frequently used in these studies are: Security, Privacy, Data integrity, Confidentiality, Availability, Reliability, Accountability. Other codes used are: Decentralization, Costs/resource efficiency, Traceability, Immutability, Transparency, Resilience, Authenticity. Table 4. Related literature used to define the codes for trustworthiness requirements Article Method Articles analyzed Alamri, B. et al., 2022 SLR 24 Ali, O. et al., 2020 SLR 87 Caldarelli, G., Ellul, J., 2021 SLR 49 Casino, F. et al., 2019 SLR 314 Durneva, P. et al., 2020 SLR 70 Hawlitschek, F. et al., 2018 SLR 62 Konstantinidis, I. et al., 2018 SLR 44 Pietrzak, P., 2021 SLR 34 Ross, R., et al., 2016 Technical Report n/a Standard ISO/IEC TR 27000, 2018. Standard n/a Teisserenc, B., Samad, S., 2021 LR + Interviews n/a Wang, Y. et al., 2019 SLR 29 During the analysis, the set of codes was refined and new codes have been added. For instance, considering that Security is closely associated with other categories (e.g., Confidentiality, Integrity, 77 Availability, Authentication) in the literature, we removed it from our code list. Conversely, other categories recurrently used in the primary sources (e.g., Compliance, Interoperability, Auditability) have been added to the code list. The evidences of requirements have been extracted and analytically mapped on the identified codes and categories, including type of trust and abstraction level (available in Table 2). 4 Results 4.1 What are the Contributions of Primary Studies? (RQ1) In this review, we examine 56 primary research studies that address trust issues and requirements by designing and developing technological solutions. We apply the classification of Wobbrock [98] to analyze the general forms this new knowledge takes. This classification defines seven research contribution types: empirical, methodological, theoretical, artifact, survey and opinion. We coded each research study with the contribution type and the type of research outcome that was produced (Table 2). The majority of studies reports on multiple contributions and outcomes. The summary is presented in Figure 3. (a) (b) Figure 3. (a) Distribution of the research contributions by type. (b) Distribution of the developed artifacts. Empirical research contributions refer to findings based on observation and data gathering. The created knowledge is embedded in new evidences and proposals [98]. 48 out of 56 (86%) examined studies make empirical research contributions providing evidences on trustworthiness requirements, issues and solutions. Theoretical contributions aim at improving the existing understanding or the existing way of reasoning about things. They provide new definitions, concepts, models, principles, or frameworks grounded on analytical thinking and reasoning [98]. Theoretical contributions are made by 27 out of 56 (48%) examined studies. Methodological contributions aim at improving the existing practice by defining novel ways to “carry out our work”. They influence how we design, develop, analyse or run systems or processes and result in new knowledge in a form of approaches, methods, metrics, techniques etc. Methodological contributions are identified in 13 studies (23%). Proposed methods and approaches are mostly grounded on observations (experience, empirical data). 78 Artifact contributions result from design and development activities. Here new knowledge is manifested by working prototypes, architectures, tools, processes, algorithms that demonstrate new concepts, or enable new explorations in the future [98]. Artifacts are developed in 26 examined studies (46%) including 9 working prototypes, tools or systems and 14 architectures. Other artifacts include process, protocols, algorithms. Survey research contributions create knew knowledge by synthesizing the previous work and identifying trends and open issues. Survey contributions are also referred to as “secondary research”. Following the selected research protocol [20], secondary research papers have been eliminated during the selection process (see Section 3.2.). Dataset contributions support the research community providing common ground for testing, analysis and evaluation of other contributions. Opinion contributions propose the arguments and seek not only to inform but to persuade the reader. These contribution types were not identified in the examined set of studies. 4.2 How Trust is Defined in Primary Studies? (RQ2) In addressing trust in the text, the examined primary research is divided as follows: studies that provide an explicit definition of trust grounded on social sciences, studies that provide technology- centric heuristics on trust, and studies that do not provide an explicit definition of trust. Figure 4 illustrates the distribution of studies. We found 10 articles (18%) that define trust as a social concept and recognize the role of the social context in their technical solution design (S17, S18, S20, S26, S32, S33, S34, S47, S49, S8). In 13 articles (23%), trust is presented not as a “cause” but as an “effect” of a technological solution. For instance, a solution is considered trusted if it exhibits some specific properties such as decentralization, transparency, traceability, data integrity, etc. (S1, S2, S28, S29, S30, S31, S40, S42, S43, S50, S51, S52, S56). The remaining studies (59%) refer to trust and trustworthiness without defining it explicitly. Our findings show little consensus in understanding trust and its social dimension in the blockchain community. This lack of theoretical foundation impacts the way the trust issues and the trustworthiness requirements are expressed in the studies. Figure. 4. Definition of trust in the primary studies: TECH – technology-centric view; SOC – social view; IM – no explicit definition of trust 4.3 What are the Trustworthiness Requirements Used by Primary Studies? (RQ3) We extracted evidences referring to trustworthiness requirements from 56 articles and identified 21 requirements recurrently expressed in these studies – TwR. Text related to the same requirement from different sources was generalized and reformulated to comply with the ISO recommendations from [35]. In the requirement statements we use the terms “trustor” and “trustee” to identify the corresponding party in an interaction as defined in Section 2.1. The term “process” refers to an interaction between trustor 79 and trustee or to a service that trustee provides for trustor (depending on the context). The term “system” refers to a technological solution implementing, supporting or mediating the “process”. We present the summary of requirements in Tables 5–7. Here RID – requirement identifier; Requirement – requirement name; Type of trust – indicates the type(s) of trust this requirement refers to (S – social, D – digital, TT – trust in technology); EA domain refers to the level of abstraction where requirement can be formulated (S – strategic, OP – operational, T – technical). Trust issues often emerge in the social domain, are grounded on (subjective) beliefs of the organizational stakeholders and conditioned by culture, politics, personality etc. Whereas trustworthiness factors defined by social science (e.g., ability benevolence, integrity) are suitable to describe and reason about users’ trust issues they are not providing enough details to guide technological solutions. Conversely, trustworthiness factors defined in technological domain (i.e., blockchain) provide a blueprint for technological solutions. Nevertheless, they are hard to trace back to the social context where the trust issues emerge on the first place and in which these solutions will be exploited. To bridge the gap between social and technological domains, we relate the extracted TwR to perceived trustworthiness factors defined by Mayer at al [2] ability, : benevolence and integrity. Ability refers to a group of skills, competencies, and characteristics that enable a party to have influence within some specific domain [2]. We identified 7 trustworthiness requirements that refer to trustee’s ability to fulfil a specific task or to ensure this task to be fulfilled in a specific way (Table 5). These requirements include: Table 5. Trustworthiness requirements: Ability EA RID Requirement Type of trust Articles domain TwR1 Competence S OP, T S17, S47, S49 TwR2 Automation TT, D OP, T, (S) S21, S16, S19, S27, S28, S34, S55 S3, S21, S12, S15, S18, S19, S20, S29, S32, S36, S48, TwR3 Decentralization S, D, (TT) S, OP, T S51, S53, S54, S55, S9, S23, S25 TwR4 Interoperability S, D, TT OP, T S14, S15, S31, S36, S44, S46, S48, S55, S56 TwR5 Performance S, TT OP, T S12, S18, S31, S32, S33, S34, S47, S49 S1, S21, S22, S12, S14, S15, S16, S17, S18, S19, S27, Efficiency/ 5.1 TT, S, D T, OP S28, S29, S31, S32, S33, S34, S47, S52, S55, S56, S8, Robustness S11 5.2 Cost effect. TT S, OP, T S21, S14, S15, S16, S19, S27, S28, S8 TwR6 Resilience S, TT, (D) OP, T, (S) S1, S6, S12, S17, S18, S27, S33 TwR7 Availability S, TT, (D) S, OP, T S15, S16, S17, S18, S20, S32, S33, S49, S56, S17 TwR1 – Competence: Trustor must be able to assess trustee’s ability/competence/skills/expertise to deliver a service or to perform a (part of) entrusted process with respect to some predefined level of quality. This requirement specifies relationships between social entities (individuals or organizations) and is related to social trust. TwR2 – Automation of data processing: Trustee must minimize physical and maximize digital processing of data. Trustee fulfils this requirement by automating their processes and/or implementing dedicated services. This requirement is grounded on an assumption that the automated process reduces (human) errors, transaction time, transaction cost. This requirement determines technology acceptance and digital interactions in the examined studies; it is related to digital trust and trust in technology. TwR3 – Decentralization: Control over process activities and data must not be delegated to a third party or to one specific party involved in the process itself. The system (trustee) has to support distributed coordination and control over transactions. This requirement is closely related to Disintermediation: trustor - trustee interaction must not rely on any intermediary for process coordination or control. This requirement is associated with social and digital trust in the literature. Only few studies mentioned decentralization in connection with technology acceptance or trust in technology. 80 TwR4 – Interoperability: Trustee must demonstrate a capability to work with trustor despite organizational, technological, cultural or other differences. This requirement is associated with all three types of trust in the studies. For digital trust and trust in technology, it can be expressed as follows: System (trustee) must be able to integrate without undue delay / work with various heterogeneous components (physical or technological). Various kinds of available data resources should be integrated. TwR5 – Performance: While providing a service / executing an entrusted task, trustee (an organization, individual or a technological solution) must ensure an efficient distribution of resources, with respect of defined timeframe and budget. These resources may include physical, human, technological resources. In S47 performance is defined as a perception of an automated system’s capability for supporting user’s goals. While some sources associate performance with efficiency and robustness (related to social trust and trust in technology), other put forward cost effectiveness (related to trust in technology). TwR6 – Resilience: Trustee has to guarantee the process execution in case of failure of one of components. Trustor must be able to recover the data and to transmit it into other system. TwR7 – Availability: All resources (human, physical, hardware/software, information) needed for process/activity execution has to be available. This requirement has to be fulfilled by a trustee (an organization, an individual, or a mediating infrastructure). According to the literature, resilience and availability mostly determine social trust and trust in technology. Benevolence is the extent to which a trustee is believed to want to do good to the trustor, aside from an egocentric profit motive [2]. We identified 7 trustworthiness requirements that refer to trustee’s ability to guarantee (or to trustor’s capacity to control) that trustee’s actions will cause trustor no harm (Table 6). Table 6. Trustworthiness requirements: Benevolence Type of EA RID Requirement Articles trust domain Authentication S1, S4, S21, S22, S17, S31, S35, S36, S37, S44, S45, TwR8 D, S, (TT) S, OP, T (entity) S47, S48, S9, S10, S23, S25 Authentication TwR9 D, S, (TT) S, OP, T S1, S21, S22, S30, S31, S35, S36, S47 (data) S6, S18, S19, S20, S27, S33, S38, S43, S44, S46, S47, TwR10 Confidentiality S, D, TT S, OP, T S49 TwR11 Authorization D (S, TT) S, OP, T S1, S4, S15, S17, S47 TwR12 Accountability S, D S, OP, T S1, S17, S43, S47, S11 S1, S7, S12, S17, S26, S27, S36, S44, S49, S52, S54, TwR13 Privacy S, D, (TT) S, OP, T S9, S10 TwR14 Usability S, TT S, OP, T S12, S47, S14 TwR8 – Authentication (entity): Trustor must be able to verify the identity of trustee. TwR9 – Authentication (data): Trustor must be able to determine the correctness and reliability of reported data (e.g., messages, events). This requirement is also referred to as data authenticity, data accuracy, data reliability in the studies. Both data and entity authentication are mainly associated with digital trust. TwR10 – Confidentiality: Trustor’s sensitive information (including identity) must not be disclosed to unauthorized parties; the executed activity is only visible to authorized resources/entities. This requirement is associated with all types of trust in the literature. TwR11 – Authorization: Trustor must be able to determine whether trustee has the appropriate permissions (i.e., is authorized) to perform a specific action or access a specific resource. This requirement is mainly associated with digital trust and is related to TwR12. TwR12 – Accountability: Trustee is held responsible for her actions and cannot deny them. In case of malicious activity/information, an authorized authority has to ensure accountability by tracing the identity of a source of malicious activity/information. 81 TwR13 – Privacy: Trustor's identity information must not be disclosed. Trustor must have the power to make decisions concerning collection, use and disclosure of personal information by trustee. This requirement is associated with social and digital trust. TwR14 – Usability: The system (trustee) must be intuitive, easy to use, requiring minimum specific training or skills from trustor. The system must be adapted for specific needs (e.g., age, handicap). This requirement determines trust in technology and associated with acceptance/adoption. Integrity is the moral quality of being sincere, honest, and consistent in one's behavior; capacity and willingness to adhere to some rules/principles [2]. Table 7 presents requirements that refer to trustee’s ability to guarantee (or to trustor’s capacity to control) that trustee’s actions comply with predefined rules, norms or agreements. Table 7. Trustworthiness requirements: Integrity Type of RID Requirement EA domain Articles trust S5, S18, S20, S27, S32, S33, S39, S40, S45, TwR15 Integrity (process) S, D, TT (S), OP, T S47, S48, S49, S53 S1, S6, S21, S15, S16, S17, S19, S28, S29, TwR16 Integrity (data) D, S, (TT) T, OP, (S) S30, S39, S40, S45, S47, S48, S50, S52, S54, S56, S8, S24, S25 TwR17 Non-repudiation D, S OP, T S1, S4, S15, S16, S20, S28, S32, S33 S22, S14, S17, S18, S26, S27, S40, S42, S8, TwR18 Compliance S, D, TT S, OP, (T) S17 S3, S4, S21, S22, S13, S16, S18, S27, S28, TwR19 Auditability S, D, (TT) S, OP, T S40, S42, S46 S2, S3, S22, S14, S15, S16, S17, S18, S19, TwR20 Transparency S, D, TT S, OP, T S28, S32, S33, S40, S42, S43, S46, S47, S53, S55, S8, S9, S11, S17 S21, S22, S12, S14, S16, S28, S38, S53, S8, TwR21 Traceability D, S, (TT) OP, T, (S) S11, S25 TwR15 – Integrity (process): Trustee must ensure correct and timely execution of activities, with respect of contract agreements or process specifications. This requirement is associated with all three types of trust: it can determine trustworthiness of both social and digital interactions as well as trust into technology (an IT object). TwR16 – Integrity (data): Trustee must ensure the overall accuracy, completeness, and consistency of data over its entire life-cycle. This includes data protection from unauthorized modification or alteration. TwR17 – Non-repudiation: Trustee must ensure that any activity, once performed, cannot be denied. All the information artefacts should be written in a permanent, tamper-proof way. TwR16–17 are primarily associated with digital trust. TwR18 – Compliance: Trustee has to act according to predefined rules, agreements or regulations (e.g., GDPR for data protection). Primarily associated with social trust, this requirement also determines digital trust and trust in technology. It is related to auditability, transparency and traceability requirements. TwR19 – Auditability: Trustor must be able to validate the trustee’s compliance with predefined rules (e.g., by executing the audit, by examining the execution traces, by supervising the trustee's process at run time etc.). TwR20 – Transparency: Trustee’s process (e.g., workflow) must be transparent and explicitly documented. Trustee must provide an accessible and non-repudiable audit trail showing use, change and viewing of the data. TwR21 – Traceability: Trustor has to access any or all information related to provenance of a physical or information object accurately and trace it upward (to its source). This requirement is mainly associated with digital trust. 82 4.4 What Types of Trust are Addressed by the Identified Requirements? (RQ4) In Table 5–7, each TwR is associated with one or several types of trust. Using Table 1, these requirements can be re-formulated for the relevant type of trust, by replacing “trustor” and “trustee” qualifiers by the corresponding types of entities involved in an interaction (e.g., individual, organization, IT object). For instance, in social trust, the compliance requirement (TwR18) can be expressed as follows: A company (= trustee) must obtain explicit consent from individuals (= trustor) before collecting or processing their data. For trust in technology: An information system (= trustee) must provide the means for users (= trustor) to express their consent and must not collect and/or use their personal data without their explicit consent. For digital trust: A blockchain-based system (= trustee) has to ensure that no personal data is collected and/or used by the network from the connected individuals or devices (= trustor) without their explicit consent. Our analysis shows that, in spite of growing importance of digital trust, social trust and trust to technology remain important drivers in organizational decision making. The TwR identified in this study not only drive the design and development of trust-enabling technological solutions but also determine relationships between individuals and organizations. They reflect the needs for all three types of trust in organizations. 4.5 At What Abstraction Levels the Trustworthiness Requirements are Defined? (RQ5) Trust concerns can be expressed by stakeholders at different abstraction levels, characterized by their scope (e.g., organization, activity, application) and/or vocabulary used. In this study, we map the identified TwR onto three abstraction levels consistent with the discipline of enterprise architecture [100], [101]: strategic, operational and IT level. To support the solution design, each trust concern should be addressed by TwR formulated at adequate abstraction level. The strategic level addresses the organizational vision and strategic objectives. For instance, for the trust issue from (S22):“In the gem industry, provenance of origin is critically important for environmental, social, and regulatory reasons.” the corresponding traceability requirement (TwR21) will be formulated as follows: Buyer (= trustor) has to access any or all information related to provenance of a gemstone accurately and trace it upward (to its source). The operational level defines how these strategic objectives are to be met through the business processes and operations. Consider the following issue from (S28): “the title registry is vulnerable to modification, essentially, the title records could be manipulated by malicious parties”. The corresponding integrity requirement (TwR16) will be formulated as follows: The title registry provider (= trustee) must ensure the overall accuracy, completeness, and consistency of data over its entire life-cycle. This includes data protection from unauthorized modification or alteration. The technological level focuses on the IT resources necessary for the digitalization of these operations and processes. Examples of TwR expressed at technical abstraction level include: “architectures must also guarantee the integrity and the confidentiality of data while remaining resilient to distributed attacks.”(S6); “citizens should not have privacy concerns about the information systems. These trust issues and privacy concerns can be solved by using decentralized identity and zero-knowledge proof-based mechanisms.” (S12). 5 Discussion Figure 5 summarizes the scope and the contributions of this work and presents the directions of the future research. The selected 56 primary studies constitute the input for this work (I). First, we provided a descriptive overview of the collected primary studies and analyzed the nature of their research contributions (C1). Our analysis shows that 86% of the examined research is grounded on empirical data and contributes to the domain with new evidences and proposals. 83 Models, frameworks, and new concepts are developed in 48% of the examined publications. Working prototypes are presented in 46% while methods and approaches for design are addressed by 23% of studies. This encourages further research on new design approaches based on explicit analysis of TwR. During our data extraction, we focused on the trust issues and trust-related requirements expressed in these primary studies. Following the ISO guidelines, we formulated a set of 21 TwR consistent with the related works (C2). While originated from blockchain literature, the TwR are formulated in a technology-neutral language and do not advocate blockchain or any other specific technological solution. To bridge the gap between social and technological domains, we associated the identified TwR with the three trustworthiness factors defined in the social science: ability, benevolence and integrity (C3). Our literature analysis shown that trust issues can be expressed by stakeholders at different organizational levels, varying in scope and technical details. We discussed how TwR can be mapped on three abstraction levels consistent with enterprise architecture (strategic, operational and IT) (C4). Figure 5. Overview of the contributions and the future work While being strongly presented in the literature, digital trust is not the only form of trust that the organizations seek to reenforce: trust in technology and social trust remain powerful drivers in decision making. Specified in the problem domain and technology-neutral, trustworthiness requirements presented in this work are not bound to blockchain technological solutions and can be used to drive alternative design decisions and technological choices. We plan to elaborate on this topic in the future (F). 5.1 Trust as a Value vs. Trust as a Requirement To enhance the trustworthiness of systems, a significant investment is needed in the requirements, architecture, design, and development of systems, alongside a fundamental shift in organizational culture [102]. The degree of trustworthiness achievable in complex systems today depends on our ability to integrate both social and technical perspectives of trust. An important number of articles examined in this study addresses trust from the solution provider perspective (some examples include [11], [16], [71], [72], [77], [80], [91] here trust is ): 84 considered as a value created for an end-user by a given technological solution (e.g., blockchain- based system), whereas the user’s need for trust or trust concerns are taken for granted and rarely elicited. As a result, many research studies report on technology acceptance issues [11], [70], [71], [77] . To ensure better fit between trust-enabling solutions and organizational needs, deeper understanding of trust concerns and explicit analysis of TwR is needed. The advantage of system design based on explicit TwR is twofold: 1. For organizations and end-users: Translating subjective (and often implicit) trust issues into and explicit trustworthiness requirements, an organization develops better visibility and understanding of potential threats, risks and priorities. It can clearly express its needs and ensure better strategic alignment of its prospective (trust-enabling) solutions. 2. For solution providers: Shifting from design of value-creating features to meeting specific trustworthiness requirements, technology providers and solution developers can ensure better acceptance for their (trust-enabling) solutions. 5.2 Threats to Validity and Directions for the Future Work This study follows a systematic literature review approach [20] to ensure accuracy and eliminate bias, nevertheless the following limitations can be listed: This study examines primary research focused on design of blockchain-based solutions. This threatens completeness of our presented requirements taxonomy. More general analysis of trust issues can bring new insights and extend this taxonomy. This SLR reveals very little agreement on trust definition in blockchain community. Whereas both social and technology-centric definitions of trust are in use, the majority of studies do not provide an explicit definition of trust. This discrepancy in trust definition represented a challenge during data extraction, analysis and coding. The authors often had to rely on their experience and interpretation, what presents a threat to the internal validity of this study. Among identified TwR, confidentiality, integrity, availability, authentication and non- repudiation are properties commonly associated with information security. Detailed analysis of relation between trustworthiness and security and between security and trust will be addressed in our future work. This study presents our preliminary findings on how TwR can be expressed at different abstraction levels (strategic, operational, IT). In our future work we plan to elaborate on this important topic by formalizing TwR for different enterprise architecture levels. This work presents a list of generic TwR. Healthcare, supply chain management, banking, IoT are examples of domains addressed by the articles analyzed in this study. Domain-specific taxonomies of trust issues and their corresponding TwR may be of particular interest for practitioners from these domains. The work presented in [103] addresses TwR in supply chain management. Other domains need to be addressed by researchers in the future. 6 Conclusion In this work, we followed the SLR guidelines defined by Kitchenham et al [20] and reviewed 56 primary research studies in ISE and blockchain that focus on trust conceptualization, trustworthy system design and development. We analyzed the trust issues presented in the literature and formulated a set of 21 TwR following the ISO guidelines. Our goal is to provide support for business and technical experts who seek to identify and articulate the scope of a problem related to trust, and to lay out the arguments that will guide design decisions and technical choices. Generalizability and completeness of the defined set of TwR is out of the scope for this study and will be addressed in the future. 85",
  "2023-RZS": "1. Introduction Business ecosystem refers to the interconnected business network of organizations and individuals that interact with and influence each other within a particular industry or market. It encompasses the complex web of relationships, resources, and interactions among various entities that collectively contribute to the functioning and success of the overall business environment. With the digital transformation and the increasing role of digital technologies in social interactions, the concept of digital business ecosystem (DBE) has emerged. In a DBE, entities interact and collaborate using digital technologies, and leverage data and information as key assets [1, 2]. DBEs are characterized by their dynamic and rapidly evolving nature. They require effective governance mechanisms to ensure fairness, trust, and accountability among the participants. Governance involves setting common rules, standards, and protocols for data exchange, resource sharing, and collaboration, as well as resolving conflicts, ensuring compliance, and managing risks within the DBE. A key aspect of DBEs is the diversity of actors and the roles they fulfill: in addition to the roles acting in traditional business networks, such as supplier, customer, and end user, DBEs rely in addition on some specific ones, such as for example - the driver role, for managing the tools that support the DBE; a governor, for providing and/or defining the standards and policies; a reputation guardian - for assessing all DBE actors' trustworthiness, reliability, solvency, and worthiness; as well as several other roles [3]. In DBE digital technology (“D”) acts as a mediator in interactions between the ecosystem participants, with the expectation of increasing trust between them and for providing them with a positive experience [4]. Trust plays a crucial role in the functioning of a DBE, for its resilience. It Companion Proceedings of the 16th IFIP WG 8.1 Working Conference on the Practice of Enterprise Modeling and the 13th Enterprise Design and Engineering Working Conference, November 28 – December 1, 2023, Vienna, Austria irina.rychkova@univ-paris1.fr (I. Rychkova); jelenaz@dsv.su.se (J. Zdravkovic); js@dsv.su.se (J. Stirna) 0000-0002-1100-0116 (I. Rychkova); 0000-0002-0870-0330 (J. Zdravkovic); 0000-0002-3669-832X (J. Stirna) © 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). CEUR ceur-ws.org Workshop ISSN 1613-0073 Proceedings provides the foundation upon which participants collaborate and share resources and as such it is a critical design consideration for the DBE supporting digital platforms. Identifying, modeling and analyzing trust relations among social and technical DBE entities is a vital design step, which requires adequate Enterprise Modeling methods and practices. Explicit analysis of trust issues in a DBE has an impact on the DBE strategy, as it can be used to identify partners and their needs in terms of trust; on the DBE operations as it can affect the processes between DBE partners; and on the DBE technology, as it can help to design relevant components and to make technological choices. In social science, trust is described as a situation in which an individual or an organization (trustor) is willing to rely on the chosen actions of another individual or organization (trustee) [5]. According to Mayer et al. [6], ability, benevolence, and integrity are the factors of (perceived) trustworthiness that characterize a trustee. In the technical domain, trust defines relationships between an individual and a technological component (trust in technology) and describes the interactions between the entities in the digital world (digital trust). Here the (perceived) trustworthiness is often connotated with security, reliability, and authenticity of digital systems, platforms, or transactions. The gap between the social and technical definitions of trust arises due to the challenges of translating a subjective, context-dependent nature of social trust into objective, measurable terms that can be addressed by technical mechanisms. While technical (or digital) trust can provide a foundation for secure and reliable digital interactions, it may not fully capture the complexities of social trust that arise from human relationships, emotions, and cultural factors. DBEs are inherently socio-technical systems, and addressing trust in DBE requires a holistic approach that integrates both social and technical dimensions of trust. Bridging the gap between these dimensions involves recognizing the interplay between different types of trust, understanding the subjective and contextual nature of trust issues, and leveraging both social and technical mechanisms to foster trust in DBEs. The goal of this work is to explicitly address trust and its implications in DBE design. In this paper, we examine the roles of DBE and discuss their trust relationships. First, we associate the DBE roles with social trustworthiness factors. To bridge the gap between the social and technical dimensions of trust, we propose a mapping of (social) trust issues into trustworthiness requirements (TwR) that can guide DBE design. We define trustworthiness requirements as the expectations of one actor (trustor) about trustworthiness of another actor (trustee) in a DBE. We demonstrate our findings with a case study of European universities forming a higher-education (HE) alliance, which fulfills the main criteria for being considered a DBE. We examine the trust building process among the actors of this DBE, focusing on the implications on the supporting information systems. We formulate the following research questions: • RQ1: What are the social factors of trust defining relationships among DBE actors? • RQ2: What are the trustworthiness requirements that guide the design and development of a DBE and its supporting systems for the case of a HE alliance? In order to formulate the TwR for a particular role in the HE alliance DBE, first, we analyze the trust issues expressed by a corresponding DBE participant and their (social) trust factors, then we use a reference list of TwR derived from the literature [7] and identify relevant generic requirements. Finally, we illustrate how these generic requirements can be contextualized for the HE alliance. The proposed approach bridges the gap between the social and technical dimensions of trust and supports business and technology experts in guiding their design decisions and technological choices. The remainder of this article is organized as follows: in section 2, we discuss the background of this study and its related works; in section 3, we provide a mapping of the generic DBE roles on the trustworthiness factors defined in social science. We also describe our approach for trustworthiness requirements elicitation. In section 4 we present our findings on the case study of higher-education alliance. In section 5 we discuss our results and provide our conclusions. 2. Background and related work 2.1. Trust In the research literature on trust, the act of trust is represented as a relationship between a subject (the trustor) and an object of trust (the trustee) [5, 6, 8]. Outcome of trust is defined as an interaction between trustor and trustee and is characterized by the resulting experience (negative or positive). Antecedents of trust refer to the factors that influence trustor’s willingness to trust and include factors related to the subject (trustor’s propensity to trust), to the object (trustworthiness of the trustee) and to the environment where interaction between the subject and the object takes place (e.g., institutional trust) [6, 8, 9]. In this study, we consider trustor’s propensity to trust and institutional trust as invariant for a given interaction. Our primary focus is on trustworthiness of the trustee as a design variable. Whereas researchers in social sciences focus on trust between social entities (individuals, groups or organizations), in IS research, trust is considered as a socio-technical concept, i.e., it is defined as a relationship between social entities and technological components (information systems, applications, infrastructure, etc.), in which a technological component can be either an object (trustee) or a subject (trustor). In modern organizations, social trust remains an important determinant of collaboration and decision making. With a constant digital transformation, trust issues that occur among social actors on the strategic and operational levels of the organizations are often addressed by socio-technical solutions developed by the IT, creating a gap between the social and technology-centric perspectives of trust. To bridge this gap, it is important to recognize the multidimensional nature of trust and consider the social and cultural contexts in which technological systems are developed and used [4]. Three forms of trust are widely recognized in the literature: social trust, digital trust, and trust in technology. Social (or interpersonal) trust is defined as the subjective probability that a trustee has the required capacity and willingness to perform an action that is beneficial or at least not detrimental to another entity - a trustor - in a specific context [5]. Digital trust defines relationships between entities in the digital world. It is the measure of confidence that a trustor has in the trustee's ability to protect data and privacy of individuals [10]. Trust in technology reflects the trustor’s beliefs that a specific technology has the attributes necessary to perform as expected in a given situation where negative consequences are possible [8, 11]. Table 1 presents a summary of these three types of trust, their associated trustworthiness factors and outcomes. Table 1 Overview of trust perspectives and types Type of Trustor Trustee View: Trustworthiness factors Outcome Trust (subject) (object) laicoS Org. / Org. / Ability, benevolence, Interaction / Social Trust Individual Individual integrity collaboration Trust in Org. / Functionality, helpfulness, IT object Acceptance, use Technology Individual usefulness, reliability Org. / IT object Privacy, security, Interaction / Individual Digital lacinhceT transparency, traceability, transaction in digital Trust Org. / control environnement IT object Individual IT object IT object 2.1.1. Trust: Social perspective It addresses the (social) context where the trust issues among the actors arise. [6] defines trust antecedents and outcomes in their integrative model of organizational trust. The authors define the trust for a trustee as “a function of the trustee's perceived ability, benevolence, and integrity and of the trustor's propensity to trust.” Propensity to trust is an intrinsic characteristic of a trustor, which can be considered as invariant. Ability, integrity and benevolence are the factors of trustworthiness that characterize a trustee; they depend on the context and the nature of a given trustor-trustee interaction. According to [6], ability defines a group of skills, competencies, and characteristics that enable a trustee to have influence within some specific domain; benevolence defines the extent to which a trustee is believed to want to do good to the trustor, aside from an egocentric profit motive; integrity refers to trustee’s moral quality of being sincere and his/her willingness to adhere to some rules and principles. Social trust is used as the basis for decision-making in diverse contexts, including enterprise strategy, governance of operations, and technology [12]. 2.1.2. Trust: Technical perspective Digital trust and trust in technology define trust in the technological domain. The trustworthiness factors of technology include usability, functionality, helpfulness, reliability and credibility of information [8, 11], as well as customizability and adaptability [4]. Digital trust reflects the trustor's beliefs that a trustee (a social entity or an IT object) has the attributes necessary to support secured digital interactions [10]. Trustworthiness factors in digital trust include privacy, security, transparency, traceability, control [13, 14]. 2.1.3. Bridging the gap between the social and technical views on trust While trustworthiness factors of digital trust and trust in technology can be formalized, measured and used to provide a foundation for technological solutions, they may not fully capture the complexities of social trust that arise from human relationships, emotions, and cultural factors. Thus, an explicit mapping between technological and social perspectives of trust is of great importance. Requirements engineering (RE) discipline plays a crucial role in design and development of socio- technical systems. The RE process involves understanding the stakeholders' needs and expectations, as well as the social and organizational context in which the system will operate [15, 16]. We address trustworthiness of the trustee in socio-technical systems from the RE perspective. Here, trustor's expectations regarding the trustee's trustworthiness can be expressed as trustworthiness requirements (TwR). We define TwR as a statement made by a trustor about the expected trustworthiness of a trustee. A TwR has to clearly express an operational, functional, design or other characteristic, which, according to trustor’s beliefs, positively impacts trustworthiness of this trustee and interaction between the two. TwR can be met by incorporating certain attributes, features, or properties by the trustee, whether a social entity or a technological solution. TwR can be eventually refined into conventional FR, NFR, process requirements or contracts. The interest in requirements related to trust is not new: in [16], trust is considered as a part of soft requirements (SR) and is associated with the aspects of the social system where a technological system is used – its context; in [17], trustworthiness requirements are defined as a special class of quality requirements and relate trust with other concepts such as capability, vulnerability and risk; in [22], the role of trustworthiness in the software development lifecycle is examined and a process for elicitation and analysis of TwR is proposed. The work presented in [21] explicitly addresses TwR in supply chain management. In this study, we examine the social trustworthiness factors that define interactions among DBE actors. First, we provide a mapping between these factors and generic DBE roles. Then, using the case study, we illustrate how the trustworthiness factors of the trustee in a DBE can be addressed by the TwRs. As a result, we identify requirements that need to be met by the DBE roles and their supporting digital solutions, providing guidance for DBE design and evolution. 2.2. Digital business ecosystems (DBE) and roles in DBE Actors, roles, capabilities, relationships, and digital components are essential elements of DBE [3]. The actors are individuals and organizations that take part in a DBE by fulfilling specific roles according to their capabilities. The interactions between the DBE actors are supported and mediated by different digital components such as the ecosystem digital platform and its services, smart devices, cloud storage, and other. Roles of archetypal kind are of a high significance for the ecosystem’s design as they define the DBE-specific responsibilities of the actors involved and provide underlying knowledge for the capabilities relevant to a DBE. In [3], the authors surveyed the relevant literature to identify the DBE roles and their responsibilities, leading to the following ones (Table 2). Table 2 DBE roles and their responsibilities [3] DBE role Responsibility Driver sets up a common vision for all actors in a DBE; provides and manages a digital platform; optimizes entry barriers for joining a DBE; acquires and retain actors within a DBE; provides end-products and services to customers and end-users; collects and raise end users’ events and feedback; ensures an integrated end user experience. Aggregator collects and combines capabilities and resources within a DBE into end-products or services, created by Modular Producer and Complementor, for offering to Customers and End-Users. Modular provides resources within a DBE; resources can be products, services, or Producer knowledge, created by the producer’s capabilities. Comple- using its capabilities, provides resources that complement the core resources within mentor a DBE, with some added-value features. Customer buys end-products and services offered in a DBE. End-User consumes end-products and services offered in a DBE; provides information about its events and feedback to other DBE roles. Governor oversees all the actors within a DBE by defining normative artifacts, such as decisions, policies, guidelines, and ethics, related to the business concern of the DBE. Reputation surveys and assesses all DBE actors' trustworthiness, reliability, solvency, and Guardian worthiness. 3. Analysis of trust issues and identification of trustworthiness requirements in DBE 3.1. Research approach This study follows the Design Science Research [18] and aims at developing a framework for the trust management in DBE types of business networks – the targeted design artifact. The need for managing trust and hence for this design artefact is expressed in [3, 19], where trust is identified as one of the important aspects of DBE design. This study paves the ground for developing the trust management framework for DBE. In this article, we report on the initial cycle of artifact design, which includes the problem identification and the framework components design and development. The theoretical view on the problem was presented in [7]; this paper is grounded on the case study and focuses on the empirical view of the problem. We conduct a structured analysis of the archetype DBE roles and identify the trustworthiness factors that determine the interactions between these roles. The resulting mapping (Table 3) is one of the framework components developed in this study. Following the identified trustworthiness factors, we proceed with identification of trustworthiness requirements (TwR) that can be further operationalized (i.e., implemented as a part of an interactive process or a supporting information system between the corresponding DBE roles). To this end, we propose and follow a process for trust analysis (Section 3.3). This process takes trust issues expressed by the specific DBE actors as an input and leads to identification of their corresponding TwR. To support the trust analysis, we use a set of generic TwR from [7]. We demonstrate the designed artifact by examining trust in the Higher-Education Alliance DBE – our case study (Section 4). In this article, we provide the results of trust analysis for the Modular Producer role in this DBE. In particular, we show the trust issues (collected from the case), trustworthiness factors (application of our mapping), generic TwR (from [7]) and specific (contextualized) TwR defined for this role. Completeness of the elaborated set of requirements as well as their prioritization are not discussed in this study. This will be addressed during the following (validation) cycle of DSR. 3.2. Trustworthiness factors in DBE In DBE, trust relationships are formed among their participants (social entities) and can be characterized by the following: (i) several entities can share the same DBE role and each entity can fulfill several DBE roles; (ii) within different interactions, each DBE role can be considered as a trustor (one who trusts) or as a trustee (one to be trusted). Following [6], ability, integrity, and benevolence are the factors of trustworthiness that influence a decision of one DBE role (trustor) to engage into an interaction with another DBE role (trustee). The impact of ability, integrity, and benevolence on building trust can vary depending on the context of this interaction. More specifically, consider a situation 1, where the two individuals X and Y are respectively a patient (trustor) and a physician (trustee), and a situation 2, where the same X and Y are playing cards together: in situation 1, the Y’s ability (i.e., medical proficiency and qualification) can be a major trustworthiness factor for X, whereas in situation 2, it will be rather Y’s integrity (honesty, compliance with the rules). Based on that, the third characteristic of trust relationships in DBE is: (iii) Trustworthiness factors defined by a trustor for a trustee within an interaction in DBE depend on the context of this interaction and on the DBE roles they play within this interaction (as defined in Table 2). Table 3 Social trustworthiness factors in the relationships to the DBE roles Trustors (subject) rotnemelpmoC rotagerggA noitatupeR remotsuC ronrevoG naidrauG resU recudorP raludoM revirD dnE Driver A B, I A A, B, I A A A, I A, I A, B, I Aggregator A, I I A, I A, I A, I A A, I A, B, I Modular A, I A I A, I A A A, I A, B, I )tcejbo( Producer Complementor A, I A A, I n/a A A, I, B A, I A, B, I eetsurT Customer I I I I I I I I End User n/a n/a n/a A, I n/a n/a A, I I Governor I, B I, B I, B I, B I, B I, B A, B, I A, B, I Reputation B B B B I, B B A, I n/a Guardian Based on our previous studies on DBEs [3, 19, 20], we analyze trustor-trustee relationships between different DBE roles and identify the major social trustworthiness factors in trust building between these roles. The results are illustrated in Table 3. Each cell {i,j} defines a trustworthiness factor (or factors) for an interaction between the two DBE roles: role i (as a trustor) and role j (as a trustee). For example, the third column of the table defines the trustworthiness factors for a DBE Modular producer (MP) role towards the other roles in the DBE with which the MP interacts as a trustor. The MP (trustor) - Driver (trustee) interaction in DBE is important to ensure consistent development and evolution of a service or product provided by the DBE. A, B, I in the cell {3,1} indicate that all the three factors – ability, benevolence and integrity - need to be considered when designing processes and digital platforms supporting and mediating their interactions. Trustworthiness factors in MP – Aggregator and MP - Complementor interactions (cells {3,2}{3,4} in Table 3) include ability (A) (e.g., skills/competences of an aggregator to collect and combine capabilities and resources within a DBE) and integrity (I) (e.g., aggregator’s honesty, capacity to adhere to the rules defined by DBE). Integrity (I) is the major factor in MP – MP and MP - Customer interactions (cells {3,3}{3,5} in Table 3). Here, integrity of MP refers to their perceived honesty in delivering a high-quality service/product. Customers’ integrity refers to their perceived honesty and compliance with the rules. Trustworthiness factors in MP - Governor interactions include benevolence (B) and integrity (I). This is related to the responsibility of the governor as a trustee, which is to oversee all the actors within a DBE (Table 2). Benevolence (B) is the major factor in MP - Reputation guardian interactions. The responsibility of the reputation guardian as a trustee is to survey and assess all DBE actors (see Table 2) and benevolence (e.g., a belief that this evaluation will be fair) provides a major contribution in building trust in these interactions. Trustworthiness factors are not applicable to MP (trustor) - End user (trustee) interactions (indicated n/a in the cell {3,6}) since, by definition, MP role in DBE does not “rely on” or “become vulnerable from” the End user. Note that the opposite is not true: End user as a trustee has to trust the MP's ability to produce a competitive, relevant service or product. This is reflected by the ability (A) trustworthiness factor in Table 3 (cell {6,3}). The rest of the table can be interpreted the similar way. 3.3. Analysis of trust in DBE Table 3 maps the trustworthiness factors on DBE roles and identifies the major social factors of trust in the interactions among DBE partners. To support digital interactions between DBE partners, these factors need to be contextualized and refined into specific TwRs. We propose the following process for trust analysis in DBEs. Consider an interaction between two specific DBE actors and the roles they play in this interaction: 1. Identify trust issue(s) of a trustor actor. This step is context-specific and can vary for different partners in the DBE. The working approach can be: empirical analysis of DBE design and operations or interviews with stakeholders. 2. Identify the trustworthiness factors of the trustee role related to this issue. This step is context-independent and defined for generic roles in DBE, c.f. Table 3. 3. Formulate TwR that express trustor’s expectations about trustee’s (social) trustworthiness factors from step 2 by using (technical) trustworthiness properties (i.e., system or process qualities). This step can be considered as a context design. Here we are working with engineers of the DBE to analyze the existing DBE design. The requirements can be extracted from this context or identified using a more generic reference list, derived from the previous experiences or from the literature. 4. Contextualize the TwRs by associating them with the trust issues identified in step 1. In this step, we are focusing on specific requirements of actors and the DBE as a whole. Here, the TwRs from step 3 are refined following the interviews with the actors’ representatives and analysis of the usage data. The expected outcome of this process is a set of explicit, contextualized TwRs that provide a reference to the social context and identify an operational, functional, design or other characteristic, which, according to trustor’s beliefs, positively impacts the trustworthiness of this trustee and interaction between the two. In the following section we illustrate this process with the case study of Higher-Education Alliance DBE. 4. Case study: Higher-Education Alliance 4.1. About alliances in Europe During the past decade a plethora of university alliances in the domain of higher education have emerged, with more than 40 of such alliances in Europe. Some alliances focus mainly on student mobility (e.g., Erasmus+), while others are aiming at a united Europe university both in terms of teaching and research (e.g., CIVIS, 4EU+, Una Europa). The latter type is featured in our case study (by the active participation of the authors in one of the outlined alliances). Through their activities and collaboration, these alliances strive to actively promote fundamental rights, solidarity, democracy, social cohesion, cultural diversity, and active citizenship. Therefore, the business foundation of the HE alliances could be condensed into the following knowledge square: Education, Research, Innovation and Civic Engagement. The alliances are typically co-funded by the EU Commission and the member universities. HE alliances perform and coordinate an extensive number and variety of activities including development of educational programs and modules at Bachelor’s, Master’s and PhD levels; student, teacher, and researcher mobility; educational and scientific calls and events; thematic working nodes, theme-labs, promotion-related activities, governance, management of the digital infrastructure. 4.2. The roles and responsibilities of the DBE participants HE alliances conform to the concept of DBE, because they consist of a large number of independent and self-organizing actors collaborating on various business objectives on a DBE level as well as individually. A key aspect of DBEs is actors acting in complementary roles, which is essential to maintain DBE’s long-term resilience. Table 4 shows the mapping of the common actors of an HE alliance to their corresponding DBE roles and responsibilities. Table 4 Actors and roles in the DBE of HE alliance Actor Description Role in Responsibility in DBE DBE European The financier of an Governor, To control the use of fundings, Commission alliance. Reputation monitoring of the progress, alliance Guardian promoter in EU forum University Alliance member, Driver Each university member leads one from a European responsibility (Table 2, Driver), or all university are responsible for some Faculty teacher, Academic staff of the Modular To develop and teach course researcher participating Producer curriculum universities Node Thematic entity Aggregator To propose course curriculum, assign tasks to modular producers and monitor development. Lab Forum for universities, Compleme To organize events (conferences, businesses, citizens to ntor seminars), present curriculum, etc. meet Steering Administrative staff of Governor To make decisions on operative levels, Committee participating to coordinate communications and universities. tasks of the universities. Consultative City and regional Governor To make cooperation decisions that Council representatives, would be applied across the citizens, and the participating regions presidents of the member universities Student Council A group of student Governor To collect and disseminate student representatives from voices for the best interests of different university students: it listens, exchanges and members proposes ideas on how the alliance should develop. Student A person registered End-User To attend campus and online courses, for studying at a take examinations, to do course member university evaluation Student First-contact Reputation To provide information about the Ambassador student(s) at every Guardian alliance to potentially interested member university. students. Business Regional organizations Customer To sponsor and attend some events of member and companies the alliance, provide guest lectures, etc. Citizen Regional citizens Customer To support co-creation of knowledge related to the curriculum content, collaboration with business, and other. Communication The alliance Reputation To encourage the participation of all Office representative office Guardian stakeholders in building the in EC envisioned university model. 4.3. Trust analysis for the Educational Alliance In this section, we provide the trust analysis for the Modular Producer role (a faculty teacher or researcher) in a HE Alliance DBE. For the sake of brevity, we do not provide the analysis for the other roles in this paper. Following the process of trust analysis (Section 3.3), we illustrate (1) the trust issues identified in the interactions between Modular producer (as trustor) and other roles in the HE alliance DBE (trustees) and (2) provide their mapping to the trustworthiness factors of DBE roles from Table 3. Next, in (3), we use the taxonomy proposed in [7] as a reference to formulate our TwR about the trustworthiness factors identified in (2). This taxonomy associates ability, integrity and benevolence with 21 TwRs derived from the literature. Finally, in (4), we contextualize the identified TwRs for the MP in the HE alliance. The summary of this analysis is presented in Table 5. Modular producers (MP) in the HE alliance are members of teaching and research staff responsible for creating content for educational programs (course materials, practical works, projects etc.) and delivering the program to the end users. When creating common courses, one of the challenges is to ensure alignment, consistency, and uniform quality of the course modules among different MPs. Therefore, an issue expressed by a trustor-MP towards the other MPs (trustees) is: 1. I am concerned with the quality of modules provided by other modular producers. This issue is associated with integrity (I) of the MP role as a trustee in Table 3. Another challenge is related to aggregation, dissemination and reuse of developed materials within common space, which are ensured by Aggregator (a node) as a trustee: 2. I am worried that the development of a common course will not follow the established milestones and deadlines. This issue is associated with integrity (I) and ability (A) of the Aggregator role in Table 3; 3. I want to make sure the aggregator will not put me in competition with another modular producers - associated with integrity (I); 4. I am concerned that, within a common course, my content can be used or modified without my knowledge - associated with integrity (I); 5. I am concerned about the integration efforts and evolution of my content: upload, update, formatting should be ensured by the aggregator - associated with ability. Once the program is developed, MPs are also concerned with its running. The following example illustrates the trust concerns towards Driver (a university) as a trustee: 6. I am concerned that the digital platform for course provisioning and communication with students will work without errors - associated with the trustee’s ability (A). Towards Complementor (a lab, a third-party technology provider for) as a trustee: 7. I am concerned with the quality of supporting services and their price (e.g., Virtual classrooms, examination tools) delivered by the complementor - associated with the trustee's ability (A). Towards Reputation Guardian (communication office in HE alliance): 8. I am concerned that a fair number of students, with adequate background and academic records will be attending the course - associated with benevolence (B) of a Reputation guardian. Once the issues are identified (column 1 in Table 5) and associated with the trustworthiness factors (column 2 in Table 5), we formulate the TwR of the MP (as a trustor) towards the DBE (column 3 in Table 5). In [7], a taxonomy of TwR is proposed. This taxonomy associates the (social) trustworthiness factors with technical features of solutions. We use the TwR from this source as a reference. For example, issue 1 can be associated with Auditability TwR. Once relevant TwR are identified, they need to be contextualized (column 4, in Table 5). For issue 1, we propose the following contextualization of the Auditability TwR: Any faculty teacher in the node must be able to validate the quality of the class materials produced by their peers. Every faculty teacher has to demonstrate the quality of the produced course materials. Note that the issues can vary among the actors playing the same DBE role (e.g., different teachers in HE alliance); they can also be shared between the roles in the DBE (e.g., issue 2 is shared by the MP and the driver role). The process above needs to be conducted for all DBE participants to collect the list of issues and requirements for each relevant trustor-trustee interaction in the DBE. Table 5 Trust analysis for HE Alliance Modular producers. (2) (3) TwR of reference (4) Contextualized TwR of reference 1 I Auditability: Trustor must be able to Any faculty teacher (Modular Producer) validate the trustee’s compliance with the in the node must be able to validate the rules (e.g., by executing the audit, quality of the class materials produced by supervising the examining the execution their peers: fit to the program, alignment traces, supervising the trustee's process at between the modules, etc. Every faculty run time). teacher should be able to demonstrate the quality of the produced course materials. 2 A, Performance: Trustee must ensure an A node (Aggregator) creates the I efficient distribution of resources, with educational programs, with respect to the respect of defined timeframe and budget. program calendar and budget set by the Compliance: Trustee has to adhere to rules, steering committee (Governor). agreements or regulations. A node acts according to the rules defined by the steering committee (Governor) and uses the standard solutions (e.g., digital Integrity: Trustee must ensure correct and portal) delivered by a leading university timely execution of activities, with respect (Driver). to contract or process specifications. A node demonstrates the program evolution to the steering committee and informs the students and faculty teachers about problems. 3 I Traceability: Trustor has to access all Faculty teacher and faculty researcher has information related to provenance of a to be able to access all the information physical or information object accurately related to other modular producers and to and trace it to its source. trace the produced content. 4 I Transparency: Trustee’s workflow must be A node’s course development plan (e.g., transparent and documented. Trustee must workflow) must be transparent to the provide an accessible and non-repudiable faculty teachers and explicitly audit trail showing use, change and documented. viewing of the data. A node must ensure the overall accuracy, Integrity (data): Trustee must ensure the completeness, and consistency of accuracy, completeness, and consistency of produced content over its entire life- data over its entire life-cycle. cycle. 5 A Automation of data processing: Trustee A node must minimize physical and must minimize physical and maximize maximize digital processing of course digital processing of data. materials and to enable students (End- Interoperability: Trustee must show a User) to access it remotely. capability to work with trustor. A node must be able to process, store and correctly integrate any numeric content from the faculty teachers. 6 A Availability: All resources and software Digital platform for student and course components needed for process/activity management has to be available to execution have to be available to the students and faculty teachers. The content trustor, by the trustee. must be accessible which is ensured by the designated university (Driver). 7 A Availability: All resources and software All resources and software components components needed for process/activity needed for the course have to be available execution have to be available to the by the digital platform through a trustor. university, to faculty teachers. Performance: Trustee must ensure an Lab (Complementor) must ensure an efficient distribution of resources, with efficient distribution of resources, with respect of defined timeframe and budget. respect to defined timeframe and budget. 8 B Accountability: Trustee is held responsible Steering committee is responsible for her for her actions and cannot deny them. actions in marketing, dissemination of Authentication (data): Trustor must be able the calls and student inscription to the to determine the correctness and reliability program. of reported data (e.g., messages, events). Faculty teachers must be able to determine the correctness and reliability of reported data (e.g., application/admission ratio, information on the students). 5. Discussion and conclusions Trust is a critical enabler of business interactions facilitating effective collaboration, efficient resource utilization, adaptive behaviors, and collective effort towards common goals. Business networks, as inherently socio-technical systems, require a holistic approach for trust analysis that integrates its social and technical dimensions. This study attempts to bridge the gap between these dimensions by incorporating the subjective and contextual nature of trust in DBE designs and management principles. Identification and analysis of trust issues among the participants of a DBE is a crucial task with a great impact on the DBE sustainability and resilience; it must be conducted upfront and it requires adequate enterprise modeling methods and practices. In this work, we proposed a framework for structured analysis of trust among the actors of DBE. We focused on the implications on the supporting digital platforms pervading any business interaction in a DBE setting. We consider that the proposed framework can be used to support (re)design of DBE and its supporting digital platforms as follows: • A list of requirements aggregated per Trustee-role provides a vision of what the DBE expects from this given role. • A list of requirements aggregated per Trustor-role provides a vision of what this particular role expects from the DBE. • Prioritization of the TwRs by identifying the TwRs most frequently expressed. • Negotiation of the TwRs and identification of the minimal set of TwRs that will be satisfactory for a particular DBE. • Identification and assessment of alternative organizational and technical solutions to cover the set of TwRs. The study is a part of an overall Design Science Research project aiming to develop and implement the models and methods for resilient DBE. Within this project, we are defining the artifacts needed for incorporating the trust aspect into the DBE design: the identification and mapping of the social trustworthiness factors on the DBE roles, and a process for trust analysis serving for deriving TwRs specific to the ecosystem in design. The proposed artifacts were demonstrated to validate their usability on the case of HE alliance - a typical example of a DBE, with its high autonomy, self- organization, and cost balance principles. Concerning limitations to this study, we have performed only the initial cycle of development – the problem has been analyzed in sufficient detail to establish requirements for the artifact and its initial version has been developed and validated in an artificial setting with real life case. While this gives input to assess the validity of the artefact in broad terms, systematic evaluation in naturalistic setting is also needed. The immediate next work will comprise further refinement of the framework and experimentation, for example., on other DBEs, to assess possible improvements for the purpose of evaluation of the framework in artificial setting which is to be followed by improvements to the framework and the guidelines for use in order to integrate the framework with a method for DBE design [23].",
  "2024-BRLG": "1 INTRODUCTION\nIn the age of data, knowledge is an essential factor that increases the capacity to make the best decisions (North and Kumta, 2018). Data visualization remains a difficult task for knowledge extraction as numerous visualizations are available and each business\nand/or application face their own challenges (Andrienko et al., 2020)(Padilla et al., 2018)(Engebretsen\nand Kennedy, 2020). It requires a perfect understanding of the goal to achieve and the manipulated data.\nVisualizing text corpora is a cross-domain application of interest for information systems and digital\nhumanities (Ja¨nicke et al., 2015). Improvements in\nvisualization lead to better analysis of multiple types\nof texts like historical newspapers (Menhour et al.,\n2023) or even poetry (De Sisto et al., 2024). However, few work has been done to visualize the relevance between documents based on their topics. Topic\nmodeling also contributes to texts’ analysis as a research orientation of the information retrieval field\nby ”uncovering latent text topics by modeling word\nassociations” (Hambarde and Proenca, 2023). Topics are simply a list of words that share a similar\ntheme: either each word is strongly/directly expressing the theme, or the collection of words illustrates\nan abstract theme by their semantic links (but alone,\nthey would not make sense). Multiple topic modeling\nmethods exist (Alghamdi and Alfalqi, 2015)(Kherwa\nand Bansal, 2019). They usually consider documents\nas a bag-of-words where the order of words is not important; only their occurrence in each document is important. Recent advances in natural language processing (NLP) also introduced neural networks combined\nwith traditional methods, allowing the capture of the\ncontext of words within documents and reusing it to\nanalyze newer documents.\nIn this paper, we explore the use of Formal Concept Analysis (FCA) (Ganter and Wille, 2012) instead of more traditional topic modeling methods,\nand we propose a visualization of the main keywords\nof a corpus and documents’ relevance on a forcebased graph. FCA is known as a viable text mining\nmethod (Carpineto and Romano, 2004) and is a good\ncandidate for multiple applications in the knowledge\nfield (Poelmans et al., 2013). FCA has been used in\nconjunction with a topic modeling method (Akhtar\net al., 2019) but not instead of it. The strength of\nFCA resides in the fact that it analyzes the relationships within data and produces a lattice that can be\nused for calculating useful measures like similarities.\nThe paper is organized as follow: First, we explain\nthe main methods currently used in topic modeling as\nwell as topic and relevance visualizations. Second, we\npresent our processing pipeline from the input documents to the output map. Then, we present three experiments, the text materials used, their topics, and\ntheir relevance. Finally, we discuss the results and\nconclude.\n2 RELATED WORKS\n2.1 Topic modeling\nAnalyzing documents implies creating statistics on\nthe used terms. Term Frequency - Inverse Document\nFrequency (TF-IDF) (Salton, 1983) calculates a ratio from the frequency of each term to the total number of documents. Documents are therefore seen as\na ratio of words independent of their ordering, like a\nbag-of-words. TF-IDF is not exactly a topic modeling\nmethod, but it shows the importance and uniqueness\nof terms within the corpus.\nLatent Semantic Analysis (LSA) (Deerwester\net al., 1990) transforms documents into a latent semantic space from which multiple outputs can be analyzed. The main method behind LSA is the singular\nvalue decomposition, which produces three matrices\nbased on a parameter K given by the user: a matrix\nof terms per K features, a matrix of K features per K\nfeatures, and a matrix of K features per documents.\nMultiple analyses can be done on those matrices, but\nin our case, the terms per K features one is the most\nimportant as it allows us to know how well each term\nis linked to each feature (that in fact represents latent topics). One problem induced by LSA is that it\ncan’t manage polysemy: each term is used as the same\nentity in any document. Homonyms, like synonyms\nand even various forms of the same word, can produce an inconsistency because of the missing context\nof each document. Standardization of the input, like\nstemming or even lemmatization, can partially leverage this problem.\nProbabilistic Latent Semantic Analysis\n(PLSA) (Hofmann, 1999) is an upgrade of LSA\nthat introduces a probabilistic point of view by\nbuilding a generative model for each text corpus.\nBecause topics are scattered within documents,\nprobabilities help find the terms that compose them.\nPLSA relies on an aspect model (the probabilities\nbetween terms, documents, and the latent topics) and\na mixture decomposition that obtains similar results\nas the singular value decomposition, allowing PLSA\nalso to build the three matrices of LSA.\nLatent Dirichlet Allocation (LDA) (Blei et al.,\n2003) is also a generative probabilistic model that\naims to model text corpora. It aims to improve the\nPLSA mixture decomposition by using a hierarchical\nBayesian model. LDA allows not only the finding of\ntopics of words within documents, in the case of text\ncorpora, but also its usage with more confidence than\nPLSA as a probabilistic generative model in multiple\ndomains.\nLDA can be combined with Bidirectional Encoder\nRepresentations from Transformers (BERT) (Devlin\net al., 2018) in order to increase the quality of its\nresults (Peinelt et al., 2020)(George and Sumathy,\n2023). BERTs are a collection of pre-trained neural networks designed to help researchers in NLP by\nconsidering words and their neighbors before and after them. By their nature, BERTs do not produce a\nlist of topics but can be used to generate a summary.\nSimilarly to BERTs, it can be noted that Generative\nPre-trained Transformers (GPT) (Brown et al., 2020)\nachieve numerous NLP tasks. Still, they also share\nthe drawback of not explaining the weights within the\nneural network to build their answers.\nCorrelated Topic Model (CTM) (Blei and Lafferty, 2006) is derived from LDA and uses another\ndistribution in order to better capture topics and their\nrelations within documents. Indeed, when a document concerns a theme, it usually talks about some\nneighbor topics: a text about travel might talk about\ntourism, beaches, and airplanes, but probably not\nabout fighter jets. Topics are uncorrelated in LDA because of the Dirichlet distribution, whereas in CTM,\nthanks to the logistic normal distribution, topics are\ncorrelated and present links to one another. The presence of a topic triggers the possibility of finding one\nor multiple other topics. CTM also proposes a visualization of topics: each topic is represented in a bubble\nof words, and each bubble is linked with the other correlated topics.\n2.2 Visualization of topics and relevance\nThe presentation of topics is important as reading a\nlist might confuse a human: a list forces an order of\nreading, which is not always the best one depending\non the context. Visualization of topics is usually made\nof tag clouds (Singh et al., 2017)(Lee et al., 2010),\nwhich is convenient but shows terms as a whole block.\nSome visualizations like (Singh et al., 2017) or (Gre-\ntarsson et al., 2012) show the distinction between topics and the terms composing them. However, if a\ndocument used in the corpus is irrelevant or contains\ntoo many irrelevant parts, the results might be altered\nwithout the user being informed. In addition, each\ndocument must be deeply reviewed to find the discrepancy. Few work has been done to explicit the relevance of documents one to another based on their\ntopics.\n(Assa et al., 1997) presents a relevance map between topics and keywords in the case of web search\nqueries. Their proposal uses dimension reduction to\ncreate a 2D map and gravitational forces for placing\nnodes. Similarly, VIBE (Olsen et al., 1992)(Olsen\net al., 1993) and its variants (Ahn and Brusilovsky,\n2009) create Points of Interest (POI) that act precisely\nlike topics around which documents are placed based\non relevance. The main drawback is that it does not\nhighlight the most important common words and topics but only places resources based on their relevance\nto all the topics found.\n(Fortuna et al., 2005) proposed a map of terms\nand documents based on MultiDimensional Scaling\n(MDS) methods. Terms and documents are placed\non a two-dimensional map, and the background color\nvaries based on the density. The main drawback of\nthis contribution is the difficulty of getting a clear\noverview of the documents and terms.\nIn (Newman et al., 2010), the authors proposed\na topic map after studying topic modeling and 2D\nprojection. First, they compare three topic modeling methods and end by using LDA. Next, they compare three methods of projection, namely Principal\nComponent Analysis (PCA), Local Linear Embedding\n(LLE), and Force Directed Layout (FDL) which is the\nbest. The topic map presents documents as nodes colored by their most important topic. Their position depends on their relevance to one another. However, the\nauthors concluded that the evaluation of visualization\nis complex and could be made only by human judgment; in addition, they also stated that maps with a\ndozen documents are probably the most accurate and\nvaluable in understandability and navigation for a human.\nTopicViz (Eisenstein et al., 2012) proposes a visualization of documents and topics by nodes with\na force-directed layout and, more importantly, interaction with the user. The topic modeling method\nused is LDA. The user can pin topic nodes in the\nworkspace, making the documents float based on their\nrelevance to each topic. Such a map allows one to distinguish which document is more relevant to some or\nmore topics based on its position. The user can also\npin document nodes, making the topics float between\nthem. This visualization is particularly interesting because document and topic nodes can be pinned, allowing it to show relevance. However, the user must pin\nthe nodes himself in order to see the relevance. Based\non the number of detected topics, deciding where to\npin each topic to see the documents’ relevance better\nmight be difficult.\nPaperViz (di Sciascio et al., 2017) is a dedicated\ntool for researchers during the paper-gathering step.\nIt offers multiple views for multiple contexts: tree\nhierarchy for search queries, a tag cloud of the 20\nmost frequent terms, the strength of the relationship\nbetween documents and a search query or a collection, and references management. The main strength\nof PaperViz is its completeness in the user interface.\n3 VISUALIZATION PIPELINE\nWITH FORMAL CONCEPT\nANALYSIS\nThe visualization pipeline comprises two main phases\n(see Figure 1): semantic pre-processing, where documents are analyzed in order to produce an occurrence\nmatrix of terms per documents, and structural analysis, where the matrix is analyzed in order to create\na graphical representation of the relevance. Globally, the pipeline relies on natural language processing\n(NLP) methods in the first phase and formal concept\nanalysis (FCA) methods in the second phase.\n3.1 Semantic Pre-Processing\nThe semantic pre-processing phase aims to extract the\nmost important terms and concepts from each document and gather them within a matrix representing\nthe whole text corpus. This phase is composed of 5\nsteps as follows:\n(PI.0) Selection of documents by the user: the\nuser selects the documents for analysis and comparison. Two requirements must be fulfilled for the best\nresults: each document must have enough content,\nand the content must be mainly textual.\n(PI.1) Extraction of texts: each document is\ntransformed into a regular flat text file. This step relies on optical character recognition (OCR) methods.\nIn our experiments, we used PDFtoText1 as the OCR.\n(PI.2) Cleaning of extracted texts: each text is\ncleaned in order to increase its quality and reduce its\nsize, typically by removing the useless spaces and artifacts that the OCR would have created and, eventually, some of the stop words. In our experiments, we\n1https://www.xpdfreader.com/pdftotext-man.html\nFigure 1: The main steps of the pipeline\nused TreeTagger (Schmid, 1994)(Schmid, 1995) with\na custom list of words to keep.\n(PI.3) Disambiguation: each cleaned text is transformed into a list of named entities by resolving polysemy and synonymy problems. Advanced NLP methods are required for this task. In our experiment,\nwe used BabelFy (Moro et al., 2014) as it understands multiple languages and calculates three scores\nfor each recognized named entity. The named entities\nare also transformed into unique references from BabelNet (Navigli and Ponzetto, 2012), allowing us to\nmanipulate the exact same named entities in all documents, whatever the input languages are.\nFigure 2: The formal concept analysis (PII.1) sub-steps\n(PI.4) Filtering of terms: the most irrelevant\nnamed entities are removed based on the coherence\nscore attributed by BabelFy in the previous step. In\nour experiments, we require a coherence score of at\nleast 0.05 to keep a named entity. This score was empirically chosen because it removes way more irrelevant named entities than relevant ones.\n3.2 Structural Analysis\nThe structural analysis phase comprises two steps\nthat calculate metrics in order to produce the mutual\nimpact graph showing the relevance of documents.\n(PII.1) The formal concept analysis is the first\nstep, divided into five sub-steps (see Figure 2). Its\nobjective is to produce the mutual impact matrix between terms and documents from the occurrence matrix in order to evaluate the relevance of documents.\n• Normalisation: occurrences of terms per documents are transformed into proportions in order\nto reduce the length disproportions between documents. Absolute values are converted into percentages per line. Thus, all documents are treated\nequally.\n• Transposition: the matrix is transposed in order to\nchange the point of view from documents characterized by occurrences of terms into terms characterized by their appearances within documents.\n• Binarisation strategy: building the lattice requires\na formal context, or, in other words, a binary matrix. Multiple strategies of binarization exist (also\ncalled refinement strategies) (Jaffal et al., 2015).\nIn our case, we use the most simple one: the direct strategy that transforms all values > 0 as 1\nand keeps 0 as 0.\n• Lattice construction: the formal context representing terms within documents is used for building a lattice and its formal concepts (Belohlavek,\n2008). A formal concept is a node containing objects and attributes (at least one of them). In our\ncase, the objects are terms, and the attributes are\nthe documents.\n• Metric calculus: the lattice is analyzed, and the\nmutual impact (Jaffal et al., 2015) metric is cal-\nculated by comparing the appearances of couples\nof terms and documents within each formal concept. The mutual impact shows how strong the\nrelationship is between each term and each document. This bond is calculated for each term and\neach document with the formula:\nMI(O i , A j ) = formal concepts containing O i and A j\nformal concepts containing O i or A j\nwhere O i represents a term and A j represents a\ndocument. The output is a mutual impact matrix\nwith a value representing the bond between each\nterm and each document.\n(PII.2) The construction of the mutual impact\ngraph is the final step. Its objective is to visualize\non a map the terms and their importance within the\ncorpus, as well as the documents and their relevance.\nThe visualization uses the mutual impact matrix as an\nadjacency matrix and produces a graph of terms and\ndocuments. We used Gephi (Bastian et al., 2009) with\nthe ForceAtlas (first version) spatialisation algorithm.\nNodes are moved until they find their optimal position\nthanks to attractive and repulsive forces. Nodes are\nrepulsing each other, and edges attract nodes based\non the values of the edges. Because of the input format, the visualization is a bipartite graph: a set of\nnodes represents documents, and another set represents terms. As presented in Figure 3, the nodes of\ndocuments are colored in grey and are linked to numerous nodes of terms. Unlike, these term nodes\nare colored based on the number of neighbors (the\nwarmer the color, the more the node of term is linked\nto different nodes of documents). When focusing on\nthe nodes of terms, we can find terms from every document in the center of the map and terms from fewer\ndocuments scattered around (terms, present in only\none document, are forming specific groups for each\ndocument far from the central set). The central set of\nterms is, in fact, a global view of the text corpus with\nits keywords. When focusing on the nodes of documents, we can visually see the relevance of documents\nwithin the corpus by checking how close documents\nare to the central set of terms.\n4 EXPERIMENT\n4.1 Scenarios of evaluation\nA proof of concept (Elliott, 2021) has been realized\nwith a specific scenario to check the pipeline’s validity and properties. A prototype has been developed2\n2https://github.com/metalbobinou/CREA-phd\nFigure 3: The output map of terms and documents\nand used in three proof of concept demonstrations. A\nfirst case expects to visualize the content of 9 PHP\ncourses. The mutual impact graph is visualized to\ncheck the corpus’ main keywords and the documents’\nrelevance. A second case introduces a Java course in\nthe corpus. The validity of the mutual impact graph\nis checked by comparing the first and second cases:\nthe Java course should be the most irrelevant because\nit is not specialized in PHP. A third case is presented\nin order to check how correcting a document impacts\nthe results.\nIn the regular case, 9 PHP courses in French are\nprocessed through the pipeline. These courses present\nweb development with HTML, PHP, and MySQL for\nbeginners. They are denoted as C1-C9 in the following figures. 6 of these courses are made of slides (C1,\nC2, C4, C7, C8, C9), and 3 are made of regular texts\n(C3, C5, C6). A Java course (CJA) in text format\nis later introduced to check how the mutual impact\ngraph behaves when some errors appear. This course\nis also in French and in a text format. The third experiment only works on courses in text format. Therefore, we introduced 4 more PHP courses in text format (C11, C15, C17, C19) to avoid disproportion by\nkeeping a close number of input documents as in the\nother cases.\n4.2 General view of the corpus’ content\nThe documents are processed through phase I, and the\ncorpus is transformed into a matrix of occurrences.\nThe mutual impact graph is generated using the direct\nstrategy in step PII.1.\nFigure 4 shows terms as nodes colored following a\ncold-warm schema. Red nodes are terms occurring in\nall the documents, orange nodes are terms occurring\nin all the documents minus one, etc. In the regular\ncase, the terms in red nodes are: post, nombre (number), me´thode post (post method), fichier (file), code,\ndonne´e (data), langage (language), site, php, navigateur (browser). These terms are typical of a course\non web development with PHP. They are extended\nwith the terms in orange nodes like base de donne´e\n(database) or even foreach, which are also typical for\na website in PHP that uses a database. Terms that\nare present in fewer documents but still in more than\nhalf of the corpus (the yellow and green nodes) are\nalso typical of web development for nearly all of them\n(session, mysql, utilisateur (user), ...).\nFigure 4: Central set terms of the mutual impact graph in\nregular case\nFigure 5: Central set terms of the mutual impact graph in\nJava case\nIn Figure 5, the Java course is added to the corpus.\nThe terms in red nodes are the same as in Figure 4,\nexcept php which becomes an orange node. This behavior is expected as the Java course does not discuss PHP; therefore, one document does not include\nit. The terms in other colors are still relevant as they\nmainly concern client-server programming, OOP vocabulary, and similar topics. It can be noted that another color is introduced in order to show that an additional document is present. Nearly all of the terms\nin green in the first case are now light green. In the\nentire graph, some of the terms in green are repulsed\ninto dark green nodes (meaning they are missing from\none more document). However, a majority of terms\nfrom the first case are still present in the second case\nwith the same number of document edges.\n4.3 Relevance of documents\nFigure 6: Relevance of documents in regular case\nFigure 7: Relevance of documents in Java case\nFigures 6 and 7 represent the relevance of documents in two cases: the regular case with only PHP\ncourses, and the Java case with the additional Java\ncourse. The relevance’s visualization is also produced\nfrom the mutual impact graph, except the focus is\nmainly on the grey nodes representing documents.\nIn the regular case of PHP courses (Figure 6), the\ndocuments in the slide format are closer to the central\nset than the text ones. In the Java case (figure 7), the\nJava document (CJA) is the most distant of the central\nset. It can be noted that C6 is as distant from the central set as CJA. This discrepancy is explained by the\nfact that the C6 document contains not only a PHP\ncourse but also reports of students’ projects in more\nthan half of the document. These reports discuss various business problems that require a website (online\nshoe store, online music store, etc). Therefore, the\ndocument is not exactly a pure PHP course like the\nothers.\nIn order to test how the mutual impact graph reacts when a document is corrected, we compared multiple cases of courses while correcting one of them.\nDocument C6 is written in three parts with nearly the\nsame amount of pages: the regular PHP course, the reports of the students’ projects, and an advanced PHP\ncourse. For the experiment, we used PHP courses in\ntext format only (in order to avoid the effect of mixing slide and text formats), and we corrected document C6 by removing the students’ projects first and,\nlater, the advanced chapters. The experiment was also\ndone twice, with and without the Java course, in order\nto have a better view of the effect of correction on a\ncorpus with and without an irrelevant course.\nFigure 9: Relevance of text documents (no student projects)\nFigure 8: Relevance of text documents (C6 intact)\nIn the regular case without any modification (Figure 8), C6 is the most outlier in both regular and\nJava cases. In the Java case, CJA is the sole document nearly as far as C6. After removing the stu- Figure 10: Relevance of text documents in the regular case\ndents’ projects from C6 (Figure 9), it becomes one (no student projects, nor advanced chapters)\nof the closest documents from the central set in both\ncases, indicating that it became way more relevant to\nthe corpus than previously. In the Java case, CJA becomes the most outlier, keeping its irrelevance. Finally, when the advanced chapters are also removed\nfrom C6 (Figure 10), it becomes the closest to the central set in the Java case, and the second closest in the\nregular case. C11 and C15 become the most outliers\nin the regular case. In the Java case, CJA and C11 are\nthe most outliers.\nIn each figure from 8 to 10, document C6 becomes\ncloser to the central set thanks to the corrections. It\ncan be noted that CJA stays the most outlier in every case, and the other documents move away a bit.\nTherefore, the corrections show improvements in the\npositioning of C6 while keeping the irrelevance of\nCJA.\n5 DISCUSSION\nIn the mutual impact graph of the regular case (Figure 4), the set of terms present in all of the documents\nat the center of the graph, shown as red nodes, are relevant to the content of the text corpus, and they also\nproduce a clear summary of the main keywords. Each\nring of colors around the central set adds more terms\nrelevant to the corpus. When the Java course is introduced (Figure 5), the terms in all the documents\nare nearly the same: only php is a bit repulsed. As\nthe Java course also mainly talks about programming\nand partially about web development with dedicated\nframeworks, the results are nearly unchanged, which\nis the expected behavior. The mutual impact graph\nwith the terms lets a user quickly get an idea of the\nsubject and keywords composing the corpus. It can\nbe used to quickly discover a new academic field and\nfind the keywords that best describe it. Another usage\nfor this graph would be to help build a book’s index:\nthe keywords are highlighted, and the author selects\nwhich words to keep or remove.\nConcerning the relevance of documents in Figure 7, the Java course is the most distant one with\nC6 (which contains a lot of unwanted content) in the\nfirst experiment. This behavior is perfect for the current case: the teacher who would like to use existing documents is warned that C6 and CJA should be\nchecked more precisely in order to detect if their content is relevant. Even if the text documents are distant\nfrom the slide documents, the most irrelevant ones\nare far away, allowing the user to measure the relevance visually. In the third experiment, the relevance\nof document C6 is greatly improved because of the\ncorrections applied while keeping the irrelevant document CJA far away. It must be noted that the shape\nof the graph changes because it reflects the relevance\nof each document relating to the general relevancy of\nthe whole corpus. The mutual impact graph is a picture of the corpus as a whole: it is not a graph about\nthe relevance of one or some documents against one\nor some other documents. Multiple use cases can be\nderived from the mutual impact graph. The graph\nwould help users select the best documents about one\nor more topics or remove the most irrelevant ones.\nAnother usage would be for a teacher to compare its\nown course with the existing ones or even with research articles to check how close it is to the state of\nthe art.\nThe global results show that FCA, with the mutual impact measure, can highlight a corpus’ main\nterms and even show its documents’ relevance. It\ndoes not create a list of topics nor calculate the probability of each term being included in a topic like\nLDA, but it does reflect the importance of each term\nfor the whole corpus. This behavior is expected by\nthe nature of FCA: it ”differs from statistical data\nanalysis in that the emphasis is on recognizing and\ngeneralizing structural similarities, such as set inclusion relation from the data description, and not on\nmathematical manipulations of probability distributions” (Carpineto and Romano, 2004). However, we\nacknowledge that BabelFy does participate actively\nin the process of topic modeling by recognizing the\nnamed entities and evaluating their relevance to each\nportion of the text. It must be noted that the construction of the formal context also filters some terms.\nFCA (formal context’s construction + metrics calculus) and BabelFy must be used together, or at least,\nFCA with an entity linking tool.\n6 CONCLUSION\nIn this paper, we proposed a visualization pipeline for\ntextual corpora analysis based on FCA instead of the\nusual LDA for the topic modeling step. Mutual impact was used within FCA in order to produce a matrix for the force-based graph algorithm. The pipeline\nproduces a map that can be used in two ways:\n• The main keywords are placed by order of importance, allowing the reader to quickly get an idea\nof the topics contained in the corpus\n• Documents are placed based on their relevance to\nthe keywords found, allowing the reader to see an\neventual discrepancy in the chosen texts.\nThe map presents a visualization of the corpus as a\nwhole. Removing a document impacts the visualization because of the absence of a node and because\nthe topic modeling step does not work on the same\ntexts. To evaluate these claims, we presented a case\nstudy on multiple PHP courses and an intruder Java\ncourse (also about programming). First, a map displayed the most important keywords and the variations with and without the Java course. Then, one of\nthe PHP courses containing more than half of its text\nabout out-of-scope topics was corrected, showing a\nsignificant upgrade in the output.\nWe consider FCA an exciting method for topic\nmodeling and expect to try other metrics on the lattice in order to find more possible usages. Multiple\nusages and combinations have been already proposed\nin (Poelmans et al., 2013), but we expect to use the\nconceptual similarity metric (Jaffal et al., 2015) for\nan even more precise combination of terms. Also, a\ndeeper comparison with LDA and other newer methods like neural networks might be interesting as the\nconstruction of the results does not rely on probabilities and is perfectly transparent thanks to the set theory behind FCA.",
  "2024-HRH": "1 Introduction Since Satoshi Nakamoto released the bitcoin whitepaper [40], a new era of decentralised systems began. Initially a peer-to-peer electronic cash system, Blockchain Technologies (BCT) evolved significantly with the Ethereum project [6], introducing Smart Contracts and decentralised Autonomous organisations (DAOs) supporting enterprise use cases. Yet Nakamoto’s initial vision, rooted in the cypherpunk movement, aligns with the anarchist idea of a decentralised society. The anarchist movement, championed by thinkers like Kropotkin [35, 36], Bakunin [4], and Malatesta [37], proposes principles aimed at avoiding exploita- tion and fostering harmony. However, these principles have never been imple- 2 Julien Hue, Irina Rychkova, and Nicolas Herbaut mented at scale. DAOs embedding anarchist principles appeal to activists, tech- nologists, and communities seeking autonomy, offering transparent, secure, and efficient ways to manage organisations without central authority, such as Liq- uid Democracy (LD), proposed in the early 21st century, combining elements of Direct Democracy and Representative Democracy [10, 18, 42]. Direct Democracy, central to anarchist principles, faces issues like voter fa- tigue and scalability. LD addresses these issues by allowing voters to either vote directly or delegate their vote [10, 18, 42] to others. Thus, LD helps implement anarchist principles in DAOs by reducing hierarchical control and promoting cooperative governance. To systematically reason about the alignment of DAOs using LD with anar- chist principles, we turn to Enterprise Architecture (EA) and Enterprise Model- ing (EM) frameworks. Traditionally, EA/EM frameworks demonstrate and an- alyze the alignment between technology and organisational strategy. They pro- vide tools and methodologies to model enterprises and align business processes with technology. However, little research addresses the alignment between socio- political and technological artifacts in decentralised organisations like DAOs. In this work, we address the following research problem: How can socio- political artifacts for decentralised organisations be addressed by Enterprise Mod- standard1 eling? We use the TOGAF to explicitly reason about processes of decentralised organisational governance grounded in anarchism. We specify the business, application, and technology layers of DAOs and LD using ArchiMate. We use Design Science Research Methodology to create an architectural artifact using TOGAF ADM and ArchiMate. The remainder of this article is organized as follows: In Section 2, we discuss the main concepts used in this study and present the related works; in Section 3, we present our research methodology and discuss the created artifacts in Section 4; in Section 5, we empirically evaluate the alignment of our artifact with anarchist society principles and provide recommendations for adapting EA frameworks to decentralisation. In Section 6, we provide our conclusions. 2 Background 2.1 Smart Contracts and DAOs Smart Contracts, first introduced by Nick Szabo in 1994 [51], are protocols for validating the conditions of a legal contract between parties. They exe- cute, control, and document events automatically according to the agreement’s terms [33,56]. Embedded within the blockchain, Smart Contracts are immutable and transparent. DAOs are organisations operating based on rules encoded as Smart Contracts [24, 53, 54]. They function without human intervention, leveraging blockchain’s transparency, immutability, and decentralisation. Key characteristics of DAOs include: decentralisation: Operate without centralized authority, distributing 1 https://www.opengroup.org/togaf Alignment of DAO with Socio-political principles 3 decision-making power among members. Autonomy: Smart Contract code gov- erns operations, executing decisions and transactions automatically. Transparency: All transactions are recorded on the blockchain, making them auditable by any- one. Programmable Governance: Rules and protocols embedded within the Smart Contract automate governance. Community-Driven: Governed by members hold- ing tokens that represent voting power, with decisions made by consensus. DAOs have various applications, such as decentralised Finance (DeFi), col- lective ownership, investment funds, charitable organisations, and decentralised governance. They can be categorized into: 1) Algorithmic DAOs, which defer entirely to software to structure and coordinate social interactions, and 2) Par- ticipatory DAOs, which emphasize active community participation in decision- making [54]. This work focuses on Participatory DAOs, specifically decentralised Au- tonomous Communities where each member has one vote, and decisions require a 2 majority [6]. We plan to use Liquid Democracy as the decision-making process 3 in our system. 2.2 Liquid Democracy Liquid Democracy combines elements from Direct Democracy and Representa- tive Democracy [18] [10] [42]. In LD, voters can either vote directly on issues or delegate their vote to a trusted party. This delegation can continue through multiple levels until the vote reaches a well-informed party, a process known as \"Meta-delegation\". Additionally, voters can recall their vote at any time. Mem- bers can choose on which topics to vote directly and which to delegate. \"Issue- based delegation\" allows voters to delegate their vote for a specific topic while voting directly on sub-topics. LD addresses issues in Direct Democracy, such as voter fatigue and uninformed voters, and in Representative Democracy, such as lack of accountability(e.g: in most Representative Democracy, elected members only handles 1-2 subjects and is incompetent in the others. While voting on a subject the member don’t know specifically about he can only vote for a so- lution he only have vague idea of. Also in this mode, the important is to win the vote and not taking the right decision or being aligned with the social body will) and minority rule(e.g: in organisation that prefer stability and coherence of choices an entrenched minority can take the power). LD was notably used by the Pirate Party in Germany, but the experiment failed when combined with Representative Democracy [10]. Issues like the concentration of power among a few agents and the emergence of super-voters remain concerns. To address these, proposals include \"Multiple Delegations Options\"(i.e: helps avoid super- voter by allowing users to specify several potential delegates instead of just one on various criteria), \"Dynamic Redistribution\"(i.e: system’s ability to distribute votes or influence among delegates to maintain a balanced representation to avoid the concentration of votes), and \"Algorithmic Balancing\"(i.e: using using mathematical models and computational techniques to optimize the delegation process) to distribute voting power more equitably [22]. This work explores the use of LD in DAOs to enhance decentralised governance. 4 Julien Hue, Irina Rychkova, and Nicolas Herbaut 2.3 EA Frameworks Enterprise Architecture is defined as \"the underlying principles, standards, and best practices according to which current and future activities of the enterprise should be conducted\" [47]. According to Fischer [17], EA involves The Fundamen- tal organisation of a System, describing system components, their relationships, and their interaction with the environment, and The Principles Guiding its De- sign and Evolution, governing the design, implementation, and development of the architecture, ensuring alignment with business goals. Several EA frameworks and tools are widely acknowledged in the literature [2,29]. The Open Group Ar- chitecture Framework (TOGAF) [28] is a comprehensive method and set of tools for developing an enterprise architecture. ArchiMate [29] is a modeling language part of \"The Open Group,\" used to model enterprise architecture. Zachman Framework [55] defines a 6x6 matrix providing a structured approach to defining an enterprise from multiple viewpoints. FEAF (Federal Enterprise Architecture Framework) [26] originates from the US Federal Government. It integrates busi- ness and IT aspects of an enterprise. The Gartner Framework [49] Focuses on EA process development and governance. To explore whether DAOs using LD can achieve an anarchist society organ- isation, we use TOGAF framework and Archimate modeling language to model and verify our system. Because first TOGAF is a widely adopted and battle tested framework, helps to define the EA from multiple viewpoints, it has a comprehensive and structured methodology to develop EA, ArchiMate bring modeling capabilities that complement very well the TOGAF ADM. 2.4 Related Works 2 For this section, we conducted a literature review covering 251 articles, to which we added 5 pre-identified relevant articles. After reviewing each, we iso- lated 19 related works. These were classified into two clusters: 1) EA for socio- political artifacts/E-Government (7 articles), and 2) EA for BC or DAO (12 articles). Each cluster was further subdivided: A) Interoperability and Integra- tion (7 articles), B) Architecture Modeling (9 articles), C) Dynamic EA Plan- ning (1 article), and D) Systemic Design (2 articles). We first present articles related to cluster 1. For sub-category D: [20] modifies TOGAF ADM to in- clude a \"Government Strategic Objectives phase\", \"Security Architecture,\" and \"customized phase.\". For sub-category A: [39] develops a structured approach, SGEA, for defining e-government EA scope. [31] proposes a National EA for implementing an e-government interoperability framework in Uganda. [38] ex- plores how EA tools aid in regulation and legislation compliance. [21] exam- ines EA tools in the public sector to achieve business ecosystem maturity. [48] 2 using this SCOPUS query string: \"(TITLE-ABS-KEY(\"Enterprise Architecture\" AND \"E-Government\" AND \"In- teroperability\") OR TITLE-ABS-KEY(\"decentralized Autonomous organisation\" AND \"Blockchain\" AND \"Gov- ernance\") OR TITLE-ABS-KEY(\"Business Process Modelization\" AND \"Enterprise Architecture\") OR TITLE- ABS-KEY(\"Enterprise Architecture\" AND \"Digital Ecosystem\") OR TITLE-ABS-KEY(\"Requirements for Enter- prise Architecture Frameworks\") OR TITLE-ABS-KEY(\"Validation of Enterprise Architecture Frameworks qual- ity\") OR TITLE-ABS-KEY(\"decentralized Governance\" AND \"TOGAF\" AND \"ArchiMate\") OR TITLE-ABS- KEY(\"Liquid Democracy\" AND \"Anarchist principles\" AND \"Blockchain\") OR (TITLE-ABS-KEY(\"Enterprise Modeling\") OR TITLE-ABS-KEY(\"Enterprise Architecture\") AND TITLE-ABS-KEY(\"Blockchain\" OR \"decen- tralized Autonomous organisation\" OR \"DAO\")))AND PUBYEAR > 2017\" Alignment of DAO with Socio-political principles 5 discusses E-Government architecture in Indonesia using TOGAF and Service- Oriented Architecture. For sub-category B: [45] presents a Smart Campus Sys- tem blueprint using TOGAF ADM, adapting TOGAF for specific domains and modeling socio-political artifacts. Next, we discuss cluster 2, starting with sub- category B: [27] explores BC’s potential in enhancing business value creation, particularly in global supply chains, using ArchiMate models. [14] examines how EA approaches, especially ArchiMate, can design BC-based applications. [3] ad- dresses the gap between enterprise engineering modeling methods and blockchain models. [16] proposes Knowledge BC for securely managing and tracking knowl- edge in organisations. [1] uses EA tools for developing digital twins for dry ports with ArchiMate. [15] discusses integrating BC into enterprise modeling and the mutual support between BC technology and modeling techniques. [30] focuses on structuring and implementing BC in enterprises, emphasizing the necessary layers for a robust BC platform. [34] develops a method for strategic analy- sis integrating business processes and IT infrastructure, focusing on GoalML, SAML, and ITML. For sub-categroy C: [12] discusses EA as a strategic tool for aligning business and IT, incorporating case-based reasoning and BC for knowl- edge storage and sharing. For sub-category A: [50] uses EA tools to address BC interoperability issues, while [41] proposes an architecture integrating BC into Health Information Exchanges (HIEs) to enhance healthcare data management and exchange. Finally, For sub-category D: [7] discusses the systematic design of BC-based applications, integrating business and IT perspectives. Analyzing this literature reveals a gap that our work aims to fill: EA for BC applied to socio-political artifacts/E-Government. 3 Research Methodology To address our research problem, we employ the Design Science Research Method- ology (DSRM) [25, 43], which consists of six steps: Problem Identification and Motivation, Objectives for the Solution, Design and Development, Demonstra- tion, Evaluation, and Communication. We merge the Demonstration and Evalu- ation steps to better address our research questions. Problem Identification and Motivation. While DAO provides a set of mechanisms to implement decentralised governance and principles of liquid democracy, there is no evidence on how these organisations form can support other socio-political mechanisms, including prin- ciples and governance processes grounded on anarchism. We choose enterprise modeling and enterprise architecture disciplines to address the problem of align- ment and traceability of socio-political artefacts in decentralised organisations. RQ1: This motivates the following research questions: How can TOGAF and ArchiMate be used to model socio-political artifacts within decentralised or- RQ2: ganisations? How TOGAF and ArchiMate should be adapted to address RQ3: socio-political artefacts in decentralised organisations? In what ways can LD and DAOs can be used to achieve the organisation of anarchist societies? Objectives for the Solution. Establish key objectives: Design an architectural ar- tifact using TOGAF ADM and ArchiMate to model DAOs with LD. Ensure 6 Julien Hue, Irina Rychkova, and Nicolas Herbaut the artifact aligns with anarchist society principles. Validate the artifact’s abil- ity to support decentralised governance at scale. Here, we want to ensure that the artifact that we build is aligned with the anarchist society principles. We use the anarchist society principles that we elicit from the literature to design our artifact. Design and Development. Develop the artifact using TOGAF ADM and ArchiMate as architectural Framework, and employing modeling languages such as i* 2.0, BPMN and SysML/UML to design the motivation, business, application, and technology layers to assure that the artifact encapsulates the anarchist society principles. Demonstration and Evaluation. Demonstrate and evaluate the artifact by applying practical use cases based on anarchist society principles to verify alignment and conduct scenario analysis to demonstrate how to the artifact supports decentralised governance. We use an analytical evalua- tion as described by Hevner et al. [25] to evaluate the artifact and answer our research questions, more precisely the \"Architecture Analysis\". Communication. We have documented the research process, findings and some recommendations. Then we present the result in this paper. 4 Artifact Design 4.1 Design Principles, Rationale, and Development Approach: In this work, we adopt the TOGAF ADM. It is an iterative process for developing architecture content, transition from the existing (As-Is) to the target (To-Be) architecture and the architecture governance [28]. We focus on the following ADM phases: Preliminary Phase, which describes the preparation and initiation activities required to create an Architecture ca- pability and definition of Architectures Principles; Architecture Vision, which describes the initial phase of an architecture development cycle; Business Ar- chitecture, which describes the development of a Business Architecture to sup- port an agreed Architecture Vision; Information Systems Architecture, which describes the development of IS Architectures to support the agreed Architec- ture Vision; and Technology Architecture, which describe the development of Technology Architectures to support the agreed Architecture Vision. In the Preliminary Phase, we conduct a literature review of the anarchist literature. We derive the anarchist society principles from the various sources, including the research articles, books and essays. Our results are presented in the Table 1. In our artefact, a strong assumption is present: we believe that participants will follow and commit to these principles. We use these principles in the later phases, to guide the design our artifact. 4.2 Preliminary Phase: In our study, we consider the organisation grounded on DAO and LD principles for the decision-making process. We model the organisation using Blockchain as the main technological babckbone. Within THIS such organisation, the team of Alignment of DAO with Socio-political principles 7 Anarchist Society and Principles Table 1. [36] [13] [23] [35] [37] [9] [46] [11] [44] [4] [19] Principles Voluntary Association Direct Democracy and Consensus Decision-Making Mutual Aid decentralisation Autonomy and Self-Management Non-Hierarchical organisation Commons and Communal Resources architects that implements TOGAF has to follow the principles of decentralisa- tion and LD. Thus, the TOGAF implementation has to be adopted according to these principles. We use Dapp and Smart Contracts to implement the LD principles and, in particular, the voting process. We choose NFT as a technological solution to determine the collective ownership of organisations, goods, services etc. We are using the TOGAF ADM from [28] and ArchiMate 3.2 to model our organisation. 4.3 Architecture Vision: In the Architecture Vision phase we use the anarchist society principles from Table 1 to define the vision of the organisation. We present the principles of traditional centralized organisations/societies (As-Is), putting forward the main criticism advanced by the anarchist thinkers, and the principles of the target, decentralised organisations/societies(To-Be), in Table 2. This table illustrates how anarchist thinkers of their time viewed the state of centralized societies, especially the \"collusion between capital and the state\", which can be understood as \"minority rule\" as discussed in 2.2. The term should be contextualized within the specific conditions of that era. Motivation Layer. In this layer, Table 2 is used to represent the current state of centralized organization with the ArchiMate Motivation \"Assessment\" con- cept. We apply the i* modeling language [52] to define actors, goals, tasks, and dependencies. From the literature, we identified three actors: Volunteer, Associ- ation, and Delegated. Figure 1 shows a fragment of the goal diagram illustrating a Volunteer’s participation in an Association and their dependencies on Federa- agents3. tion, LD, Blockchain infrastructure, and Smart contract In i*, an agent is \"an actor with concrete, physical manifestations, such as a human individ- ual, organization, or department\" [8]. Goals derive from principles in Table 1. Several agents can share the same goal. Tasks associated with goal realization are based on anarchist literature analysis from the preliminary phase. Goals and tasks for Blockchain and Smart Contract agents are specified following [52]. Dependencies between agents’ goals/tasks are also added, e.g., Federation and 3 Full goal model available at: https://github.com/edoc2024/paper 8 Julien Hue, Irina Rychkova, and Nicolas Herbaut As-Is and To-Be Architecture Visions Table 2. As-Is traditional Centralized organisa- To-Be decentralised organisa- tion/societies tions/societies - Hierarchical and Centralized Power - decentralised Self-Governance Structures - Economic Inequality and Exploitation - Collective, consensus and decen- tralised Decision-Making - Competition over Cooperation - Voluntary Association and Mutual Aid - Lack of Autonomy - Direct Action and Self-Management: This involves self-management prac- tices where individuals and collectives take initiative and responsibility for managing tasks and projects - \"State and Capitalist Collusion\" or Minority Rule - Autonomy and Independence - Resistance to Change and Innova- - Restorative Justice tion (the capitalist mode of production resists changes that threaten existing power structures or profit margins, even if such changes could benefit society as a whole) - Environmental Exploitation - Equitable Resource Distribution -Continuous Learning and Adaptation Goal Diagram of the Architecture Vision Fig. 1. Alignment of DAO with Socio-political principles 9 Association depend on Liquid Democracy for consensus, and Federation depends on Blockchain for enabling the DAO and data storage. Smart Contracts depend on Blockchain for execution. Using Figure 1 and principles from Table 2 and layer4. Table 1, we define the ArchiMate motivation ArchiMate Drivers link As- sessments, Goals, Requirements, and Principles to stakeholders, isolating five drivers: 1) Community Need, 2) Social Expectation, 3) Organizational Culture, 4) Technology Advancements, and 5) Regulations. We use the literature analysis to further refines the motivation layer, including items not expressible in the i* diagram, such as the requirement for \"Sustainable Practices.\" 4.4 Business Architecture The Business Architecture phase defines the organisation’s business processes. We use BPMN to model the business processes. The main business processes are: 1) The unified process for voluntary association, mutual aid and self-management, 2) The Decision-Making Process using LD, 3) The Equitable Resource Distribu- tion Process. In this paper we present the process diagrams for Unified Process 5. for Voluntary Association, Mutual Aid and Self-Management Unified Process for Voluntary Association, Mutual Aid and Self-Management We define process activities and constraints using [4, 9, 11, 13, 19, 23, 35, 36, 46]. The Run Federation process diagram shows how members organize within fed- erations, which are networks of autonomous groups based on mutual aid, volun- tary association, and non-hierarchical organization (see [44]). Members establish goals, principles, structure, and coordination, operating by consensus and creat- ing associations to meet needs. We describe how to run and manage associations following anarchist principles in the Collaborative and Initiatives Project Process and Conduct Association Operations. Associations are voluntary, cooperative groups pursuing common goals. We outline the global lifecycle of associations and the strategies for managing them. We define three main forms of association, each with its own processes: Run Community Engagement Project for address- ing issues like education and healing; Run Awareness and Advocacy Project for promoting ideas; and Run Economic Project for providing goods and services. These processes facilitate organizing society without central authority, adhering to anarchist principles. Business Architecture Layer We integrate all the business processes (including those not present in this pa- ArchiMate6. per) into the Business Architecture Layer in The Unified Process for Voluntary Association, Mutual Aid, and Self-Management is the organisation’s 4 Complete motivation layer diagram available at: https://github.com/edoc2024/ paper 5 Extra BPMN diagrams: https://github.com/edoc2024/paper/tree/main/BPMN 6 The complete business layer can be found here: https://github.com/edoc2024/paper 10 Julien Hue, Irina Rychkova, and Nicolas Herbaut Run a Federation Process Run community Engagement Projects Process Redistribute Does the association r e s o u rc e s a n d context changed? Dev b e n e f its f ro m th e Implement association among communication Evaluate and Education and colla b o r a ti v e p r o jects m embers a n d Adapt outreach a n d in i ti a t iv e s y es coor d in ation no yes M o b i l iz in g Plan n in g P r oject re s o u r c e s a n d L o g is t ic s p a r t i c i p a n t s Organize Conduct Conduct Does the association no federative Meeting around Community make benefits? structure current topics vote Implementing Consensus and m a i n t a i n ing dec is o n m a king Establish th e p r o j e c t m e e t i ng s common principles and objectives Collaborative and Initiatives Project Process Run Awareness and Advocacy Project Process I d e n t i f y (R e ) C r e a te D o e s th e a s s o ci a ti on R e d i s t r i b u t e C a l l f o r r e s o u r c e s a n d c o n t e x t c h a n g e d ? com m o n n e eds t h e v ol u n t e e r b e n e f it s f r o m th e yes an d g o a ls a ss o ci a t io n association among members yes ( R e ) D e f i n e t h e C r e a t i n g C h o o s i n g C on d u c t s t r a t e g i e s i n d o e s i t n e e d s In f o r m a t io n a l c h a n n e l s f o r no yes co m m u n i ty r e s p o n s e t o t h e e x p a n s i o n ? no Identify Resources an d p r o m o t i o n al dis t ri b u t i o n s a nd meeting around (skills, materials, external changes materials engagement financial resources,...) those needs and Does the association no goals make benefits? yes Conduct O r g a n i z i n g C o n s e n s u s A s s o c i a ti o n e v e n t s o r de c i s o n m a k ing no O p e r a t io n s does the need/goals a c t i o n s m e e t i n g s still exists? Conduct Association Operations Run Economic Project Process Define C o ll e c t i v e ' s G o a ls a n d E s t a b l is h R o l e s A n d R e d i s t r i b u t e P r i n c ip l e s w i t h i n t h e R e s p o n s a b i l i t i e s D o e s th e a s s o ci a ti on r e s o u r c e s a n d o r g a n i z a t i o n (r o l e s r o ta t i o n t o a v o id c o n t e x t c h a n g e d ? b e n e f it s f r o m t h e in f o r m a l h i e r a r c h y ) a s s o c i a t i o n a m o n g members yes C o m m u n i t y e n g agement Run Comm u n i t y e ngagement no P r o j e c ts S e c u r i n g P r o j e c ts D e ve l o p in g a res o u r c e s a nd bu s in e s s o r o ps f u n d in g if p l a n yes wh ic h k in d o f n e c e s s a r y Sharing and Does the association assocation is it? Economic Initiatives m u t u a liz in g make benefits? no Run Economic Projects reso ur c e s w it h in the O p e r a t i n g t h e association C o n s e n s u s in it i a t i v e a n d de c i s o n m a k ing m a n a g i n g i t s m e e t i n g s Run Awaren e ss a n d Advocacy g r o w t h P ro je ct s Awareness and Advocacy Campaigns BPMN of the Unified Process for Voluntary Association, Mutual Aid and Self- Fig. 2. Management core. It uses the LD process for decisions at both the federation and association levels. The Equitable Resource Distribution Process ensures fair resource distri- bution, from raw materials to dwellings, at both levels. For this paper, we present only a simplified version of the Unified Process for Voluntary Association, Mu- tual Aid, and Self-Management (see Figure 3). We can observe that the Run Federation Process achieves the decentralised Federation organisation Business Service. The Voluntary Association, Mutual Aid, and Self-Management Business Services are realized by the Collaborative and Initiatives Project Process, which embeds all of the other subprocesses described in Figure 2. 4.5 Application and Technology Architecture Layer To design the Application and Technological Layer we use a detailed sysML Block Definition Diagram. As shown in Figure 4, we make some technology choices due to the need to study LD in DAO. To enable DAO, blockchain is used in the Technology Layer. The blockchain must support Turing-complete smart contracts. To construct this model, we use the goal diagram (see Figure 1). In white are actors identified during the anarchist and LD literature analysis. In blue are elements in the ArchiMate Application Layer, representing software components enabling business processes and motivation goals. Federation and Alignment of DAO with Socio-political principles 11 Business Layer Person Volunteer Member Decentralised Federative Organisation Voluntary Association Self-Management Mutual Aid N e e d F o r a E n d o f t h e Run A Federation Process Ne w F e d e r a ti on Fe d e ra t i o n Federation Someone Identify Records a need/goal Start Association Lifecycle End Collaborative And Initiatives Project Conduct Association Operations A s s o c i a t ion Process Process L if e c y c l e Association M e m b e r E n d C o m m u n i ty Re c o r d s N e e d s F o r E n d F o r e n g a g e m e n t s C o m m u n it y A w a r e n e s s an d P r o je c t N e e d s F o r E n d F o r E n g a g e m e n t A d vo c a c y P r o je ct N e e d s F o r As s o c i a ti on E c o n o m ic E c o n o m i c A w a r e n e s s a n d E n d th e P r o j e c t P r o j e c t R e c o r d s A d v o c a c y P r o je ct P ro j e c t Run Co m m u n i t y E n g a g ements Run Aw a r e n e s s a n d A dvocacy P r oj e c t P r o c e s s P r o j e c t P ro c e ss Run Economic Project Project Process Simplified ArchiMate Model of the unified process for voluntary association, Fig. 3. mutual aid and self-Management bdd [Package] System [Detailed System Block Definition Diagram] <<block>> <<block>> <<data>> <<data>> Association Goal Association <<block>> Data Strategies Data <<actor>> <<data>> Association Role Person Data <<block>> <<block>> <<component>> <<component>> Association's Goals Association's Management strategies Management <<actor>> <<actor>> <<actor>> <<actor>> Member Delegated Delegator Volunteer participates_in <<block>> <<component>> Association's Roles Rotation Component < < b l o c k > > < < b l o c k > > < < b l o c k > > < < b l o c k > > < < d a t a > > < < c o m p o n e n t > > < < c o m p o n e n t > > < < c o m p o n e n t > > Co m m u n i t y M e e t ing C o m m u n i t y M e e t in g Fed e r a t io n D ata Asso c i a t i o n D a s h b oard R e q u e s t S e s s i o n M a n a g e m e nt < < b l o c k > > < < b l o c k > > < < b lo c k > > << c o m p o n e n t> > << c o m p o n e n t >> Volun te e r s M a n a g e ment < < D A O > > F e d e r a t i o n ' s Economic Association <<block>> Association Dashboard <<component>> participates_in Community Meeting Validation & Planning <<block>> <<block>> <<block>> < < D A O > > < < b l o c k > > < < D A O > > < < D A O > > 0..* < < b l o c k > > C o m m u n i t y E n g a g e m ent < < c o m p o n e n t > > F e d e ra ti o n A s s oc i at io n < < d a t a > > A s s o c i a t i o n < < b l o c k > > V o t i n g & D e l e g a t io n < < c o m p o n e n t > > D e l e g a t i o n D a ta C o m p o n e n t Co m m u n i t y M e e t ing 1 1 1.. * M a n a g e m e n t < < b l o c k > > < < b l o c k > > < < b l o c k > > < < b l o c k > > < < c o m p o n e n t> > < < c o m p o n e n t > > < < D A O > > < < d a t a > > Liqui d D e m o c r a c y V oting Is s u e o r L e g a l T e x t Advo c a c y a n d A w a r e n e ss S y s t e m P r o p o s a l D a t a P r o p o s i n g A s s o c i a t i o n <<block>> < < b lo c k > > FrontEnd <<block>> De c e n tr a l iz ed <<block>> < < c o m p o te n t > > <<device>> Autonomous Equ i ta b l e R e s s o u rce Organization Mobile Distribution <<block>> <<block>> < < b lo c k > > < < da t a > > Provider <<block>> << p h y s ic a l >> Limite d R e s s o urces <<device>> <<block>> <<block>> Data Device B r o w s e r < < c o m p o n e n t > > Smart Contract < < b l o c k > > < < b l o c k > > < < b l o c k > > U s e r & I d e n t i t y << c o m p o n e n t> > < < c o m p o n e n t > > << c o m p o n e n t >> M a n a g e m e n t Re s s o u rc e s u s a g es Lim it e d R e s s o u r ces R e s s o u r c e s < < b lo ck > > T r a c k i n g < < b lo c k > > M a n a g e m e n t A l l o c a t i o n s Block c h a in S y stem < < b l oc k > > << p h y s ic a l >> << d e v ic e >> Miner < < b l o c k > > M a c h in e < < d a t a > > <<block>> DAO Member Data <<block>> << component>> <<component>> <<block>> Limited Resources Limited Resources <<data>> Access Validation Access Request Ressources <<block>> Allocations Data <<block>> <<block>> Execution Client Consensus Client Blockchain Pattern <<block>> <<data>> Ressources Level Data <<block>> <<block>> Oracles Reverse Oracles <<block>> <<block>> <<block>> <<block>> <<block>> Execution State Tx Mempool Blobs data Beacon State Beacon Chain Detailed Block Definition Diagram of the system Fig. 4. 12 Julien Hue, Irina Rychkova, and Nicolas Herbaut Association derive from the DAO block, built with the LD component, Commu- nity Meeting component, User & Identity Management component, and Equi- table Resources Distribution component. These components support previously described business processes. In green are elements in the ArchiMate Technology Layer. We describe the Blockchain System to provide comprehension, including execution and consensus clients, and blockchain patterns like Oracle and Re- verse Oracle for user interaction and smart contract triggering. We implemented a simplification of the BDD of the system in ArchiMate (see Figure 5). Application Layer Federation Association Community Management Management Meeting Service Service Service Association Strategies LD Voting System Data Association Service Goal Data Federation Association User Identity A s so c ia ti o n Management R o le D a t a Service Application Resources Interface Management Service Federation Data Delegation Data Community Meeting Management Proposal Liquid data Ressources User Interface Democracy Allocations Decentralised Voting System Data Autonomous Equitable DAO Member User & Identity Ressources Resource Organisation Data Level Data Management Distribution Limited Ressources Data Technology Layer Network Smart Contract UI Server Blockchain Provider Miner Blobs Data Consensus Machine Blockchain Client Pattern Beacon State Beacon Chain Execution Client Tx Mempool Reverse Execution State Oracle Oracle Simplified ArchiMate Model of Application & Technology Layers Fig. 5. 5 Evaluation 5.1 Evaluation Strategy: Here, we are assessing how well the designed architectural artifact aligns with the anarchist society principles, goals and principles. Here is our evaluation ob- Alignment of DAO with Socio-political principles 13 jectives: 1) Validate the alignment(i.e: make sure that every elements from the motivation layer is linked to elements from the business, application and technol- ogy layer. And that the layer business is supported/realized by the application layer and that the application layer is supported/realized by the technology layer [29]) of the architectural artifact with anarchist society principles, 2) Asses the effectiveness of the artifact in supporting decentralised governance using DAOs and LD. This evaluation is conducted using the ArchiMate enterprise model and ac- cording to the analytical methods described in [25]. Therefore we will follow these steps: 1) Map principles to architecture elements, we are using ArchiMate view- point to be able to highlight the alignment, then we ensure that each principles, goals and requirements are supported by the architectural artifact 2) Scenario based analysis, here we develop hypothetical scenarios to test how the architec- layer7 ture handles specific challenges To do so, we will use the motivation that describes all of our requirements, principles goals. 5.2 Evaluation Criteria and Measures Goals and principles Alignment and Traceability realization8 Using the ArchiMate viewpoint of the requirements , we can see that all of the business, application and Technology layers are used to support the motivation layer. We sum up these alignments in the Tables 3 and 4. To better understand them we recommend first to look at Figure 6. Core Layers Business Internal Business Internal Motivational Business Business Business Object Business Event Business Service Active structure Interface Layer Elements Behavior Element Element realizes realizes realizes serves serves serves realizes realizes serves serves serves Application Application Internal Internal Application Application Application Data Object Application Event Activ e S tr u c ture realized by Service Interface Layer Requirements/Principles Behavior Element E le m e n t realizes realizes realizes realizes serves serves serves /Goals realizes realizes realizes serves serves serves Technology Technology Internal Technology Technology Technology Internal Artifact Technology Event Service Interface Layer Activ e S tu r c ture Behavior Element E le m e n t ArchiMate & TOGAF layers alignment Fig. 6. In the Table 4, we merge \"Transparent Governance\" and \"Conflict Resolution Mechanism\" because they rely on the same Business, Application and Techno- logical elements. Same thing for \"Community Engagement\" and \"Collaborative Projects\". Finally only the \"Sustainable Practice\" is not covered directly by the architectural artifact but this requirements apply to how individuals are behav- ing in the system and how they decide to respond to members’ needs, so this 7 The motivation layer can be dound here: https://github.com/edoc2024/paper 8 The complete view can be found here: https://github.com/edoc2024/paper/ 14 Julien Hue, Irina Rychkova, and Nicolas Herbaut Goals alignment and Traceability Table 3. Goals Business Layer Application Layer Technology Layer \"decentralised Federative \"Community Meeting Service\", organisation\", \"Federation Management Service\", \"Mutual Aid\", \"Blockchain Network\" Non-Hierarchical \"Resources Management Service\", \"Voluntary Association\", and all the elements organisation \"Liquid Democracy Voting System\" \"Self-Management\" and supporting the network and all of the application \"Run a Federation Process\" supporting these services and all the subprocesses \"Community Meeting Service\", \"decentralised Federative \"Federation Management Service\", \"Blockchain Network\" organisation\" and \"Resources Management Service\", and all the elements decentralisation \"Run a Federation Process\" \"Liquid Democracy Voting System\" supporting the network and all the subprocesses and all of the application supporting these services \"Resource Sharing\" \"Resources Management Service\" \"Blockchain Network\" Equitable Resource business service and all the application and all the elements Distribution and the \"Equitable Resource supporting this service supporting the network Distribution Process\" \"Federation Management Service\", \"Mutual Aid\" business service \"Association Management Service\" \"Blockchain Network\" and the \"Collaborative and and \"Resources Management Service\" and all the elements Mutual Aid Initiaves Project Process\" and all and all the application components supporting the network the subprocesses of this process supporting these services \"Self-Management\" \"Association Management Service\", business service and \"Community Meeting Service\", \"Blockchain Network\" Autonmy &amp; the \"Collaborative and \"Liquid Democracy Voting system\" and all the elements Self-Management Initiatives Project Process\" and all of the applications supporting supporting the network and all of the subprocesses these services of this process \"Voluntary Association\" business service and \"Association Management Service\" \"Blockchain Network\" the \"Collaborative and Initiatives and all of the applications and all the elements Voluntary Association Project Process\" and all of the and services supporting it supporting the network subprocesses of this process Motivation Layer Requirements alignment and Traceability Table 4. Requirements Business Layer Application Layer Technology Layer Transparent \"Achieve Consensus\"service \"Liquid Democracy Voting System\", \"Blockchain Network\" Governance and and the \"Consensus \"User Identity Management Service\", and all the elements Conflict Resolution decision making through \"Community Meeting Service\" supporting the network Mechanism Liquid Democracy\" process \"Community Meeting Service\", Community \"Federation Management Service\", \"Blockchain Network\" Engagement and \"Collaborative and Initiatives \"Resources Management Service\", and all the elements Collaborative Project Process\" \"Liquid Democracy Voting System\" supporting the network Projects and all of the application supporting these services \"Resource Sharing\" \"Resources Management Service\" \"Blockchain Network\" Equal Access business service and all the application and all the elements to Resources and the \"Equitable Resource supporting this service supporting the network Distribution Process\" principle will be used when association are created to ensure sustainable prac- tices. Finally the Principle \"decentralised organisation\" is applied to all of the system as well as \"Liquid Democracy\" principle. Scenario based analysis We examine five scenarios involving our architectural artifact: 1) Federation Management of Member Needs, 2) Deciding the Direction for an Association, 3) Resource Sharing Among a Federation, 4) Conflict Resolution Between Associ- ations, and 5) Emergency Response and Resource Reallocation. 1) In the first scenario, a member submits a need request. The federation analyzes it, drafts a proposal, and uses the Liquid Democracy process for consensus. If approved, a new association is formed to address the need, with roles assigned. Outcomes Alignment of DAO with Socio-political principles 15 include effective need fulfillment and a transparent, inclusive process following anarchist principles. This scenario involve the following principles: \"voluntary association\" [4, 19, 23, 35, 36, 46] and \"direct democracy and consensus decision- making\" [4,11,13,19,23,37,44]. 2) In the second scenario, if an association needs a change in direction, feedback is gathered. The LD system aids in building consen- sus, refining goals, and developing an action plan. Roles and responsibilities are assigned, and progress is monitored and adjusted as needed. Outcomes are clear strategic direction and adherence to anarchist principles. This scenario involve the following principles: \"direct democracy and consensus decision-making\" [4, 11,13,19,23,37,44] and \"autonomy and self-management\" [9,11,19,36]. 3) In the third scenario, resource requests are evaluated within the federation. Resources are categorized, and limited resources are allocated based on priority. Outcomes are effective resource use and fulfillment of members’ needs. This scenario involve the following principles: \"commons and communal resources\" [11, 19, 23, 35, 44] and \"Mutual Aid\" [9,13,19,35,36]. 4) In the fourth scenario, conflict reports are discussed in a federation meeting. A resolution proposal is submitted to the LD process. If approved, the resolution is implemented, and the outcome is reviewed in the next meeting. Outcomes include fair conflict resolution and process trans- parency. This scenario involve the following principles: \"direct democracy and consensus decision-making\" [4,11,13,19,23,37,44] and \"Non-Hierarchical Organ- isation\" [13, 19, 37, 46]. 5) In the fifth scenario, an emergency prompts a rapid assessment and reallocation of resources. The LD system prioritizes emergency needs, reallocating resources from non-essential projects. Outcomes are efficient emergency response and improved system resilience. This scenario involve the following principles: \"decentralisation\" [9, 11, 19, 44, 46] and \"autonomy and self- management\" [9, 11, 19, 36]. These scenarios demonstrate that our architectural artifact supports anarchist society principles, effectively handling challenges in a decentralised organisation. 5.3 Discussion RQ1: The key findings are as follows: TOGAF ADM and ArchiMate effectively model socio-political artifacts for decentralized organizations. By structuring the model with the Motivation Layer (covering goals, requirements, and principles) and supporting it with the Business, Application, and Technology layers, we could visualize and manage alignment with anarchist society principles. The fi- nal steps of TOGAF ADM (Opportunities and Solutions, Migration Planning, Implementation Governance, and Architecture Change Management) were not implemented as they are meant for bringing the architecture into the real world, while our focus was on constructing the architecture and verifying its align- ment. However, these steps can be carried out using this reference architecture. RQ2: We found minor areas for improvement in ArchiMate, but not in TO- GAF ADM. For example, translating the Technology layer from Figure 4 into ArchiMate required compromises like making the miner central in the technology layer rather than the blockchain system. These issues are mainly syntactic, not 16 Julien Hue, Irina Rychkova, and Nicolas Herbaut semiotic. Overall, modeling decentralized organizations is feasible with Archi- RQ3: Mate and TOGAF ADM in their current forms. Our evaluation shows that the architectural artifact aligns with anarchist society principles, indicat- ing that DAOs using LD can support an anarchist society (see Section 5.2). We also developed a model for LD decision-making, but due to space constraints, here9. it is not included Anarchist literature underpinned the artifact’s develop- ment, with elements in the Motivation Layer (Section 4.3) representing principles and goals from the literature. Business processes enable the creation of federa- tions and associations using LD-based decision-making and equitable resource sharing, operationalizing preferences while adhering to anarchist principles (see Section 4.4). These processes are supported by DAOs and decentralized appli- cations, powered by blockchain and smart contracts, as detailed in Section 4.5. 6 Conclusion and Future work This paper provides a foundational model for integrating DAOs and LD with Empirical Val- anarchist society principles, but further exploration is needed. idation: Our theoretical model needs real-world validation. We plan to imple- ment and observe the system and individual behaviors in this new socio-political context. The goal will be one to document the instantiation of such an artefact and then conduct an Ethnography on the organization to be able to empirically validate the anarchist principles and this artefact already theoretically validated. COMSOC instead of LD: Computational Social Choice [5] can be another way to make decisions within DAOs. We believe that applying the same kind of Enhanced research we conducted on DAOs using COMSOC is very interesting. Modeling Techniques: We identified new modeling languages and techniques, Blockchain such as the DECENT framework by [32], to enhance our model. Patterns: We identified blockchain patterns to enhance the technology layer such as the \"Oracle\" or \"Reverse Oracle\" to let the real world interact with the blockchain and enables Smart Contracts. During the implementation of our artefact we might faced issues (e.g: Token management to determine collective ownership). Therefore We will conduct a systematic literature review to identify all relevant patterns to cover all of our use cases. This work could not be included in this paper. In this research, we investigate How can socio-political artifacts for decen- tralised organisations be addressed by Enterprise Modeling?, focusing on how DAOs and LD can model anarchist societies. We designed an architectural arti- fact using TOGAF ADM and ArchiMate for this purpose. Our main contribution is the reference architecture presented in this paper and where a more detailed version is available here https://github.com/edoc2024/paper. 9 These models can be found at: https://github.com/edoc2024/paper Alignment of DAO with Socio-political principles 17",
  "2024-KHRS": "1 Introduction The Model-Driven Architecture (MDA) provides a foundational framework for the design and development of enterprise information systems, emphasizing a top-down design approach with three levels of model abstraction: the Compu- tational Independent Model (CIM) for capturing business goals, the Platform Independent Model (PIM) for representing system functionality independent of specific technologies, and the Platform Specific Model (PSM) for detailing the technological choices made [13]. This structured approach allows for deferred technological decisions, assuming that such choices primarily have localized im- pacts. However, the MDA approach often overlooks the integration of softgoals due to their subjective and qualitative nature. Softgoals, unlike hardgoals, lack clear- cut achievement criteria and encompass aspects such as cost, security, response 2 E. Kiomba et al. time, and performance. These qualitative attributes are crucial for successful software production as they influence user satisfaction, feature prioritization, and development trade-offs. Motivated by the need to better incorporate qualitative attributes and ana- lyzing technological impacts into enterprise modeling, we previously introduced Technology Aware for Enterprise Modeling (TEAEM)[17] which extends MDA by integrating model checking, validation and impact analysis of technological choices before product generation. This approach enables the unification of CIM- PIM within a “Unified Model”, allowing the identification and resolution of incon- sistencies across different MDA levels. If inconsistencies are detected, TEAEM reports constraint violations to the business user and assists in decision-making. TEAEM had some limitations. Firstly, it focused on the unification of CIM and PIM without extending support to the PSM. Additionally, the approach was centered on the use of hardgoals, with little attention given to softgoals. This paper aims extends TEAEM with two major contributions: First, we integrate the MDA low-level PSM using SysML component modeling, achiev- ing a unified CIM-PIM-PSM model that enables bottom-up constraint propa- gation and analysis. This integration ensures that technological choices made at the PSM level are consistently reflected across all abstraction levels, enhancing the overall coherence and bottom-up traceability within the enterprise modeling process. Second, we extend TEAEM by integrating softgoals into the CIM. By including softgoals in the early stages of modeling, we can better address qual- itative attributes throughout the system development life-cycle. This approach allows for the generation of code (configuration) guided by the satisfaction of these softgoals, thereby optimizing the system for user satisfaction and strategic business goals. The remainder of this paper is organized as follows: Section 2 provides a brief overview of key concepts such as MDA, Software Product Lines, and Softgoals. Section 3 details the TEAEM approach and its extension to integrate softgoals. Section 4 discusses the implementation, mapping details, and an illustrative example used to validate our approach. Finally, Section 5 presents the discussion and conclusions of this work. 2 Background 2.1 MDA, Top-Down Design and Enterprise Architecture The architecture of a system constitutes what is essential about that system considered in relation to its environment[8]. There are different frameworks used in enterprise architecture and software engineering. Model Driven Architecture is a software design approach that provides a set of guidelines for structuring specifications, which are expressed as models. The core of MDA is its focus on three primary types of models; Computation Independent Model represents the system’s requirements and business context, without detailing the structure or processing. Platform Independent Model spec- ifies the system’s structure and functionality but abstracts away the details of Title Suppressed Due to Excessive Length 3 any specific platform. Platform Specific Model combines the specifications in the PIM with the details of how the system uses a particular type of platform. Many works exist in the literature that exploit the MDA concept to make goal alignment and traceability of goal. In [14], authors presents a strategy-to-code (S2C) methodological approach integrating organizational, business process, and information system modeling to support strategic alignment in software devel- opment. It emphasizes model-driven development and conceptual modeling for semi-automatic software generation and traceability across different modeling levels. By using a working example, it illustrates how strategic definitions can be transformed into specific software components through the integration of Lite*, Communication Analysis, and the OO-Method, demonstrating the feasi- bility of achieving strategic requirements, traceability, and automatic software generation. Recent Enterprise Architecture frameworks exist that give advice on the pro- cesses and practices of Enterprise Architecture, and one of these, that the Archi- Mate Specification is fully aligned with, is the TOGAF standard[8]. ArchiMate is focus on the the problem of aligning strategic business plans with the plans and goals for the development of Information Systems based on Information Technology (IS/IT). 2.2 Software Product Lines MDA and SPL are complementary approaches that greatly benefit from the integration of softgoals. Software Product Line Engineering (SPLE) serves as a methodological framework for developing software families, characterized by significant overlap in functional and non-functional attributes [3]. A primary focus of SPLE is to identify and codify the common features of a cluster of software-intensive applications within a specific domain [1]. An SPL is generally composed of three main types of artifacts: Problem Space: this describes the features and functionalities desired by users across all potential products in the family. Solution Space: This includes the design and implementation specifics for all components across different products within the SPL and The Mappings: These establish the connections between features in the problem space and their respective components in the solution space. In this work, we choose SPL like application domain of TEAEM. 2.3 Softgoals and Utility Functions for Evaluating Softgoals Softgoals serve as quality criteria that software systems or their components must meet during development. Softgoals are subjective, relative, diverse, and interactive, making them crucial yet challenging to model due to their qualitative nature [7]. The literature highlights the significance of softgoals in determining the suc- cess of software projects. For instance, consider the softgoals for a software sys- tem such as performance (\"The system should respond quickly to user inputs\") 4 E. Kiomba et al. and security (\"The system should protect user data from unauthorized access\"). These qualitative goals pose the question: how can we effectively evaluate them? Significant research has been conducted on identifying softgoals within In- formation Systems (IS). For example, one study presented a comprehensive list of 114 softgoals identified in information systems [12], while another proposed a taxonomy categorizing these softgoals into four groups: Real and Web-based Systems, Web-based Systems, Real-time Systems, and Information Systems [5]. We aim to adapt some of these identified softgoals and quantify them. To quantify softgoals, we propose writing a utility function for each softgoal, facilitating the evaluation of how well a system meets these goals. Softgoals like usability, reliability, and performance are inherently qualitative. By converting these into quantifiable metrics through a utility function, we enable more objec- tive assessments and comparisons. The following section will introduce our extended approach and explain how to generate configurations that meet these softgoals. 3 TEAEM Approach guiding configuration by softgoals TEAEM is a holistic approach that extends the top-down MDA design paradigm with bottom-up traceability and constraint propagation analysis. 3.1 TEAEM Extension We propose extending TEAEM with the following contributions: (1) Integration of PSM to illustrate technological choices using component modeling (SysML). (2) Integration of softgoals to guide business users in generating configurations within the SPL. Model Design Constraints Analysis (top down) (bottom-up) Fig. 1: TEAEM approach to meet Softgoals. Hashed blocks show TEAEM ex- tension proposed in this paper Title Suppressed Due to Excessive Length 5 Figure 1 illustrates the extension of the TEAEM approach. The model is subdivided into two parts: On the left hand side represents the top-down MDA design approach with three abstraction levels: CIM, PIM, and PSM. – CIM Layer: We start by creating the goal model using the i* framework[4] with the PiStar tool[15]. This model includes both hardgoals (functional requirements) and softgoals (non-functional requirements). – PIM Layer: We design the Feature Model using the FeatureIDE tool [10]. – PSM Layer: We create component models with SysML, which include tech- nical metadata about alternative implementations of the designed solution. On the right-hand side, we extend TEAEM by unifying the PIM, CIM, and PSM layers of MDA into a single model called the “Unified Feature Model (UFM)”. This unification is crucial for several reasons: – Model checking and validation help validate the UFM to confirm it meets requirements, identify inconsistencies in the specification, and show conflicts between elements of the unified feature models. – Impact Analysis: Helps identify how changes in one part of the model affect others by reporting constraint violations from low-level to high-level. – Configuration Generation in SPL: Facilitates the generation of configurations that satisfy softgoals. To achieve this unification, we define the following mapping: – Goal to Feature Mapping (TEAEM): This mapping associates business goals with the technical features that fulfill those goals. It reflects the technical assumptions made by system engineers. – Feature to Goal Mapping (TEAEM): This mapping associates technical fea- tures with the goals that could potentially be compromised or hindered by their implementation. – Feature to Component Mapping (this paper): This new mapping associates features with the specific technological components that implement them. The mappings are formalized using a symbolic mathematics library to struc- ture the logical expressions. This library also develops utility functions to opti- mize the softgoals. Further details are provided in section 4. After unifying the model, we use model-checking with a boolean solver to validate the UFM and identify inconsistencies causing conflicts between goals, features, and components. If model checking returns false, indicating no feasible configuration, we pro- vide a domain-specific interpretation of these conflicts and recommend solutions. If model checking returns true, indicating multiple feasible configurations, we in- tegrate softgoals into the decision-making process. We use a utility function to quantify each softgoal and apply a Multiple-Criteria Decision Making (MCDM) algorithm to optimize softgoal satisfaction, determining the best configuration for the business user. 6 E. Kiomba et al. The main distinction between TEAEM and other frameworks like MDA or ArchiMate is its integration and analysis of inconsistencies across different ab- stract level (goal, feature, component) within the MDA. While ArchiMate offers a holistic view of enterprise architectures, it does not specifically focus on de- tecting and managing inconsistencies between different model types or guiding product generation by softgoals. 3.2 Modeling Softgoals with the Utility Function As mentioned in section 2.3, we reuse some of the softgoals defined by [12]. In this context, Key Performance Indicators (KPIs) are commonly used to measure the performance of various aspects of a system. However, we use a utility function instead of KPIs because the utility function allows for a more comprehensive and integrative approach. While KPIs provide valuable insights into specific performance metrics, they often fail to account for the trade-offs be- tween conflicting goals. The utility function, on the other hand, can encapsulate multiple softgoals into a single measure, allowing for the optimization of overall user satisfaction and system performance in a balanced manner. The utility function quantifies values derived from component modeling (value of each component) and serves as input for the MCDM algorithm to generate configurations that maximize user satisfaction. We focus on the following soft- goals for the implementation and test phases in sections 4 and 5. Table 1: Description of Softgoals with Blockchain Components Softgoals Description Refined-into Notation Cost System Operates with minimal C should be expenses and reduces the cost-efficient. overall cost of ownership. Response_Time System Processes requests quickly R should be and efficiently, providing fast. timely feedback to users. Throughput System Processes incoming data T should have quickly and sends new data high without delay. throughput. The overall goal is to minimize the total cost of the components, which is the C (x ) i. C sum of individual cost functions for each component is the total cost, i i x R and are the decision variables adjusted to optimize the cost. is the total i R (x ) i, T response time, is the response time function for each component and i i min T is the total throughput that we aim to minimize, , the throughput 0 i n i   i. function for each component We define this objective function by integrating the different softgoals to optimize: Title Suppressed Due to Excessive Length 7 F = w C + w R w T Min 1 2 3 \u0000 F C R Where is the combined objective function, is the total cost, is the T w , w , w total response time, and is the total throughput. are the weights 1 2 3 assigned to cost, response time, and throughput, respectively, reflecting their relative importance for the business users. The next section presents the different modules we developed. 4 Implementation We implemented TEAEM, an automated process for model unification across different MDA abstraction levels, detecting constraint violations, performing impact analysis, and generating software product line configurations driven by softgoals. modules1: We developed the following 1. Generation of the Unified Feature Model (UFM): The unified model includes the goal model, feature model, component models, and constraints. For top- down model design, we use the i* modeling language [4] and the PiStar tool for Goal Modeling, FeatureIDE for feature modeling, and SysML for component models. Mapping for CIM-PIM-PSM is achieved using SymPy with logical expressions. This module automates the generation of the unified feature model. 2. Extraction of Constraints from the UFM : This module performs model check- ing/validation. We use FeatureIDE and its bundled Sat4j solver [11] to re- trieve constraints from the UFM, generating an output in JSON for con- straint analysis when inconsistencies arise between the three MDA levels. 3. Interpretation of Constraint Violations and Their Implications: We devel- oped a module that takes the outcomes.json list of constraint violations from FeatureIDE and produces domain-specific interpretations of these violations to guide business users in their decision-making. 4. Generation of Configurations that Meet Softgoals: This module calculates utility functions using SymPy, based on parameter values from SysML com- ponent models. The utility function’s Min/Max values help identify the best configuration that matches the specified requirements. 5. Optimization Using the Topsis Algorithm: We implement the Topsis algo- rithm for Multiple-Criteria Decision Making, ensuring the optimization and evaluation of our Unified Model to achieve highly optimized solutions. We make three types of mapping as specified on the section3.1. For The mapping of Goal to Feature and Feature to goal, We use the mathematical logical expressions rules that we passed with Sympy in order to execute automatically the logic. 1 https://github.com/Eddykams/TEAEM_develop 8 E. Kiomba et al. For the mapping PIM-PSM, We have two cases: (1) Mapping One Feature - One Component: The following bijection function modeling the mapping between PIM-PSM : H = F = C (A) 1 ) with, F: Features and C: Components. (2) Mapping One Feature With Multiple Components: We use TOPSIS (Tech- nique for Order of Preference by Similarity to Ideal Solution) is a multi-criteria decision analysis method that ranks alternatives based on their distance to an ideal solution and a negative-ideal solution. It helps in selecting the best option by comparing the relative closeness of each alternative to the ideal solution. In the next section, we present an illustrative example where we apply TEAEM to configure products to meet soft goals. 5 Illustrative Example: Counterfeiting Drugs in Supply Chain 5.1 Motivation The market of counterfeit drugs has become a 200-billion-dollar business annu- ally, according to the World Health Organization (WHO). According to a WHO report, up to 10% of all sold drugs globally are fake, with significantly higher rates in parts of Africa and Asia. This business is very dangerous because of life- and-death implications for patients (thousands of deaths every year) and also for the pharmaceutical industry’s reputation, which can lose a lot of money. The complexity of pharmaceutical supply chain operations is the primary reason for this issue. With a large number of handovers to different supply chain partners before drugs reach hospitals and pharmacies, there is a significant lack of trace- ability. Manufacturing is vulnerable to counterfeit raw materials or ingredients from unknown sources. Moreover, illicit producers can relabel fake products to infiltrate legitimate distribution channels. For the pharmaceutical sector, the European Union (EU) and the Drug Supply Chain Security Act (DSCSA) in the United States provide a deadline of 2023 for the industry to implement a traceability system. Based on the work [9][6], we present the following illustrative example. Com- pany X, based in Europe and Africa, must enter the pharmaceutical market in compliance with EU regulations and operate in Africa. The organization has many branches in different countries that require a system to manage trans- actions recorded across multiple locations. Additionally, there is a need for a system that enhances trust among various stakeholders. The system must address the following needs: Reduce drug counterfeiting (fraud detection) by ensuring traceability and transparency at every step of the product transfer. Ensure secure data management by enhancing privacy through access controls and encryption, and by maintaining the integrity and immutabil- ity of data. Manage recalls of defective products; conditions of transporting drugs Title Suppressed Due to Excessive Length 9 must be verified through real-time audits using IoT, sensors, or RFID tags. The authenticity of products must be checked by log-audit. 5.2 TEAEM for configuration in SPL to meet softgoals The main goal of Enterprise X is to determine if, with their goals, features, and components, they can generate a product or if there are any inconsistencies between them before generating the product. The organization proposes to inte- grate the following softgoals: maximal performance, minimal cost for blockchain components, better response time, and minimal cost for the electronic solutions (RFIDs, IoT sensors, or NFC tags). Step1: We present a partial goal model of the enterprise, depicting different softgoals and the links between goals and softgoals in the Figure 2. Step 2: To build the feature model, we made a literature review to iden- tify different solutions proposed for combating drug counterfeiting in the sup- ply chain illustrated in 2. According to some findings in the literature, [2][16] assert, for example, that the ideal anti-counterfeit technology in an enterprise system should have a high level of safety, faster product application, established standards, simple to check, have automatic authentication, be accessible to con- sumers, and comply with industry regulations. Table 2: Requirements and Technologies Identified Goals - Requirements References Blockchain Cloud ML ￿ Transparent [16] - - ￿ ￿ Traceability [16][2][19] - ￿ ￿ ￿ Reliable [18][16][2][19] ￿ High Level of Safety [16] - - ￿ faster product application [18] - - ￿ Simple to audit [16] - - ￿ ￿ Automatic Process [18][16][2] - ￿ ￿ Compliant with industry Regulations [18][19] - ￿ Accountability [18] - - ￿ Trust [16] - - ￿ Decentralized [16][2] - - ￿ High Security [2] - - ￿ Flexibility [18] - - We built the feature model based on the technologies identified in 2. Three prominent technologies were proposed: blockchain technology, for its features such as traceability, trust, and immutability; Cloud Computing [18], known for its flexibility and faster application production; and Machine Learning, which can identify counterfeit drugs through algorithmic data analysis. Additionally, associated electronic equipment for traceability is proposed, such as RFID tags, IoT sensors (pressure, geolocation), and NFC tags. 10 E. Kiomba et al. Fig. 2: Goal Modeling of Enterprise X Fig. 3: Feature Model of Enterprise X Fig. 4: Component Model of Enterprise X Title Suppressed Due to Excessive Length 11 Step 3: Based on the analysis presented in Table 2, we decided to use blockchain technology as a component to illustrate our example. Blockchain is a novel technology that has not yet been widely adopted by enterprises due to the lack of standardization and scarcity of engineers skilled in its implementation. The literature identifies two principal types of blockchains: public blockchain and private blockchain. Using SysML, we illustrate the component model for blockchain technology and other technologies. However, for the evaluation phase, we focus on the value of the blockchain component. Step4: TEAEM - The unified model, as depicted in Fig.5, put within a sin- gle model the three abstract level of the MDA. In our study, the model checking results were positive, indicating that the unified model is valid. However, the next step involves generating configurations that satisfy the softgoals defined in the previous section. Fig. 5: Model Unification Step 5: After model unification, we use model checking/validation to de- termine if a valid configuration exists for this unified model or if there are any inconsistencies. The unified feature model generates 4 possible configurations, in- dicating that there are no inconsistencies or constraint violations. Therefore, refining the model is unnecessary. In [17], we presented an example where con- straint violations were detected. The question then arises: how do business users choose the \"best\" configuration that meets their softgoals? Step 6: As outlined in the previous section, to generate configurations guided by the satisfaction of softgoals, we detail the softgoals discussed in Section 3.2, including their associated utility functions and the blockchain components em- ployed. The values of component properties are inputted as parameters into the utility functions to select the optimal configuration. We use blockchain configu- rations and utilize the TOPSIS method for multi-objective optimization. 12 E. Kiomba et al. In the absence of a generalizable throughput/response time formula for all possible cases, we have chosen to formulate the following assumptions: Response Time is the minimal time for the detection of counterfeiting drugs and depends on the component properties: Transaction Processing Time, Block Creation Time, and Smart Contract Execution Time. The throughput is a function of block size, block execution time, and network bandwidth for the blockchain. Table 3: Utility functions and blockchain component properties of softgoals Softgoals Utility function Components properties Deployment cost, n C = (min C (D , M )) Blockchain Cost i c c i=1 Maintenance cost Block Size, P T = T (B , B , N ) Throughput Block Time, s t b Network Bandwidth. Tx Processing Time R = R(T x , B , SC ) Response Time Block Creation Time i c e SC Execution Time Deployment cost n EC = (min C (D , E , M )) Electronic_Cost Execution cost i c c c i=1 Maintenance cost P 5.3 Results After integrating the blockchain component values as parameters in the utility function to generate products, we made the following tests: (1)We help to choose configurations that meet one softgoal. For example, when the organization needs to minimize blockchain costs: Fig. 6: Configuration guiding by the Minimal Blockchain Cost To minimize blockchain costs, the implemented module maps the value of the component across different configurations, calculates the value using the utility function implemented by SymPy, and returns this value for comparison with Title Suppressed Due to Excessive Length 13 other configurations. The configuration with the lowest cost is considered the winner. For one component, it is easy to generate a product that matches this softgoal. (2) Help to choose configurations that meet multiple criteria: In this case, we have a multi-objective optimization. We need to choose the best configuration guided by multiple softgoal criteria. Table 4: Softgoals Criteria with Heigh Attributes Blockchain Cost Response_Time Throughput Electronic Cost Conf0 300 15 seconds/bloc 18 Tx/seconds 140 Conf1 350 15 seconds/bloc 16 Tx/seconds 120 Conf2 200 2 seconds/bloc 100Tx/seconds 200 Conf3 250 3 seconds/bloc 200Tx/seconds 180 For the test phase, it is important to clarify that the values presented in Table 4 are provided for illustrative purposes only and do not represent calculations from a utility function. Instead, these values are intended to demonstrate the application of MCDM with TOPSIS. The table 4 illustrates the four configurations found after the model check- ing/validation. Different values (Blockchain Cost, Response Time, Throughput, and Electronic Cost) were passed as inputs to our component model. In this case, we have a multi-objective optimization where we need to choose the best configuration guided by multiple softgoal criteria. Many optimization algorithms exist in the literature, and we implemented the TOPSIS method for Multiple- Criteria Decision Making. For the evaluation phase, for example, if we have input data illustrated in Table 4, we have the following outcomes: Scenario 1: We defined the same weight for all parameters [0.25,0.25,0.25,0.25] and identified Response Time, Blockchain Cost, and Electronic Cost as non- benefit parameters (functions to minimize) and Throughput as a benefit param- eter (function to maximize). We have the following outcomes: Table 5: Scenario 1 Outcomes Overview Attributes BC_Cost R_Time Throughput E_Cost S_POS S_NEG Score Rank Conf0 300 15 18 140 0.2215 0.0862 0.2801 3 Conf1 350 15 16 120 0.2349 0.0613 0.2069 4 Conf2 200 2 100 200 0.0689 0.2139 0.7561 2 Conf3 250 3 200 180 0.0532 0.2215 0.8061 1 To reduce the blockchain cost, the cost of the electronic equipment, and the re- sponse time, while increasing performance with throughput, the best configura- tion is Configuration 3 (Conf3), which achieves 80% satisfaction among business 14 E. Kiomba et al. users. The worst configuration is Configuration 1 (Conf1), with only 20% satis- faction among business users. This is clearly specified with different constraints and the values of blockchain components as parameters. The configuration that meets the specified softgoals is Configuration 3. Scenario 2: Business users place significant emphasis on Response_Time com- pared to other criteria. We adjust the weight according to preferences, for ex- ample, 0.6 for Response Time and 0.13 Blockchain Cost, 0.13 Throughput and 0.13 Electronic Cost.([0.13, 0.61, 0.13, 0.13]). Table 6: outcomes for Scenario 2 Attributes BC_Cost R_Time Throughput E_Cost S_POS S_NEG Score Rank Conf0 300 15 18 140 0.3682 0.0344 0.0856 3 Conf1 350 15 16 120 0.3695 0.0245 0.0622 4 Conf2 200 2 100 200 0.0275 0.3675 0.9301 1 Conf3 250 3 200 180 0.0347 0.3416 0.9075 2 The best configuration has changed and is now Configuration 2 (Conf2), with 92% satisfaction among business users. The worst configuration is still Configu- ration 1 (Conf1), with only 6% satisfaction among business users. We can clearly see that a different softgoal weight will lead to different configurations. 6 Lesson Discussion, & Research Challenges The TEAEM approach by integrating softgoals ensures that qualitative at- tributes such as user satisfaction, security, and performance are prioritized, re- sulting in configurations that align more closely with business goals. The unified CIM-PIM-PSM model enhances coherence and traceability across abstraction levels, leading to fewer errors and more complete configurations. This integra- tion ensures that technological choices made at the PSM level are consistently reflected across all levels, reducing the risk of inconsistencies. TEAEM-generated configurations exhibit fewer errors and higher completeness, as validated by model checking. The use of the TOPSIS method for MCDM in TEAEM facilitates the selec- tion of optimal configurations based on multiple softgoal criteria. This approach allows business users to make informed decisions quickly, balancing trade-offs between competing goals. Empirical data show that TEAEM reduces the time required for configuration generation and decision-making compared to manual methods and other SPL approaches. Compared to other approaches, TEAEM adds value in model-checking val- idation and constraints analysis propagation for bottom-up traceability. MDA focuses on top-down design, traceability, and alignment of goals from business goals to code generation. ArchiMate is designed to provide a comprehensive, integrated view of enterprise architecture, focusing on the relationships between Title Suppressed Due to Excessive Length 15 different domains (such as business, application, and technology). However, it does not inherently include specific mechanisms for detecting and managing in- consistencies between different model types. Additionally, while ArchiMate can represent various goals and requirements, it does not explicitly focus on guiding product generation through softgoals. The implementation of TEAEM and the various evaluations carried out have allowed us to learn the following lesson: – Business users have the ability to express their preferences on the different softgoals, and expressing these preferences will have a technical impact. The weight or importance that a business user assigns to a softgoal during the configuration of a product in a Software Product Line (SPL) significantly influences the best configuration that can be proposed to them. TEAEM presents certain limitations, such as the complexity involved in for- malizing utility functions for systems with non-linear attributes. This complex- ity can challenge the accurate quantification and optimization of softgoals. Also, managing the explosion of the feature model in large-scale projects remains a significant challenge, potentially complicating the optimization of solutions for specific product derivations. Future work on the TEAEM approach should focus on developing formal guidelines for defining and applying utility functions, which will improve accu- racy and make the approach more adaptable across various scenario. Exploring advanced modularity techniques will be crucial for managing the complexity of feature models and addressing the feature model explosion problem, simplifying the process of configuration generation. Evaluating the methodology’s practical impact through real-world applications is also critical to ensure its effectiveness. By tackling these areas, the TEAEM approach can be further enhanced to offer greater value in enterprise modeling. 7 Conclusion In this paper, we extend the Technology-Aware Enterprise Modeling approach by incorporating softgoals to generate configurations within Software Product Lines. Our enhancement addresses the limitations of MDA methods in managing non-functional requirements, which are essential for user satisfaction. Our primary contribution is the unification of the CIM-PIM-PSM into a single model, ensuring a traceable design process that connects business goals with technical implementations. We also integrate model checking and impact analysis to detect inconsistencies early, aiding decision-making and minimizing errors. The second contribution integrates softgoals into the TEAEM process, enabling SPL configurations to meet both goals and softgoals, ensuring the final products align with both user needs and business goals. We demonstrate our approach using a counterfeit drug detection example in the supply chain, leveraging blockchain technology as a component. Future work will focus on managing feature models to address the challenge of feature model explosion. 16 E. Kiomba et al.",
  "2024-RKHPNS": "1 Introduction The Model-Driven Architecture (MDA) provides a fundamental framework for the design and development of enterprise information systems, prioritizing the top- down design process. It introduces three levels of model abstraction and a model transformation process, ensuring a traceability and alignment between the high- level business goals and developed technological components [10]. In this method- ology, the selection of technological platforms and components is deferred to lower levels of the MDA design hierarchy. This approach is justifiable under the assump- tion that technological choices have primarily local effects. However, the emergence of disruptive technologies, such as blockchain challenges the established principles of MDA by exerting a broader influence on processes and goals of the enterprise as a whole.[9]. The intrinsic properties of blockchain not only create efficiencies locally, but also introduce strategic limits extending beyond the initially targeted areas of implementation. Consequently, the adoption of such technologies requires a holis- tic analysis of their potential impacts on the business processes and objectives enterprise-wide. Moreover, this analysis cannot be postponed to the later de- sign stages, but accompany the whole design process, increasing the technology- awareness already at the early design stages. 2 I. Rychkova et al. We propose an approach for Technology-Aware Enterprise Modeling (TEAEM) that extends the MDA paradigm with the bottom-up constraint propagation and analysis. We depart from the idea that technical properties defined at the lower MDA abstraction levels can create implicit ‘side effects’ in the higher MDA levels. This position paper introduces TEAEM and presents the first results of its imple- mentation. Some of the TEAEM steps are supported by the well known modeling tools. We developed several modules for model transformation, model unification and interpretation of results. The current version of TEAEM supports only goal - feature model unification. The component modeling will be integrated in the nearest future. We illustrate our approach on a theoretical example of an Orga- nization seeking to implement a blockchain solution for its processes. The results of this analysis provide the ground for a technology-aware business (re)design and decision-making for the Organization. In Section 2, we provide a brief reminder on the MDA and discuss its challenges; in Section 3, we introduce the TEAEM approach; in Section 4, we present our il- lustrative example; in Section 5, we provide a road-map for the future development of TEAEM and present our conclusions. 2 Background Model Driven Architecture (MDA) is a software design approach that provides a set of guidelines for structuring specifications expressed as models, support- ing traceability and business-IT alignment. MDA defines three primary types of models: Computation Independent Model (CIM) represents the system’s require- ments and business context without detailing the structure or processing; Platform Independent Model (PIM) specifies the system’s structure and functionality but abstracts away the details of any specific implementation platform; Platform Spe- cific Model (PSM) provides the technical details on system implementation using a particular technology or platform. Methods and approaches for enterprise system design grounded on MDA are discussed in research literature for several decades. Alignment between enterprise models at different MDA abstraction levels is addressed in [17, 5, 21, 22]. Numer- ous works focus on validation and analysis of alignment between business pro- cesses and goals [4, 3, 1]. Formal methods are proposed for alignment verification in [18]. To acknowledge the constantly changing business environment, integra- tion of organizational strategy and structure into MDA is addressed in [13]. In [14], a semi-automated strategy-to-code approach that integrates organizational, business process, and information system modeling is introduced. This approach is grounded on LiteStart modeling method [12] and ensures traceability across modeling levels. The research presented in [9, 16, 11] acknowledges the rapid evolution and com- plexity of technological solutions and their impact on organizational strategy and processes. While technological solutions may initially excel in addressing specific business goals, their implementation can introduce strategic limits in other areas. Title Suppressed Due to Excessive Length 3 This necessitates the evolution of design approaches, including MDA, and moti- vates the TEAEM approach presented in this work. 3 Technology-Aware Enterprise Modeling TEAEM is a holistic approach that extends the (top-down) MDA design paradigm with the bottom-up traceability and constraint propagation analysis. 3.1 TEAEM: Steps Figure 1 illustrates the TEAEM approach. We use goal modeling [20], feature modeling [23] and component modeling [19] to represent an enterprise solution at the three MDA abstraction levels. The top-down Model Design follows the MDA paradigm and consists of de- veloping a Goal Model (CIM), a Feature Model (PIM) and a Component model (PSM) and their respective model transformations. The bottom-up Constraint Analysis represents our main theoretical contribution and consists of Model uni- fication and Model checking/Impact analysis steps. TEAEM approach Fig. 1. Goal Modeling (CIM). A goal model representing the high-level business goals, requirements, and domain concepts is created in this step. This model cor- respond to CIM - the highest level of abstraction within MDA. It outlines the de- pendencies between goals and intended outcomes ensuring alignment with broader organizational context. These dependencies are explored in the further constraint analysis. Feature modeling (PIM). A feature model is defined in this step. This model specifies the hierarchical structure of abstract technical functionalities (fea- tures) of the prospective solution and corresponds to PIM. The dependencies and 4 I. Rychkova et al. constraints between the features define alternative configurations for the technical solution. Component modeling (PSM). A component model is created in this step. This model specifies technological components and platform-specific details nec- essary for implementation of the features defined in PIM. Technical constraints between the components provide the information about alternative implementa- tions of the designed solution. Model Unification. In this step, business goals, technical features, and com- ponents are specified within a single Unified Feature Model (UFM). For further impact analysis, we use logical expressions to formalize the mappings between the model elements defined at different MDA abstraction levels. We define the following mapping types: Goal to Feature Mapping associates the business goals with the technical features satisfying these goals. This mapping reflects technical assumptions made by system engineers. Feature to Goal Mapping associates the technical features with the goals that can be possibly compromised or inhibited by implementation of these features. Feature to Component Mapping associates the features with the specific technological components implementing these features. Component to Feature Mapping explicitly defines possible restric- tions or incompatibilities between the features and the components. Component to Goal Mapping indicates possible restrictions between the (soft) goals and the components such as quality, feasibility, performance issues etc. We combine these mappings with the constraints defined in the previous TEAEM steps to form the UFM. Model checking / Impact analysis. We use model-checking to validate the UFM and to identify inconsistencies (if any) in its specification. A solver finds possible configurations of a solution and/or shows the conflicts between the elements of the unified model. We propose a domain-specific interpretation of these conflicts and recommendations for their resolution. The Constraint Analysis can be performed at different stages of Model design and serve to: (a) identify and propagate the effect of design decisions specified in PIM on CIM; (b) identify and propagate the effect of design decisions specified in PSM on PIM and/or on CIM. 3.2 TEAEM: Implementation In this work, we illustrate a semi-automated model unification and constraint analysis between CIM and PIM abstraction levels of MDA. developed3: To implement the approach, the following three modules are 1. Generation of the Unified Feature Model (UFM): The unified model comprises the goal model, features model, and constraints. For the top-down model de- sign, we use the i* modeling language [2] and the PiStar tool [15] for Goal Modeling, and the feature modeling environment FeatureIDE [7]. The map- ping for different MDA abstraction levels is added manually with an external 3 https://github.com/Eddykams/TEAEM_develop Title Suppressed Due to Excessive Length 5 file using logical expression notation. We develop an automated Python mod- ule [transform.py] which takes as input the goal model, the feature model, and the provided constraints, and automatically transforms them into a unified model. 2. Extraction of the constraints from the UFM: We use FeatureIDE and its bun- dled Sat4j solver [8] to retrieve the constraints from the UFM. We generate an output in JSON that is further used for the constraint analysis step. 3. Interpretation of the solution (constraints violation and their implication bottom- up): We develop a Python module [interpretation.py] that uses the .json list of constraint violations extracted from the UFM in FeatureIDE as an input and produces the domain-specific interpretations of these violations that can be further used for enterprise models redesign. The current version of our tools is semi-automated. The goal model is designed with piStar tool, the feature model is designed with FeatureIDE. The mapping between goal model and feature model is formalized using logical expressions in an external text file. This file is used as an input to generate our Unified Model. The model-checking of the unified feature model is automatically applied by Fea- tureIDE. The model-checking results are extracted and interpreted by the devel- oped module. We are working to provide a fully automated mapping process for the next version. 4 Illustrative Example We illustrate the TEAEM approach on a simple example of the Organization that seeks to enhance transparency and auditability of its processes. The Organization considers blockchain technology as a platform for its enterprise solutions. In par- ticular, the Organization needs to choose between two blockchain platforms (Pub- licBC_X and PrivateBC_Y). Further, in the design, it will configure the selected blockchain platform and its components. Since the Organization is also concerned with the GDPR compliance, more detailed analysis of the abstract capabilities and technical functionality of the prospective blockchain solution is required. We illustrate how TEAEM can support the analysis of technical constraints and their impact on the defined business goals of the Organization. Goal Modeling (CIM): The goal model illustrated in Fig. 2.(a) defines the two goals of the Organization: ‘GDPR Compliance’ and ‘Auditability’ that have to be achieved. We specify this dependency with the i* And-refinement link that corresponds to the logical conjunction: Business Goals => GDPR Compliance AND Auditability Feature Modeling (PIM): The feature model illustrated in Fig 2.(b) specifies the abstract functionalities associated with blockchain technology. The prospective Technical Solution of the Organization (modeled as a root feature) will be grounded either on the functionalities of a ‘PublicBC_X’ or a ‘PrivateBC_Y’. We specify this with the alternative feature group in FeatureIDE that corresponds to exclusive OR expression: 6 I. Rychkova et al. Technical Solution => PublicBC_X XOR PrivateBC_Y (a): Goal Model of the Organization; (b): Feature Model with a proposition of Fig. 2. alternative blockchain solutions; (c): Unified Feature Model that merges (a) and (b). Once the goal model and solution proposition are formalized, we proceed with the (bottom-up) constraint propagation and analysis. Model Unification. We create a unified feature model that specifies the ‘En- terprise System’ of the Organization (Fig. 2.(c)). Our UFM represents the goals (from CIM) and the features (from PIM) using the feature modeling formalism. Here we model ‘Technical Solution’ and ‘Business Goals’ as mandatory features of the ‘Enterprise System’ root feature. To formalize the mapping between the created MDA models, We assume the following assumptions: we consider generic properties of Public blockchain X and Private blockchain Y. The Private blockchain Y limits the auditability of the blockchain because their nodes are controlled by a single (private) entity and the immutability is contingent upon the trustworthiness of this entity. The Pub- lic blockchain X does not face this immutability issue because any node can participate in the consensus and validation processes, ensuring that operations are conducted fairly and transparently. Nevertheless, this immutability of public blockchains prevents from implementation of a fundamental GDPR’s ’right to be forgotten’ - an alteration/deletion of data on demand. We map the technical fea- tures defined in PIM to the goals that can be compromised by implementation of these features using the following logical expressions: Compliance; PublicBC_X => NOT GDPR Title Suppressed Due to Excessive Length 7 PrivateBC_Y => NOT Audit Goal model, Feature model and Feature to Goal mapping presented above are doc- umented as XML files and are used for semi-automated generation of the UFM. Model checking/ Impact analysis. The FeatureIDE automatically executes the model-checking on the generated UFM. In our example, the UFM is invalid (i.e., the model is void), meaning that no Enterprise System satisfying actual constraints can be configured. We identify and trace the sources of conflicting con- straints to the model elements defined at different MDA levels (PIM and CIM in our example). Using our developed Extraction and Solution Interpretation mod- ules, we provide the domain-specific constraint interpretation, aiming to assist business users in redesign. Domain-specific constraint analysis & interpretation. Fig. 3. In this example, the business expert faces the problem where any choice of blockchain solution compromises one of the business goals defined by the Organi- zation. Along with modification or refinement of the prospective Technical Solu- tion, this analysis calls for redefinition or re-prioritization of the Business Goals in response of the bottom-up constraint propagation. 5 Conclusion and Future Work We presented TEAEM approach that contributes to design of enterprise systems within the MDA paradigm. While existing approaches support traceability and alignment between goals and technical solutions top-down, they often fall short in guiding stakeholders through the decision-making process and analysis of incon- sistencies bottom-up. Our approach enables reconciliation between business goals, technical capabilities and specific solutions through formal analysis and propaga- tion of constraints. We outlined the TEAEM steps (illustrated in Fig.1). Model unification and Model checking / Impact analysis are the two steps that extend the MDA provid- ing constraint propagation and analysis. We developed a technique for the semi- automated generation of the Unified Feature Model (UFM) and used the feature modeling environment FeatureIDE for automated model-checking. 8 I. Rychkova et al. In this article, we illustrated the propagation of constraints from PIM to CIM. Identification and propagation of the effect of design decisions specified in PSM on PIM and/or on CIM, and the automated mapping between the elements at different MDA levels of abstraction will be addressed in our future work. We illustrated our approach on a short theoretical example. Developing a re- alistic example is an important next step. We plan to conduct a case study on the design and analysis of a Supply Chain Management solution based on blockchain technology. In this case study, propagation and impact of technical properties related to specific blockchain solutions on the organizational supply chain man- agement process and strategic goals will be examined. In particular, we will focus on trustworthiness goals addressed in our previous work [6].",
  "2024-RR": "1 Introduction Cyber-physical systems (CPS) refer to integrated computational capabilities, networking, and physical processes. These systems use embedded computers and networks to monitor and control physical processes, often with feedback loops, where physical actions influence computational decisions and vice versa [12]. Social entities (organizations and individuals) are inherently involved in the CPS lifecycle from development and standardization to implementation and daily use. Trust is an essential component of the CPS adoption [6]. CPS are charac- terized by their high degree of complexity, interconnectivity, and the ability to interact with both the physical world and computational elements seamlessly. Compared to social or interpersonal trust [5,13], in a CPS context, there are mul- tiple entities (social, physical, or computational) upon which CPS users need to place their trust. Examining trust formation in such a complex heterogeneous system provides valuable input for CPS designers and developers, improving sys- tems’ trustworthiness and positively affecting their acceptance and adoption by the users [6, 19, 24]. We address this challenge with a method for iterative analysis and elicitation of trustworthiness requirements elaborated from [16]. The method is grounded on the Six-Variable Model [25] originally defined to support design of control 2 P. Rambert et I. Rychkova systems. It provides a structured framework for presentation and analysis of the relationships between various CPS elements. The presented method supports traceability and alignment between trust concerns expressed by the CPS users and stakeholders, trust assumptions made by the CPS designers in order to address these concerns, trustworthiness requirements, and technical system com- ponents that will be developed to meet these requirements. This article presents the details of the method and reports on the case study of the ASSA project where this method was applied. ASSA (Assistance Sécurité Seniors Application) is a startup creating a personal emergency response system for the elderly. ASSA solution uses connected smart devices to monitor user’s vital parameters and to trigger an alert in case of emergency.The team of two engineers delivered design documentation for ASSA following a conventional software design process. How- ever, explicit trust analysis and trustworthiness requirements elicitation were not conducted. This led us to consider ASSA an appropriate case for the method application. The goal of our study is to demonstrate that explicit trust analysis offers valuable insights for the CPS design process, enhancing traceability and leading to the identification of new relevant requirements that contribute to the system’s trustworthiness. This article is organized as follows: In Section 2, we discuss the background on trust in CPS; In Section 3, we provide the details on the method of trustworthi- ness requirements elicitation; In Section 4, we present our research methodology and introduce the ASSA case study; In Section 5, we present our case study results; We provide the concluding remarks in Section 6. 2 Background 2.1 Trust in CPS Research Trust is a social construct that emerges from interactions between individuals or groups and can be described by a situation where a subject (trustor) is willing to rely on a chosen actions of an object of trust (trustee) [20] [5] [13]. Advances in socio-technical systems introduce novel models of social and business interac- tions, where IT artifacts can take the role of a trustee [22]. Trust in technology reflects trustor’s beliefs that a specific technology has the attributes necessary to perform as expected in a given situation where negative consequences are possible [14] [15]. CPS applications for assisted living provide individuals with support within their living environments [23] [4] [1]. Recent technological advances expanded the capabilities of CPS, enabling real-time health monitoring, fall detection, medica- tion management, and personalized assistance. However, multiple issues related to privacy, data security, interoperability, standardization, and integration of CPS systems into larger ecosystems undermine the CPS users’ and stakehold- ers’ decision to trust and to be engaged with a CPS [6] [1] [19]. Trust, defined as the user’s willingness to rely on a system in case of an emergency, is a pre- requisite for CPS technology adoption and must be explicitly addressed in their design [24] [19] [17]. While interpersonal or social trust can be defined as a function of trustee’s perceived ability, benevolence and integrity [13], physical and computational Title Suppressed Due to Excessive Length 3 entities exhibit technical properties and attributes that might predict the user’s decision to trust (and by consequence to be involved with) the CPS [14]. Given the complex and interconnected nature of a CPS context, where multi- ple social, physical and computational entities involved, there may be no obvious central or identifiable trustee upon which to base trust decisions [6]. Moreover, non-technical CPS stakeholders have limited capacity to objectively assess tech- nical properties of a CPS and to reason about its trustworthiness. In [6], the following eight trust constructs in CPS are defined: familiarity and understand- ing of the CPS by consumers; reliability, predictability and consistency; security; integrity; competence, expertise and functionality required to interact with the CPS; the benevolence and helpfulness of the CPS for consumers; personaliz- ability; faith and belief consumers have in the service delivered. These concepts capture the complex nature of trust relationships in CPS. To accurately address CPS trustworthiness during design, it is essential to conduct a thorough analysis of user trust-related concerns and systematically translate these concerns into trustworthiness requirements [16]. 2.2 Trustworthiness Requirements and Trust Assumptions In systems engineering, trustworthiness of a system means “to be worthy of being trusted” to fulfill some specific requirements [18]. In this work, we elaborate on the method for explicit trustworthiness requirements elicitation proposed in [16]. Trustworthiness requirement (TwR) can be defined as a statement made by a trustor about the expected trustworthiness of a trustee. This statement must clearly express an operational, functional, design, or other characteristic, which, according to the trustor’s beliefs, positively impacts the trustworthiness of this trustee and the interaction between the two. Sutcliffe [24] introduces ’soft’ requirements as a linguistic concept that encom- passes various phenomena related to people, organizations, and society, including trust. Grounded on this work, TwR can be considered as a subclass of soft re- quirements and can be refined by functional and non-functional requirements. A RE method aiming to systematize the elicitation and analysis of requirements, including trustworthiness requirements, and grounded on the ontological analysis is proposed in [3]. Here, TwR are considered as a special class of quality require- ments. The Reference Ontology of Trustworthiness Requirements (ROTwR) [2] proposes decomposition of TwR into reliability requirements, truthful informa- tion communication requirements and transparency requirements. In the context of CPS, TwR define the desired outcomes these systems should achieve to address the trust concerns (TC) of users and stakeholders within the socio-physical environment. However, the CPS controlling software - the main focus of the development project addressed in this study - can only affect the machine domains, not the socio-physical ones. This concept is known as the world-on-machine paradox, as formulated in [26]. Consequently, TCs cannot be directly mapped into functional, non-functional, or quality requirements, nor can they be linked directly to CPS components within the machine domain. Instead, system engineers must interpret these concerns from the social domain to the machine domain [27] [7] [21]. This process involves delegating the user’s trust decisions to system engineers, who make explicit or implicit choices to trust 4 P. Rambert et I. Rychkova certain technical characteristics or components. These choices are referred to as design assumptions in [9]. An ontology of assumptions proposed in [26] defines world assumptions (WA), machine assumptions (MA), world dependence assumptions (WDA), and ma- chine dependence assumptions (MDA). A world assumption is an assumption about world (or social) phenomena, which constraints the machine environment. A machine assumption is an assumption about a machine’s internal phenomena. A machine dependence assumption states that an external world phenomenon depends on some machine phenomena. In contrast, a world dependence assump- tion states that a machine phenomenon depends on some world phenomena. The authors use situation calculus to reason about assumptions and requirements. Grounded on [8] [26], trust assumptions (TA) can be defined as the assump- tions made by the system engineers about the properties of a system-to-be and its various components (including human components), which will positively affect the perceived trustworthiness of the system. 3 Method for Trustworthiness Requirements Elicitation Fig. 1. (a) Overview of the ASSA Case Study: the timeline; (b) Method for Trust Analysis and TwR elicitation integrated into the human-centered design approach [10] The method presented in this work is grounded on [16]. For trust analysis, we use the Six-Variable Model [25] originally defined to support design of control systems. This model provides a structured framework for analysis of the relation- ships between various system elements situated in the socio-physical environment (i.e., users, stakeholders) and in the machine domain (i.e., a control machine, Title Suppressed Due to Excessive Length 5 sensors, actuators, other connected systems). It defines six variables that are de- picted as relations between different system elements: referenced variables that focus on the properties that should be observed in the system’s environment; monitored variables represent the properties that need to be monitored dur- ing the system’s operation; input variables represent the external stimuli that affect the system’s behavior; output variables refer to the outcomes produced by the system; controlled variables refer to the properties that the system can actively control or actions the system can take to achieve its desired behavior; desired variables refer the properties in the system’s environment that should be achieved during the system’s operation. Figure 2 illustrates the six-variable model for the ASSA system (our case study). By analyzing the relationships between monitored, controlled, input, output, referenced, and desired variables, developers can precisely articulate the system requirements – TwR in our case - and their implication of system functionalities and behavior. For trust analysis in CPS, we use the formalism proposed in [8] [16] for documenting TC, TA, TwR and traceability between TA, TwR and system elements. Our method defines seven steps (see Fig. 1-b) as follows: Step 1: A global system model based on the Six-Variable Model is created. This model focuses on the problem domain and the effects that must be achieved in this domain. It captures the context of use and forms the foundation for the subsequent steps. Step 2: Trust concerns are collected from the users and project stakeholders. In case of retrospective analysis, trust concerns can be extracted from the existing design documentation; Step 3: Trust assumptions are collected from the technical stakeholders. An assumption is something taken as being true or factual and used as a starting reasoning1. point for a course of action or Trust assumptions refer to the (social) trust concerns and justify the design choices made during the solution design. We use the four kinds of assumptions defined in [26] to address the ’world-on- machine paradox’ and to align the problem domain and the machine domain. Step 4: The trust concerns and trust assumptions are mapped to their relevant elements in the global system model. Each trust concern may refer to one or several elements in the system model. Step 5: The global system model is refined: the high-level problem is refined into sub-problems. Trust concerns and trust assumptions are elaborated. If some trust concern is not addressed - a design assumption should be made and a new element, feature or property needs to be suggested for the solution. This step repeats until all trust concerns are covered and a desired level of detail is achieved; Step 6: The trustworthiness requirements are derived from the trust assumptions and integrated into system design documentation. Step 7: The new technical features and/or components are derived, contributing into trustworthiness of the design solution. The design specification is updated. The method is consistent with the activities of the ISO 9241 human-centered design approach [10]: Specifying the context of use (Step 1); Specifying the user 1 https://www.merriam-webster.com/thesaurus/assumption 6 P. Rambert et I. Rychkova requirements (Step 2-6); Producing the design solutions (Step 7). Based on the evaluation results, a new iteration of trust analysis and TwR elicitation can be triggered. The User-centered evaluation of the design is out of scope for this article and will be addressed in future. 4 The Case Study To demonstrate our method for eliciting trustworthiness requirements and to evaluate its effectiveness, we conducted a case study on the ASSA software de- velopment project. 4.1 ASSA: the Application for Assistance and Security for Elderly ASSA is an innovative personal emergency response application designed for iOS and Android devices. Potential users of ASSA are elderly people and people liv- ing in isolation, both geographically and socially. The application can potentially integrate with compatible smartwatches and other wearable devices to monitor vital parameters (e.g., heart rate, blood pressure, oxygen saturation levels) and detect accidents (e.g. falls). When an irregularity in vital parameters or an ac- cident is detected, the application initiates a series of actions to ensure quick assistance. First, it triggers a timer, during which the user is prompted to con- firm or cancel the emergency. If the emergency is not canceled, ASSA transmits the emergency alert to the designated healthcare providers (e.g., a hospital emer- gency service) and notifies a designated caregiver (e.g., a person of trust or a family member), providing them with the data related to the situation (e.g., a health report). The team of two engineers conducted the market analysis, requirements spec- ification and developed initial design documentation for ASSA between August and November 2022 (see Fig. 1-a). Regular monitoring and emergency handling are the main uses-cases of ASSA. Other functionalities have been elaborated later during the project. In particular, systematic generation and management of on-line health reports have been added to ASSA as a result of this study. According to the latest design specification, the ASSA application ensures user monitoring and alert generation in case of emergency and provides rele- vant historical data for the medical professionals (for both regular and emergent medical interventions) for more efficient personalized treatment. 4.2 Research Methodology We adapt the single-project case study research protocol defined by Kitchenham and Pickard in [11]. Planning and designing of the case study. The objectives of the case study is to evaluate the feasibility, efficiency and relevance of our method for the anal- ysis and elicitation of trustworthiness requirements in the ASSA project. While trust was recognized by the ASSA developers as an important factor for adop- tion, no specific trust analysis has been conducted during the project. This made Title Suppressed Due to Excessive Length 7 the project a relevant case for the study. We define the following hypothesis for this case study: H1: The method is complementary with a design process not focused on trust. H2: The method application uncovers implicit TwR in the existing system spec- ifications. H3: The method application leads to identification of the new TwR (extending the system specifications). H4: Documentation of trust assumptions enhances traceability and alignment in the designed system. H5: Method application contributes into (re)definition of a valid and relevant design solution. Conducting the case study. The study was conducted between November 2022 and June 2023 (Fig. 1-a). First, we conducted the trust analysis of the system in retrospective, applying the method on the data collected in the ASSA project before November 2022 and to the design documentation produced by the ASSA developers. In the second iteration, we conducted the trust analysis of the system in prospective, applying the method on the new data. We conducted seven semi-structured interviews with prospective ASSA users and healthcare professionals. Additionally, we organized a series of working sessions with the ASSA developers, concentrating on the themes of trust, trustworthiness, and acceptance of the prospective application. Each interview lasted between 20 and 45 minutes. After a brief presentation of the assisted living technologies, the interviewees have been introduced to ASSA and invited to discuss their own ex- perience with the assisted living technology and their reasons to adopt (or not) one. Compared to interviews conducted by ASSA developers in the earlier days of the project, this data collection focuses on subjective trust issues and beliefs of the (non-technical) stakeholders. Here are some example questions from the interview guide (translated from French): - How would the personal emergency response system improve your work or your personal security? - For what reasons would you take a personal emergency response system? - Do you have any preoccupation when a relative of yours uses a personal emer- gency response system? - Does a “connected” personal emergency response system changes any preoccu- pation you have regarding a typical personal emergency response system? Both retrospective and prospective analysis demonstrated the method feasibility (H1). We addressed the method efficiency (H2, H3) by eliciting the TwR in both iter- ations. We were able to identify new features contributing to the system trust- worthiness and to propose an update for the base line ASSA design solution (As-Is). To evaluate the method relevance (H4, H5), we presented the refined ASSA system model and the updated design specification to the ASSA devel- opers for review and validation. The case study results are presented in the following session. 8 P. Rambert et I. Rychkova 5 Trustworthiness Requirements Elicitation for ASSA 5.1 The Retrospective Trust Analysis Step 1: The global system model of ASSA is created based on the market analysis and design documentation produced by the ASSA developers (Fig. 2). Here ASSA mobile application is represented as a control system in a Six-Value- Model [25]. The root requirement is defined by the main use case of ASSA: Provide monitoring and assistance. The problem domain includes the user (an elderly person whose health is monitored), her designated as caregivers and healthcare practitioners. The control machine or the software to-be represents the ASSA mobile application. The connection domain between the machine and the problem domains includes sensors (e.g., smartphone, smart watch) and the external systems with which the control machine interacts by sending alerts, re- ports, and emergency calls (e.g., health emergency services, messaging service, and online reporting system). Reference variables include “health report” and Fig. 2. Step 1: The ASSA global system model “help notification”; monitored and input variables include “detection of motion”, “vital signs” and “location”; output variables include “call ambulance” linked to the health emergency system (e.g., Ambulance), “send notification to caregiver” linked to the messaging service (e.g., SMS), and “create health report” linked to the reporting system. The controlled variables include “send ambulance” linked with the User; “alert”, and “location of the hospital” linked to the Caregiver; “share health report” linked to the Healthcare practitioners and Caregiver. The desired variables are composed of “help received”, linked to the User; “information sent”, linked to the Caregiver; “health report”, linked to the Healthcare practi- tioner. Step 2: We conducted a semantic analysis of the ASSA design documentation (As-Is) provided by the ASSA developers, including the market reports and in- terviews with medical professionals, potential users and caregivers. Using various Title Suppressed Due to Excessive Length 9 definitions of trust from the literature, we identified seven trust concerns TC1-7 (Table 1). Step 3: We identified the design decisions related to the TC specified in the previous step from the ASSA design documentation (As-Is). We identify the as- sumptions made by the ASSA developers about the properties of the ASSA CPS and its various components to justify these decisions. We conducted the inter- views with the ASSA developers to validate the TA that have been discovered. We classify them according to the ontology proposed in [26]. Table 2 illustrates the list of TA with their related TC. During the retrospective trust analysis, we extracted 11 trust assumptions TA1.01-TA1.11. We use the assumption ontol- Table 1. Trust concerns expressed by the prospective ASSA users and stakeholders Iter. Step 2: Trust concern 1 TC1 User is concerned about whether she will be constantly monitored 1 TC2 User is uncertain whether she will receive help upon feeling unwell 1 TC3 User is concerned with who will be able to access the monitoring data 1 TC4 User is concerned about whether she will be able to use the system in a proper way 1 TC5 User and caregiver are concerned about whether the device is charged and working 1 TC6 Caregiver is concerned about whether the assistance will be timely pro- vided in response to the alert 1 TC7 Healthcare practitioners are concerned about relevance and exactitude of the health report 2 TC8 User is concerned about the system generating false alerts 2 TC9 User is concerned that the monitoring data and the health report will not be used by the healthcare professionals to personalize/improve the treatment 2 TC10 User is concerned that his data will be used for commercial purposes 2 TC11 User is concerned about the health report being shared/used without her consent ogy from [26], identifying world assumptions (WA), machine assumptions (MA), world dependence assumptions (WDA), and machine dependence assumptions (MDA). World assumptions are engineers assumptions about the social phenomena. For example, we assume that the healthcare provider sends an ambulance each time the ASSA emergency alert is received (see TA1.10) - this assumption means that our solution cannot be trusted if the healthcare provider ignores the alert or has not enough resources to respond to it. We also assume that the user should find the ASSA interface simple and intuitive (TA1.08). Otherwise the system will not be trusted by the user. Machine assumptions are engineers assumptions about the internal properties of the application, smart devices or other (external) systems and services that 10 P. Rambert et I. Rychkova Table 2. Trust assumptions discovered from the retrospective analysis (TA1.xx) and made during the prospective analysis (TA2.xx) linking the trust concerns with system elements Trust Assumption: Kind [26] TC: Linked to System elements: TA1.01 Redundant sensors increase the MA TC1 Smartwatch/Smartphone monitoring reliability TA1.02 User can check the sensor’s sta- MDA TC1 ASSA Mobile application; tus and activity Smartwatch/Smartphone TA1.03 Irregularities in health metrics WDA TC2, ASSA Mobile application are correctly identified from the TC8 data TA1.03.01User can trigger an alert man- WDA TC2, ASSA Mobile application ually TC8 TA1.04 The alert transmission is guar- MDA TC2 ASSA Mobile application; Net- anteed by the system work provider TA1.04.01WiFi and Mobile communica- MA TC2 Network provider tion is reliable TA1.05 Emergency calls are responded WA TC2 Health Emergency service 24/7 TA1.06 Data is encrypted MA TC3 ASSA Mobile application; Mes- saging service; Reporting system TA1.07 Data can be accessed only by WDA TC3, Messaging service, Reporting authorized persons with the TC10- system, Caregiver, Healthcare verified identity 11 practitioner TA1.08 The interface is simple and in- WA TC4 ASSA Mobile application; tuitive Smartwatch/Smartphone TA1.09 User is informed when the bat- MDA TC5 ASSA Mobile application tery of the smartphone or the smartwatch is low TA1.10 The healthcare provider han- WA TC6, Health emergency service, dles emergency alerts received TC9 Healthcare authorities from the system TA1.11 Software is certified and recog- WA TC7, Healthcare authorities nised by healthcare authorities TC9 TA2.01 Redundant sensors prevent MDA TC8 Smartwatch/Smartphone from false alerts TA2.02 Explicit request for the user MDA TC10- ASSA mobile application, Re- consent 11 porting system TA2.03 GDPR-complience MDA TC10- ASSA mobile application, Re- 11 porting system TA2.04 Reporting system is highly MA TC9 Reporting system, Healthcare available practitioner TA2.05 Healthcare practitioners recog- WA TC9 Reporting system, Healthcare nise and use the health report practitioner TA2.06 Healthcare practitioners pro- WA TC9 Healthcare practitioner vide qualified help TA2.07 User training materials are pro- MDA TC1, ASSA Mobile application; vided TC4-5 Smartwatch/Smartphone Title Suppressed Due to Excessive Length 11 will positively affect the perceived trustworthiness of the system. We assume that the redundant sensors in the system affect reliability of the patient monitoring (TA1.01). A machine dependence assumption states that an external world phenomenon depends on some machine phenomena. For example, we assume that the user can trust the system if she can check and make sure that the sensor is working and constantly monitoring (see TA1.02). A world dependence assumption states that a machine phenomenon depends on some world phenomena. For example, we assume that the system detecting an anomaly (and/or raising an alert) means that the user is not well (TA1.03, TA1.03.01). Step 4: We map the TC (Step 2) and TA (Step 3) to the relevant parts of the ASSA system. We update the system model to show the traceability (Fig. 3). ç Concerns TC1, TC4, TC5 are related to ASSA mobile application and/or the smart devices - the User interface. TC2 questions the whole system and its purpose - it is related to the main requirement. TC3, TC7 are related to the ASSA interfaces for data exchange with the external systems. TC6 is related to the interfaces between the system and the control domain. They refer to service- level agreements and operational-level agreements that need to be defined. Fig. 3. Global system model of ASSA annotated with trust concerns from the Iteration 1 (TC1-TC7) and Iteration 2 (TC8-11). Step 5: We refine the global system model by decomposing the main problem ’Provide monitoring and assistance’ into the sub-problems corresponding to the 12 P. Rambert et I. Rychkova use-cases of ASSA defined in coordination with the project developers: P1: Monitor the user and detect emergencies. This sub-problem consists of continuously collecting data about the user’s vital parameters from the sensors (e.g., smartwatch, smartphone) and generating user’s health metrics. Data are analyzed by the dedicated algorithms (i.e., compared to historical data or some baseline established by a physician). If some irregularities are detected, the alert is triggered. P2: Send ambulance to the user if an emergency is detected Once the alert is triggered, the system transmits the alert to a healthcare emergency service, which sends an ambulance in response. Healthcare practitioners (e.g., paramedics, physicians) intervene to provide help to the user. User data related to the triggered alert (i.e., the health report) is communicated to healthcare professionals through the reporting system. P3: Inform the caregiver about the emergency. The system notifies the caregiver via a text message (e.g., an SMS), providing her with the data related to the alert and the emergency intervention that followed (e.g., the address of the hospital where the user was transported). P4: Manage and share health reports. The system records the user’s health metrics creating regular health reports and incident health reports in an on-line Reporting system. With the user’s authorization, these health reports can be communicated with the caregivers and the healthcare professionals for emergency interventions and regular check-ups. We illustrate the refined system diagram for the sub-problem P2 in Fig. 4. TA1.01-04, TA1.08, TA1.09 are related to the ASSA Mobile system, smartwatch and smartphone – the elements that will be directly manipulated by the user. TA1.05-07, TA1.10 are linked to the external systems in the machine domain (i.e., Health emergency service, Messaging service, Reporting system). These are assumptions about how the ASSA Mobile application should be integrated with these systems and about specific properties or functionalities that these external systems must ensure. TA1.04, TA1.10-11 are related to the entities that are not included in the problem model – Network provider and Healthcare au- thorities. Step 6: Using the TA, we formulate 11 trustworthiness requirements (1.01-1.11 in Table 3). We link the TwR with the TC they are addressing. Each TwR reflects one or more TA. For example, the requirement TwR1.05: The system shall be integrated with (recognized by) the health emergency service (e.g., Ambulance or SAMU in France) is associated with TA1.05: Emergency calls are responded 24/7 and TA1.10: The healthcare provider handles emergency alerts received from the system (Table2). Step 7: The TwR are formulated for ASSA Mobile application, for the external systems and services (e.g., health emergency service, reporting system), and for the entities in the domain environment (e.g., healthcare practitioner, healthcare authorities, mobile network provider). The TwR related to ASSA mobile appli- cation can be considered as functional or non-functional requirements. Some of these requirements align with the As-Is design solution (indicated ’+’ in Table2), whereas the others lead to new design features. For example, TwR1.01, TwR1.03, TwR1.07 suggest the update in the ASSA mobile interface As-Is. The TwR re- lated to external systems and services can be considered as non-functional, in- Title Suppressed Due to Excessive Length 13 tegration requirements, and regulatory and compliance requirements. They also introduce new elements to the design. For example, TwR1.09 focuses on the in- tegration and compliance with the (existing) platforms for sharing medical data. TwR1.05-06, TwR1.08 highlight the importance of service-level agreements with healthcare and mobile network providers. 5.2 The Prospective Trust Analysis The objective of this analysis is to extend our understanding of the system and its environment, focusing on the trustworthiness and acceptance of the system by its stakeholders. Compared to the retrospective analysis, here we apply the method to the newly collected empirical data for prospective TwR elicitation. Step 1: We use the global system model of ASSA developed in the Iteration 1 (Fig. 2) and proceed with TC extraction. Step 2: Through qualitative data analysis we were able to confirm the TC iden- tified in the Iteration 1 and to identify new TC (see Table 1). In particular, the interviews reveal that the prospective users are concerned with a possibility of a false alert and the purposeful use of the collected data (see TC8-11, Table 1). Step 3: We made the new trust assumptions about the (technical) properties of the system To-Be, which, if implemented, would alleviate the TC expressed by the users (TC8-11) and improve system trustworthiness. Note, that some trust concerns are addressed by the TA formulated in the Iteration 1. For example, TA1.03, 03.01 are already addressing the trust concern about the false alerts (TC8). The new TA are listed in Table 2 (see TA2.01-2.05). For example, we assume that the user can trust the system if she can learn to use the system from the documentation/support materials provided (TA2.07). Step 4: We associate new TC and TA with the system components. Fig. 4 illustrates the traceability between the TA and the system elements for the sys- tem model for P2: Send ambulance to the user if an emergency is detected. Step 5: We update the sub-problems (see the retrospective analysis) and their system models (omitted in this paper). Step 6: We formulate five TwR (TwR2.01-2.05) and link them with their cor- responding TA and TC. Step 7: While TwR2.01 aligns with the ASSA As-Is design solution, the other four TwR are new. They have been validated by the ASSA developers and led to the design update. TwR2.02, TwR2.05 require the extension of the ASSA application interface whereas TwR2.03-04 need to be addressed by the external Reporting system and by the Healthcare practitioners (integration, regulatory and compliance requirements). 5.3 Discussion Our study shows that the proposed method is applicable in retrospective (fol- lowing up on a design process not focused on trust) and in prospective (being integrated in a method such as human-centered design), validating H1. We ex- amined the identified TwR with the ASSA developers: five TwR correspond to the ASSA requirements As-Is; six new TwR led to the ASSA specification update 14 P. Rambert et I. Rychkova Table 3. Trustworthiness requirements TwR Description Associated Addressing Included with TA TC: into ASSA? TwR1.01 The system shall provide the user with TA 1.02 TC1 new / val the means to control the sensors status TwR1.02 The system and the watch shall have a TA1.08 TC4 + simple and clear interface TwR1.03 The system shall alert the user when the TA1.09 TC5 new / val battery charge is low TwR1.04 The system shall be compatible with cer- TA TC1, TC8 + tified/reliable sensors/components 1.01/2.01 TwR1.05 The system shall be integrated with TA1.10, TC2, TC6, new / val (recognized by) the health emergency TA1.05 TC9 service (e.g., Ambulance or SAMU in France) TwR1.06 The system shall be certified / approved TA1.11; TC2, TC7, new/val by the healthcare authority TA1.03 TC8, TC9 TwR1.07 The user shall be able to trigger an alert TA1.03.01 TC2, TC8 new/ inval manually TwR1.08 The system shall use a reliable mo- TA1.04; TC2 new/val bile/internet network provider TA1.04.01 TwR1.09 The system shall use a report format TA1.06-07 TC3, TC9- new / val compatible with cloud reporting systems TA2.02-04 11 TwR1.10 The system shall encrypt all data, stored TA1.06 TC3 + and exchanged TwR1.11 The system shall be integrated with an TA1.07 TC3, + on-line Reporting system TC10-11 TwR2.01 The system has to be GDPR-compliant TA2.02 TC10-11 + TwR2.02 The user shall be able to control report TA2.02; TC3, new/val sharing TA1.07 TC10-11 TwR2.03 The reporting system must be highly TA2.04-05 TC9 new/val available TwR2.04 The healthcare practitioner has to use TA2.05; TC9 new/val the received health report while taking TA2.06 the user in charge TwR2.05 The user shall be able to access training TA1.02, TC1-2, new / val materials and guidelines for the ASSA TA1.03.01, TC4-5, mobile application online (e.g., FAQ, TA1.08-09 TC8 getting started videos etc.) Title Suppressed Due to Excessive Length 15 Fig. 4. Refined system diagram for the sub-problem P2: Send ambulance to the user in case the emergency is detected. Trust assumptions from the Iteration 1 (TA1.xx) and from the Iteration 2 (TA2.xx) are linked to the system elements. (Table 3). This validates our research hypothesis H2, H3 and demonstrates the method efficiency. We formulated 18 trust assumptions based on the retrospective and prospec- tive trust analysis (Table 2) and created a traceability matrix and system models explicitly linking the (social) trust concerns and the system elements. We con- ducted feedback sessions with the engineers to ensure a shared understanding and an added value of this traceability, validating H4. We formulated 11 new TwR with 10 recognized important and validated by the ASSA developers. TwR 1.07: ’The user shall be able to trigger an alert manually’ was not validated as it goes against the product vision of ASSA, where the system takes the whole responsibility for the emergency detection. By this, we were able to partly validate H5 and the method relevance. 6 Conclusions In this work, we applied the method for iterative trustworthiness requirements analysis and elicitation elaborated from [16] to the case study of the ASSA CPS. We formulated trust assumptions that justify technical decisions, supporting alignment between (social) trust concerns and specific properties of the system, and addressing the world-on-machine paradox formulated in [26]. The conducted trust analysis led to updates in the ASSA solution design, with a focus on enhanc- ing system trustworthiness. The ASSA developers acknowledged the significance of this analysis. In future work, we intend to evaluate the proposed design to assess the impact of TwRs on the perceived trustworthiness of the system. To achieve this, we plan to conduct user-centered evaluations and post-implementation usability testing. 16 P. Rambert et I. Rychkova",
  "2025-AR": "1 Introduction\nThe rise of Blockchain technology (BCT) marked a\nnew digital era by eliminating the need for centralized authorities, creating a trustless environment for\ntransactions. Instead of placing their faith in an intermediary, users could trust the blockchain. Historically, trust has been crucial for the adoption of new\ntechnologies. For instance, distrust in security hindered online shopping adoption in the 90s [Hoffman\net al., 1999]. Despite enhanced transparency, reduced\ncosts, and improved traceability, BCT adoption remains limited1. According to PwC2, in 2020, “45 %\nof companies investing in blockchain technology believe that lack of trust among users will be a significant obstacle in blockchain adoption”.\nTechnology adoption, defined as the act of beginning to use a new technology, is closely related to,\nbut distinct from, technology acceptance. While technology acceptance encompasses a more subjective behavioral intention to support or embrace the technology, it serves as a precursor to actual use (i.e., adopa https://orcid.org/0000-0002-1100-0116\n1Deloitte, 2021 Global Blockchain Survey,\navailable at: https://www.deloitte.com/global/en/\nour-thinking/insights/topics/emerging-technologies/\nunderstanding-blockchain-potential.html\n2PwC, ”Time for Trust: How Blockchain Will\nTransform Business and the Economy,” available\nat: https://www.pwc.com/gx/en/issues/blockchain/\ntion). Research community uses various theoretical\nmodels to explain the BCT acceptance mechanism\nand to define its factors [AlShamsi et al., 2022, AlAshmori et al., 2022, Norbu et al., 2024]. One of the\nrecurring factors of BCT acceptance is trust.\nThree forms of trust are widely recognized in the\nliterature: social trust, digital trust, and trust in technology. Social (or interpersonal) trust is defined as\nthe subjective probability that an entity - a trustee\n- has the required capacity and willingness to perform an action that is beneficial or at least not detrimental to another entity - a trustor - in a specific\ncontext [Gambetta et al., 2000]. Compared to social trust, digital trust defines relationships between\nentities in the digital world. It is the measure of\nconfidence that a trustor has in the trustee’s ability\nto protect data and privacy of individuals [Pietrzak\nand Takala, 2021]. Trust in technology is another\nform of trust that reflects trustor’s beliefs that a specific technology has the attributes necessary to perform as expected in a situation where negative consequences are possible [Mcknight et al., 2011, Meeßen\net al., 2019]. These forms of trust are intrinsic to\norganizations and have important implications in organizational decision-making and technology adoption [Mcknight et al., 2011, De Filippi et al., 2020].\nThey need to be explicitly addressed both in policy\nmaking, digital transformation strategies and solution\ndesign.\nWhile the research studies confirm that trust is an\nimportant factor in BCT acceptance, the conceptual-\nization of trust and its specific role are not discussed\nin detail. In this critical review, we intend to contribute into a deeper understanding of this role. We\nexamine the impact of trust on BCT acceptance, focusing on the role it plays in the defined theoretical\nmodels — whether as a predictor, mediator, or moderator. We explore the conceptualization of trust within\nthe defined BCT acceptance models. By analyzing\nhow trust is defined and measured, we aim to determine whether the researchers refer to social trust,\ndigital trust, or trust in technology in their theoretical\nframeworks.\nThe remainder of this article is organized as follows: In section 2, we go through the definition of\nkey terms and discuss the related works. Our research\nmethodology is described in section 3. In section 4,\nwe report the results of the analysis of 21 selected\nstudies, answering to our research questions. In section 5, we discuss our results and their potential implications and present our conclusions in section 6.\n2 Background and Related Works\n2.1 Theoretical Models for Technology\nAcceptance\nWhile adoption is focused on consistent use of technology in daily activities, it is often associated with\ntechnology acceptance. Technology acceptance refers\nto the willingness and readiness of individuals or organizations to adopt and use new technology. Technology acceptance is closely related to and often predicts technology adoption [Davis, 1989, Venkatesh\net al., 2003].\nTheoretical models such as the Technology Acceptance Model (TAM) are often applied within the\ncontext of information systems to understand predictors of human behavior toward potential acceptance or\nrejection of the technology [Marangunic´ and Granic´,\n2015]. The theoretical models specify a set of constructs and the relationships between them that explain a phenomenon of interest. The theoretical BCT\nacceptance models, for example, explain the effect of\nspecified independent variables on Behavioral Intention (BI) to use BCT - the dependent variable.\nThe constructs defined by the theoretical models\ncan play a role of predictors, mediators or moderators (Fig. 1) for the examined phenomena. Predictors are independent variables that directly influence\nthe dependent variable. Mediators are variables that\nexplain the mechanism through which predictors influence the dependent variable. They act as intermediaries in the causal chain, helping to clarify how or\nwhy a certain effect occurs. Moderators are variables\nthat affect the strength or direction of the relationship\nbetween predictors and the dependent variable. They\nprovide insights into when or under what conditions\ncertain effects occur. Some variables function both as\npredictors and mediators, depending on the specific\nrelationships being examined.\nTrust in blockchain technology (BCT) can be\nmeasured either directly, through survey items like\n”Do you find BCT trustworthy?”, or indirectly by capturing data on trust antecedents or related indicators.\nDirect questions pose a challenge as they leave ”trustworthiness” open to individual interpretation. Indicators are the observable variables that reflect the presence or extent of trust. They depend on the trust conceptualization chosen for the study (see Table 1).\nFigure 1: Overview of the variables and their relations in\ntheoretical acceptance models\nUnderstanding acceptance factors and relationships between them is crucial for technology adoption. Knowing the predictors allows for the design of\ntargeted interventions. Understanding mediators provides insights into the adoption process (i.e., how or\nwhy a certain effect occurs). For example, trust might\nmediate the relationship between system quality and\nuser acceptance. Understanding moderators allows\nfor the development of context-specific strategies. For\ninstance, age, gender, or cultural background might\nmoderate technology adoption, necessitating tailored\ncommunication and support strategies for different\ngroups.\nSeveral theoretical acceptance models are widely\nacknowledged in the literature. The Technology Acceptance Model (TAM) developed by Fred Davis in\n1989 is a theoretical framework used to understand\nand predict user acceptance of information technology [Davis, 1989]. This model defines two primary\npredictors of user acceptance: Perceived usefulness\nis the degree to which a person believes that using a\nparticular system would enhance his or her job performance. Perceived ease of use is the degree to\nwhich a person believes that using a particular system would be free of effort. TAM2 [Venkatesh and\nDavis, 2000] extends the original model by introducing the concepts of social influence processes (subjective norm, voluntariness, and image) and cognitive\ninstrumental processes (job relevance, output quality,\nresult demonstrability, and perceived ease of use) as\nkey drivers of user acceptance.\nThe Unified Theory of Acceptance and Use\nof Technology (UTAUT) was created in 2003\n[Venkatesh et al., 2003]. Built upon TAM, it identifies\nthe four predictors of acceptance: performance expectancy, effort expectancy, social influence and facilitating conditions. Those four factors are influenced\nby four moderators: age, gender, experience, and voluntariness of use, which result in possible gaps between the intentions to use a technology and the actual use of the technology.\nThe Task-to-Performance Fit (TTF) model [Goodhue and Thompson, 1995] is a theoretical framework\nthat examines how well technology matches the tasks\nit is intended to support, and how this fit impacts performance and technology adoption.\nThe Technology-Organization-Environment\n(TOE) [Baker, 2012] framework defines the factors that influence the organization’s decision to\nadopt new technology divided into three contexts:\ntechnological, organizational, and environmental.\nAcceptance predictors from these theoretical\nmodels show the importance of combining technical\nfactors and social factors, related to user’s personality\nand the context of use.\n2.2 Trust\nTrust is a social construct that emerges from interactions between individuals or groups and can be described by a situation where a subject (trustor) is\nwilling to rely on a chosen actions of an object of\ntrust (trustee) [Rousseau et al., 1998, Gambetta et al.,\n2000, Mayer et al., 1995]. Mayer, Davis and Schoorman define trust as a function of the trustee’s perceived ability, benevolence, and integrity and of the\ntrustor’s propensity to trust [Mayer et al., 1995]. Here\nability defines a group of skills, competencies, and\ncharacteristics that enable a trustee to have influence\nwithin some specific domain; benevolence defines the\nextent to which a trustee is believed to want to do\ngood to the trustor, aside from an egocentric profit\nmotive; integrity refers to trustee’s moral quality of\nbeing sincere, honest, and her capacity and willingness to adhere to some rules/principles.\nAdvances in technology introduce the new models of social and business interactions, where IT artifacts can take the role of a trustee [So¨llner et al.,\n2012]. Trust in technology reflects trustor’s beliefs\nthat a specific technology has the attributes necessary to perform as expected in a given situation where\nnegative consequences are possible [Mcknight et al.,\n2011] [Meeßen et al., 2019]. According to [So¨llner\net al., 2012], the antecedents of social trust (i.e., ability, benevolence and integrity [Mayer et al., 1995])\nare poorly suited for studying trust relationships between users and IT artifacts, since they are defined\nto fit the human character traits and human decision\nmaking. The authors of [Mcknight et al., 2011] provide a framework for understanding how trust in technology is formed and its effects on technology usage.\nThey put forward performance, functionality and reliability as the factors of trust in specific technology.\nDigital trust defines relationships between entities in the digital world. It is the measure of confidence that a trustor has in the trustee’s ability to\nprotect data and privacy of individuals [Pietrzak and\nTakala, 2021]. Digital trust and Trust in Technology\nare closely related to specific properties of a technology or solution (e.g., security, reliability, availability etc.), with digital trust focused on data security and privacy. Whereas these properties are considered objective measures of technology trustworthiness in the literature [Murtin et al., 2018, Jacovi\net al., 2021, Garry and Harwood, 2019, Rychkova and\nGhriba, 2023,Gharib et al., 2020], they are not always\ngood predictors of technology acceptance: users (social actors) do not always have the technical expertise\nto objectively evaluate the complex technical properties and ground their decisions on their subjective beliefs. This work examines the complex nature of trust\nin blockchain and its effect on the BCT adoption. Table 1 provides an overview of the trust conceptualizations and factors.\n2.3 Trust and Blockchain Adoption\nBlockchain technology has emerged as a potential solution to cope with mistrust in traditional (centralized) institutions and online intermediaries in general [De Filippi et al., 2020]. Blockchain can be defined as a distributed database that allows its users to\ntransact in a public and pseudonymous setup without the reliance on an intermediary or central authority [Glaser, 2017]. Despite its popularity and efficiency, blockchain technology experiences challenges\nof user adoption. Depending on the industry sector and the use case, privacy, security, scalability, interoperability, performance are considered the main\nchallenges [Konstantinidis et al., 2018, Casino et al.,\n2019,Belotti et al., 2019,Marengo and Pagano, 2023].\nTheoretical models of acceptance are used by researchers to reason about the factors of blockchain\nTable 1: The types of Trust\nRef. theory\nSocial trust [Mayer et al., 1995]\nTrust in technology [Mcknight et al., 2011]\nDigital trust [Pietrzak and Takala, 2021]\nadoption in a structured way. The study by Shin\n[Shin, 2019] shows the impact of Trust, Security and\nPrivacy factors on the blockchain-based-solution acceptance (behavioral intention to use). Following\nthis work, the studies in [Jena, 2022, Kumar et al.,\n2022, Alazab et al., 2021] extend the TAM/UTAUT\nmodels with Trust, Perceived Security and Perceived\nPrivacy. The authors of [AlShamsi et al., 2022] are\nexamining the technology acceptance models, theories and influential factors in BC adoption. Among\nthe 11 factors identified, trust is the most common factor affecting the BCT adoption. The authors of [Taufiq et al., 2018] study the influence factors of BCT\nadoption taking the example of the payments system\nin Indonesia banking industry. They highlight the importance of non-technical factors, such as attitude,\nsubjective norm, cognitive style of the user. In [AlAshmori et al., 2022], the authors identify the 18 factors of BCT adoption. These factors are consistent\nwith TAM / UTAUT theoretical frameworks and include trust. The systematic literature review [Taherdoost, 2022] confirms that trust is an important factor\nof BCT adoption, in particular for supply chain industry. The authors of [Marengo and Pagano, 2023]\nconduct a systematic literature review to examine the\nfactors of adoption of BCT across different countries\nand industries and identify trust as a recurring factor\nfor supply chain, real estate, and banking. The study\npresented in [Norbu et al., 2024] focuses on trust as\na primary driver for BCT adoption in digital payment\nsystems.\nWhile existing literature confirms that trust is an\nimportant factor in BCT acceptance, the nature of\ntrust and its specific role are not discussed in detail.\nIn this work, we investigate the role of trust within\nthe proposed theoretical models (i.e., as a predictor,\nmediator, or moderator). We also analyze the conceptualization of trust used in these models. By analyzing how trust is defined and measured, we aim to determine whether the researchers refer to social trust,\ndigital trust, or trust in technology in their theoretical\nframeworks.\nUnderstanding the nature of trust and its impact on\nand their factors\nFactors\nAbility, benevolence, integrity,\npropensity to trust, perceived risk\nPerformance, functionality,\nreliability, propensity to trust,\ninstitution-based trust\nData privacy, data security, confidentiality\nBCT acceptance is crucial for addressing the adoption\nprocess. We use trust conceptualization from social\nsciences [Gambetta et al., 2000, Mayer et al., 1995]\nand from technology [Mcknight et al., 2011, Pietrzak\nand Takala, 2021], addressing this concept from the\nbroader perspective.\n3 Methodology\nIn this study, we present a critical review of theoretical models predicting BCT acceptance. Unlike a systematic literature review, our approach offers a more\nreflective analysis, emphasizing judgment and argument over exhaustiveness. We particularly focus on\nthe interpretation and critical evaluation of theoretical acceptance models supported by empirical data,\naiming to uncover trends, patterns, and conflicts in\nthe literature. Although formal guidelines for critical\nreviews are not universally established, we adapted\nthe systematic literature review principles by Kitchenham and Charter [Kitchenham et al., 1995], following\nthese steps: defining research questions, identifying\nrelevant studies, extracting data, conducting critical\nanalysis and evaluation, and reporting the results.\n3.1 Research questions\nWe formulate the following research questions for our\nreview:\nRQ1 : What are the most commonly used theoretical\nmodels to explain blockchain acceptance?\nRQ2: How is the Trust construct addressed in the\nstudies?\nRQ2.1 How is Trust defined in these studies?\nRQ2.2 What trust indicators are used?\nRQ3: What types of Trust are associated with BCT\nacceptance?\nRQ4: How does Trust influence BCT acceptance?\n3.2 Study selection\nFor our critical review, we selected the studies that\nexplicitly integrate trust in their proposed theoretical models of BCT acceptance. The flow diagram\nadopted from PRISMA (Preferred Reporting Items\nfor Systematic Reviews and Meta-Analyses) presents\nan overview of the source selection process in Fig. 2.\nA literature search was conducted using the Scopus\ndatabase (scopus.com). We used the following keywords for our search: blockchain, trust, acceptance\nOR adoption, model OR framework. To improve relevance, these terms have to appear in the title, abstract\nor key words of the retrieved studies.\nAccording to dimensions.ai, the interest in BCT\nadoption reflected by the number of publications on\nthis topic increases exponentially since 2019. Therefore, for our study, we chose articles published from\nJanuary 2019 on. We limit the publication year by\n2023 to obtain a consistent set of publications that will\nnot be affected by more recent apparitions. The resulting query is illustrated in Fig. 2. 218 sources have\nbeen automatically identified in the Scopus database.\nAfter removing non-primary sources we kept 175\nrecords for screening.\nWe proceeded by screening the metadata of identified records and eliminated irrelevant studies based\non the following exclusion criteria:\nEC1: The study does not formalize a theoretical\nacceptance model.\nEC2: The study does not provide an empirical validation of the proposed model.\nEC3: The study does not consider trust among the\nfactors of acceptance or adoption.\nWe kept 57 records for the full text assessment.\nWe proceeded with the full text reading and kept 18\nrelevant studies for the analysis. We conducted backward and forward citation analysis of the eligible publications from the previous step (a so-called “snowballing” technique) and identified 14 records which\nwere re-injected into the process. Three studies have\nbeen eventually added to our data set. The final set of\n21 studies was used for the analysis.\n3.3 Data Extraction\nWe examined each selected study, extracting data on:\n• The theoretical BCT acceptance model proposed\nby the study: the reference acceptance model\n(e.g., TAM); the factors affecting BCT acceptance\naccording to the model;\n• Conceptualization and measurement of trust in the\nstudy: the reference definition from the literature;\nthe indicators (i.e., the questions on trust) defined\nby the study;\n• The role attributed to trust within the proposed acceptance model: the hypothesis related to trust;\nthe results of empirical validation of these hypothesis.\nThe critical analysis of the extracted data is presented\nin the next section.\n4 Results\nThrough the analysis of the extracted data, we addressed our research questions and obtained the following results:\n4.1 What are the most commonly used\ntheoretical models to explain\nblockchain acceptance?\nThe examined studies adapted and extended various\ntheoretical acceptance models to reason about BCT.\nTAM [Davis, 1989] is the most commonly used theoretical model that was extended in 14 studies out\nof 21. Six studies use UTAUT [Venkatesh et al.,\n2003] as their ground model. Other models used include Technology-Organization-Environment (TOE)\nand Theory of Planned Behavior (TPB). While most\nof the studies use a single theoretical model, studies in [Alazab et al., 2021, Kamble et al., 2019, Ullah et al., 2021] are grounded on several theoretical\nmodels. Table 2 provides the summary.\n4.2 How is the Trust construct\naddressed in the studies?\n4.2.1 How is Trust defined in the studies?\nTo address this research question, we analyze the explicit definitions of trust presented in the text as well\nas the indicators used in the studies to measure Trust\nas an independent variable.\n10 studies out of 21 provide the explicit definitions in the text. Seven studies define trust as a social construct grounded on willingness of an individual to take risk or to be vulnerable to the actions of\nanother party [Queiroz et al., 2021, Jena, 2022, Khazaei, 2020, Hannoun et al., 2021], or on confidence in\nthe ability and integrity of the other party [Yu et al.,\n2021, Gil-Cordero et al., 2020, Albayati et al., 2020],\nwhich is aligned with the definition from [Gambetta\nFigure 2: The PRISMA flowchart summarizing\nTable 2: Theoretical models\nModel Studies Ref\nTAM 14 [Kumar et al., 2022, Saputra\nLi, 2021, Rijanto, 2021, Palos-Sanchez\n2021, Gil-Cordero et al., 2020,\net al., 2019, Ullah et al., 2021,\nUTAUT 6 [Chang et al., 2022,Jena, 2022,Queiroz\n2021, Hannoun et al., 2021]\nOther 4 [Kamble et al., 2019, Ullah\n2021]\net al., 2000]. Study in [Chang et al., 2022] defines trust in association with transparency as ‘the degree to which one believes that data provided with\nblockchain technology and services is error-free, safe,\nand transacted transparently.’ Calculation-based trust\ngrounded in rational evaluation and the expectation\nof benefits or costs is used in two studies [Chittipaka\net al., 2022, Liu and Ye, 2021]. Other studies do not\nprovide an explicit definition of trust.\n4.2.2 What trust indicators are used in the\nstudies?\nTrust construct is a latent variable that cannot be directly observed. It is measured in the studies via\ntrust antecedents and trust-related indicators used in\na survey or questionnaire. These indicators and antecedents are defined by the conceptualization of trust\n(e.g., social trust, trust in technology or digital trust)\nas presented in the Table 1.\nThe nine studies use trustworthiness as a direct indicator of trust: [Queiroz et al., 2021, Kumar et al.,\n2022, Yu et al., 2021, Gil-Cordero et al., 2020, Saputra and Darma, 2022, Albayati et al., 2020, Dirsehan, 2020, Liu and Ye, 2021, Chang et al., 2022].\nthe literature selection process\nadapted for BCT acceptance\nand Darma, 2022, Sciarelli et al., 2021, Gao and\net al., 2021, Yu et al., 2021, Liu and Ye,\nAlbayati et al., 2020, Dirsehan, 2020, Kamble\nShrestha et al., 2021]\net al., 2021,Khazaei, 2020,Alazab et al.,\net al., 2021, Chittipaka et al., 2022, Alazab et al.,\nIn these surveys, the perception of blockchain trustworthiness is assessed through affirmative statements\nsuch as ”I believe that blockchain is trustworthy”\n[Queiroz et al., 2021] and ”BCT is trustworthy” [Yu\net al., 2021, Chang et al., 2022]. Respondents express\ntheir agreement with these statements using the Likert\nscale, relying on their own interpretation of ”trustworthiness.”\nStudies in [Kumar et al., 2022, Saputra and\nDarma, 2022, Albayati et al., 2020] focus on promise\nand commitment of BCT. This indicator evaluates\nwhether users believe that the technology prioritizes their best interests. Studies in [Queiroz et al.,\n2021, Gil-Cordero et al., 2020] include questions on\ntrust in the legal structures surrounding blockchain\nand cryptocurrencies referred to as institutional trust\nin [Mcknight et al., 2011]. ”I am confident that the\nlegal and technological structures protect me from\nproblems with cryptocurrencies” [Gil-Cordero et al.,\n2020] underscore the importance of the institutional\ntrust. Studies in [Liu and Ye, 2021, Kamble et al.,\n2019] emphasize trust built upon experiences, their\nquestions suggest that familiarity and previous interactions with the system or service play a crucial\nrole in forming trust. Trust is measured via safety\nand security in [Palos-Sanchez et al., 2021, Yu et al.,\n2021,Chang et al., 2022,Chittipaka et al., 2022,Dirsehan, 2020]. Other indicators mentioned include: reliability, transparency, integrity, honesty, fairness, confidence, capability, skills.\nWe analyze the indicators implying user’s trust\nin other model constructs. For example, in [PalosSanchez et al., 2021], the respondents are requested\nto express their belief in BCT security as its ’ability\nto act in the user’s interest’. The survey items related\nto this indicator include: IT Devices using blockchain\ntechnology would be safe from external threats, such\nas hacking; IT Devices using blockchain technology\nwould be safe from the risk of data forgery and alteration; IT Devices using blockchain technology would\nsecure personal information. This is consistent with\nthe definition of trust in technology [Mcknight et al.,\n2011]. In [Gil-Cordero et al., 2020], the following\ntrust indicator is used: ’I have confidence in the system’. Similarly, the indicators of privacy and information quality are formulated using an implicit notion of trust in [Gil-Cordero et al., 2020, Ullah et al.,\n2021, Liu and Ye, 2021, Palos-Sanchez et al., 2021].\n4.3 What types of Trust are associated\nwith BCT acceptance?\nBy analyzing the definitions of trust presented in the\ntext, along with the trust indicators and indicators implying user trust in other model constructs, we classified the Trust constructs used in the studies into Social\ntrust, Trust in technology, and Digital trust. Table 3\nprovides a summary of this classification.\nSocial trust conceptualization is used in the nine\nstudies. These studies either use explicit trust definition consistent with [Gambetta et al., 2000, Mayer\net al., 1995] or they define their trust indicators using\nthe factors of social trust from Table 1. For example,\nin [Kumar et al., 2022], respondents are asked about\ntheir perception of BCT’s commitment to their best\ninterests: ’It gives an impression of promise and commitment; It keeps my interest in consideration’ - property consistent with the predictors of social trust. The\nfactors of Trust in technology are found in the nine\nstudies, and the factors of digital trust - in the seven\nstudies. For example, the indicator ’My firm’s data in\nthe cloud might be utilized by an outsider without our\nconsent’ is used in [Chittipaka et al., 2022], the indicator ’Data in blockchain technology would be saved\nsecurely.’ is used in [Chang et al., 2022] to measure\ntrust. These indicators associate trust with reliability\nand security of data on the blockchain.\nSeveral studies define trust indicators based on\nmultiple conceptualizations. For example, in [Kumar\net al., 2022], the definition of trust provided in the text\nis implicit, co-notated with privacy and security (consistent with digital trust), whereas the trust indicators\nare defined using the notion of commitment and ’consideration of the user’s best interests’ (aka benevolence), consistent with social trust.\nOur analysis shows that social trust plays important role in defining trust in BCT. In several studies, social trust is identified with a relationship built\nbetween the users (trustors) and the BCT providers\n(trustees) - social entities. For example, in [Albayati\net al., 2020], the trust indicator ’I believe the service\nproviders (both cryptocurrency and blockchain) keep\nmy best interests in mind.’ is used. In the other studies, the technology itself is identified with the qualities intrinsic for a social entity: in [Kumar et al.,\n2022, Saputra and Darma, 2022], trust indicators include the affirmations ’It [technology] keeps my interest in consideration’ and ’The services provided by\nMy T Wallet keep my best interests in mind.’\nTrust in technology is a fundamental conceptualization of trust in BCT as well: it provides the user\nwith objective metrics to assess trustworthiness, such\nas reliability, performance, functionality.\nDigital trust conceptualization focuses on more\nspecific technical properties related to data security\nand privacy in the digital world. In the examined\nacceptance models, these properties are often integrated into the Perceived Usefulness construct in\nTAM, or Perceived Security and Perceived Privacy\nconstructs, specifically defined in the models [Jena,\n2022, Shrestha et al., 2021, Kumar et al., 2022].\n4.4 How does Trust influence BCT\nacceptance?\nWe examined the hypotheses made in the studies\nabout the role of trust on BCT acceptance and validation of these hypotheses.\n4.4.1 Trust as a Predictor of BCT acceptance:\nPredictors are independent variables that directly influence the dependent variable.\nTrust → VAR X\nTrust has a direct effect on behavioral intention (BI)\nto use BCT according to 10 studies. In [Chittipaka\net al., 2022], trust is positioned as a predictor of Adoption, which stands for actual use and follows the BI.\nTrust is defined as a predictor of Attitude towards use\nin [Kumar et al., 2022, Albayati et al., 2020]. While\nattitude towards use reflects how positively or negatively an individual feels about using a technology,\nTable 3: Trust affecting\nStudy reference\nSocial trust [Gil-Cordero et al., 2020] [Jena,\n2021] [Saputra and Darma, 2022]\nnoun et al., 2021] [Albayati et\nTrust in technology [Gil-Cordero et al., 2020] [Jena,\net al., 2021] [Yu et al., 2021]\n2020] [Queiroz et al., 2021]\nDigital trust [Kumar et al., 2022] [Chittipaka\n2022] [Liu and Ye, 2021] [Palos-Sanchez\nN/A [Sciarelli et al., 2021] [Rijanto,\nbehavioral intention reflects the individual’s readiness\nand plan to use that technology. They are often used\ninterchangeably. In some theories, however, the attitude precedes BI. Therefore, we conclude that 13\nstudies recognize trust as having direct effect on acceptance.\nSeven studies show significant effect of trust on\nother constructs that affect acceptance: Perceived\nusefulness, Perceived ease of use, Performance expectancy, Perceived privacy. Table 4 provides a summary of our results.\n4.4.2 Trust as a Mediator:\nMediators are variables that explain the mechanism\nthrough which predictors influence the dependent\nvariable. They act as intermediaries in the causal\nchain, helping to clarify how or why a certain effect\noccurs.\nVAR X → Trust → VAR Y\nAccording to [Jena, 2022], Trust acts as a mediator\nbetween Facilitating conditions and Performance expectancy and BI (behavioral intention to use BCT).\nThis means that facilitating conditions and performance expectancy enhance users’ trust, which in turn\naffects their intention to use BCT. According to [GilCordero et al., 2020], trust mediates the effect of eWorm (the electronic word of mouth), web quality,\nand Perceived risk on BI. Here e-Wom refers to ’any\npositive or negative statement made by potential, actual or former customers about a product or company, which is made available to a multitude of people\nand institutions through the Internet’. Similarly, Perceived privacy [Shrestha et al., 2021] has a positive\neffect on Trust, which in turn affects the intention to\nuse BCT. Trust mediates the effect of Perceived Security [Kumar et al., 2022, Shrestha et al., 2021] and\nPerceived privacy [Kumar et al., 2022] on Attitude towards use BCT. Table 5 presents the summary of the\nBCT acceptance\n2022] [Alazab et al., 2021] [Queiroz et al.,\n[Kumar et al., 2022] [Dirsehan, 2020] [Hanal., 2020]\n2022] [Alazab et al., 2021] [Palos-Sanchez\n[Liu and Ye, 2021] [Ullah et al., 2021] [Dirsehan,\net al., 2022] [Gao and Li, 2021] [Chang et al.,\net al., 2021] [Kamble et al., 2019]\n2021] [Shrestha et al., 2021]\nresults.\n4.4.3 Trust as a Moderator:\nModerators are variables that affect the strength or\ndirection of the relationship between predictors and\ndependent variable. They provide insights into when\nor under what conditions certain effects occur. Some\nvariables function both as predictors and mediators,\ndepending on specific relationships being examined.\nTrust → (VAR X ×VAR Y )\nTrust is defined as enhancing the effects of other\nvariables on BI: Performance expectancy, Effort expectancy, Social influence, Facilitating conditions\n[Alazab et al., 2021], Subjective knowledge [Dirsehan, 2020]. According to [Gil-Cordero et al., 2020],\nTrust moderates the effect of e-Worm, web quality\nand Perceived risk on the user behavior. According\nto [Shrestha et al., 2021], Trust increases the positive impact of Privacy on the Attitude towards use\nof BCT. According to [Palos-Sanchez et al., 2021],\nTrust moderates the effect of the perceived risk on\nperceived easiness of use. This means that the level\nof trust a user has can influence how perceived risk\nimpacts their perception of the technology’s ease of\nuse. Specifically, higher trust can mitigate the negative effects of perceived risk, making the technology\nseem easier to use despite potential risks. Conversely,\nlower trust can amplify the negative impact of perceived risk on perceived ease of use.\n5 Discussion\nAnalyzing the theoretical models of BCT acceptance,\nwe found that social trust is widely used when reasoning about, defining, or measuring trust. Social trust\nfactors appear as indicators in 9 out of 21 studies.\nTable 4: Trust as a direct Predictor\nVAR X (dependent) Study\nBehavioral intention (BI) [Jena, 2022] [Gao and Li,\nand Ye, 2021] [Queiroz et\nand Darma, 2022] [Gil-Cordero\nAdoption [Chittipaka et al., 2022]\nAttitude towards use [Kumar et al., 2022] [Albayati\nPerceived usefulness [Liu and Ye, 2021] [Dirsehan,\nPerceived ease of use [Albayati et al., 2020] [Saputra\nPerformance expectancy [Chang et al., 2022]\nPerceived privacy [Palos-Sanchez et al., 2021]\nTable 5: Trust as a Mediator and Moderator\nRole of Trust VAR X (independent)\nFacilitating conditions, Performance\nMediator expectancy, eWorm, Web quality,\nPerceived risk, Privacy\nPerceived Security, Perceived Privacy\nPerformance expectancy, Effort expectency, Social influence, FacilitatModerator ing conditions, Subjective Knowledge\neWorm, Web quality, Perceived risk\nPerceived privacy\nPerceived risk\nAbilities of BCT to eliminate the need for a central\nauthority, to ensure transparency and immutability of\ntransactions also frequently used as trust indicators.\nWhile many studies focus on technical properties of\nblockchain and co-notate the acceptance of BCT with\nperceived security and privacy [Jena, 2022, Kumar\net al., 2022, Alazab et al., 2021], they also highlight the important role of benevolence of the service provider or the technology itself [Kumar et al.,\n2022, Albayati et al., 2020, Dirsehan, 2020]. Integrity\nand honesty of BCT is also used as a trust indicator [Kumar et al., 2022, Liu and Ye, 2021, Albayati\net al., 2020].\nTrust in technology, which focuses on technological properties and excludes the factor of benevolence\n(as technology is considered not to be able to act\nor not in users’ best interests), is used in nine stud-\nof the BCT Acceptance: Summary\n2021] [Yu et al., 2021] [Hannoun et al., 2021] [Liu\nal., 2021] [Khazaei, 2020] [Dirsehan, 2020] [Saputra\net al., 2020]\net al., 2020]\n2020]\nand Darma, 2022] [Palos-Sanchez et al., 2021]\nof the BCT Acceptance: Summary\nVAR Y (depen- Study\ndent)\nBI [Jena, 2022] [Gil-Cordero\net al., 2020] [Shrestha et al.,\n2021]\nAttitude [Kumar et al., 2022]\n[Shrestha et al., 2021]\nBI [Alazab et al., 2021] [Dirsehan, 2020]\nUser Behavior [Gil-Cordero et al., 2020]\nAttitude [Shrestha et al., 2021]\nPerceived ease [Palos-Sanchez et al., 2021]\nof use\nies. This form of trust serves as a foundational element. The results suggest that before considering\nblockchain adoption, users must first establish trust in\nthe technology itself. However, once this basic trust\nis formed, the focus appears to shift toward the digital\nfunctionalities and attributes of blockchain, referred\nto as Digital Trust.\nDigital trust, similar to trust in technology but centered on data privacy, security and confidentiality, is\nused in seven studies. Some studies incorporate multiple trust conceptualizations.\nThese findings suggest that trust is a multifaceted\nconcept in BCT acceptance models, with different dimensions of trust (social, technological, and digital)\nplaying distinct roles. Understanding these dimensions can enhance the precision of acceptance models\nand improve the interventions to boost BCT adoption.\nAnother outcome of our study is the importance\nof considering BCT independently from BCT solution providers. While BCT is often referred to as a\n”trust machine”, which excludes human factors and\nsocial trust, solution providers and software engineers\nremain crucial social actors. Therefore, trust building\nbetween solution providers and users must be examined and taken into account alongside trust in BCT as\na technological entity.\nAccording to 13 studies, trust has a direct impact\non the behavioral intention, attitude towards use or\nadoption of BCT. Several studies highlight the effect\nof trust on perceived usefulness and perceived ease\nof use. Mediating effect of trust explaining the effect\nof other variables on BCT acceptance is found in the\nfour studies, while moderating effect is confirmed in\nthe five studies.\nThere is no general agreement among the studies about mediating or moderating role of trust. For\nexample, according to [Jena, 2022], trust mediates\n(or explains) the effect of Performance expectancy on\nbehavioral intention to use BCT. In other terms, expected performance leads to trust, which, in its turn,\nleads to acceptance. However, according to [Alazab\net al., 2021], trust only moderates this effect, meaning\nthat the impact of performance expectancy on acceptance does not depend on trust. Along these lines,\nin [Kumar et al., 2022], trust mediates the effect of\nperceived privacy on the attitude towards use of BCT,\nwhile in [Shrestha et al., 2021] it is considered as a\nmoderator for this effect.\nThese findings highlight the critical role of trust in\nthe acceptance of BCT, indicating that it, both directly\nand indirectly, shapes users’ attitudes and intentions.\nWe suggest the following research questions that\ncan be added to the research agendas:\n- What are the most effective communication strategies for BCT solution providers to build trust with\ntheir users?\n- How can organizations measure the level of trust in\nBCT among their users accurately?\n- What role does trust in technology provider play in\nenhancing trust in BCT?\n- What are the long-term impacts of early trustbuilding efforts on the sustained adoption of BCT?\n- How does trust in BCT compare to trust in traditional centralized systems?\n- What lessons can be learned from trust-building in\nother emerging technologies that can be applied to\nBCT?\n- How do users’ previous experiences with technology\ninfluence their trust in BCT?\nRelated to Social trust:\n- How do various cultural contexts affect social trust\nin BCT solutions?\n- What actions taken by BCT solution providers are\nmost effective in fostering social trust among users?\n- How does social trust develop over time with continued use of BCT?\nRelated to Trust in technology / Digital trust:\n- What are the most critical technological features that\naffect user trust in BCT?\n- What are the best practices for ensuring data privacy\nand confidentiality in BCT to build digital trust?\nWhile our findings identify gaps and provide directions for future research on technology acceptance,\nthe question ”How can we improve trust in emerging\ntechnologies, including BCT?” remains crucial. To\nincorporate the concept of trust into technology design, a specific type of requirement—trustworthiness\nrequirements—has been defined in the literature\n[Amaral et al., 2020, Kambilo et al., 2023]. Other\nstudies propose adapting current design practices\nby explicitly documenting and tracing user trust\nconcerns and trustworthiness requirements, and by\nclearly justifying the design assumptions [Haley et al.,\n2004, Wang et al., 2016] behind technical decisions\n[Mohammadi, 2019, Rambert and Rychkova, 2024].\nWe believe that understanding the role of trust in\ntechnology acceptance is crucial for developing new\nmethods and approaches that support technology\ntrustworthiness by design.\n6 Conclusion\nIn this paper, we conducted a critical review to analyze the role of trust in the acceptance of blockchain\ntechnology (BCT). The concept of technology adoption is often used interchangeably with acceptance,\nbut it is important to distinguish between the two.\nAdoption refers to the actual use and integration of\ntechnology into regular practice, while acceptance\nrefers to the user’s willingness and intention to use\nthe technology. In the theoretical models, acceptance\nis often identified with behavioral intention to use and\nis closely related to attitudes towards use. This distinction is crucial because a user may accept a technology (intend to use it) without fully adopting it\n(consistently using it in practice). Conversely, a decision to adopt a technology without full acceptance\nby the prospective users can lead to important issues\nsuch as low utilization rates, resistance, and potential\nabandonment of the technology. Understanding both\nconcepts helps in designing better strategies for promoting both initial acceptance and sustained use of\nblockchain technology.\nWe examined 21 scientific publications that de-\nfine theoretical BCT acceptance models, most of\nwhich extend the well-known Technology Acceptance Model (TAM) [Davis, 1989]. Our analysis focused on how trust is conceptualized, measured and\nintegrated into the proposed models, identifying the\ntypes of trust and the roles trust plays in BCT acceptance.\nOur findings provide a nuanced understanding of\ntrust’s multifaceted role, guiding future research to incorporate various dimensions of trust and refine survey instruments for more accurate measurement. This\nadvances both theoretical development and practical\napplication in the field of technology acceptance.",
  "2025-LR": "1 Introduction Generative Pretrained Transformer (GPT) marks a significant breakthrough in the NLP domain, revolutionizing our way to interact with technology and open- ing avenues for applications in numerous industries. With ChatGPT considered the fastest-growing consumer application in history numerous concerns and chal- lenges related to GPT adoption are raised [30]. Technology trustworthiness is one of them [4][2]. Trust is a social construct that emerges from relationships and interactions between individuals or groups. It involves a willingness to rely on others [6, 16] and is influenced by factors such as past experience, reputation, and social norms. Trust in technology is described by a situation in which an individual user or an organization (trustor) is willing to rely on technology (trustee) to accomplish a specific task [18]. In contrast to interpersonal (or social) trust, trustworthiness of technology is mainly identified with its specific technical properties (e.g., se- curity, fault tolerance etc)[18]. However, for the modern technologies, including GPT, technical properties are not sufficient predictors of trust: Users are often unable to objectively reason about technical properties due to complexity of a system or service [7]. Moreover, AI assistants powered by GPT technology \"can understand and communicate using language in a manner that closely resem- bles that of humans\" [30]. They simulate human-like behavior, social interaction, and provoke emotional feedback, comparable to interpersonal relationships. This shows an importance of factors of interpersonal (or social) trust in technology. 2 H. Li et I. Rychkova In this study, we closely examine the trust formation process in the context of GPT services and the role of social trust in GPT services acceptance and use. We define a theoretical model for trust in GPT services grounded on the Integrative Model of Organizational Trust [16]. While this model has been ex- tensively applied in organizational settings, there is currently limited explicit application of this model in studies on trust in ChatGPT or Generative AI ser- vices. In particular, we explore how trust and decision to use GPT services de- pend on (a) the factors of interpersonal trust [16] and (b) the socio-demographic characteristics of a user, including education and awareness in AI. We provide an empirical validation of our model, conducting a survey among 124 participants. This study aims to contribute to the existing literature by: - Reaffirming the role of trust as a significant predictor of user engagement with GPT services, aligning with previous research on the influence of trust in tech- nology adoption. - Expanding the conceptualization of trust in GPT services by recognizing the role of its anthropomorphic characteristics in shaping the user-technology rela- tionship. The remainder of this paper is organized as follows: in Section 2, we present our foundational concepts and discuss the related works; in Section 3 we present our theoretical model for trust in GPT services and detail our research method- ology; in Section 4, we discuss the results of our analysis; in Section 5, we present our conclusions. 2 Background and Related Works 2.1 Generative AI and GPT technology Artificial Intelligence (AI) is defined as \"intelligence exhibited by machines, par- ticularly computer systems\" [23]. Generative AI (GenAI) are AI models able to generate content, whether in textual, audio, image or video form. GenAI models use neural network models and usually answer to a ’prompt’ which is the textual input from a user. Transformer model in particular, developed by Google in 2017 [28], allowed for shorter training as it is a model with no recurrence and worked by transforming text into numerical values (tokens). Generative Pretrained Transformer (GPT) is a deep learning model based on the Transformer architecture, designed for natural language processing tasks. GPT models are pretrained on a large corpus of text data and fine-tuned for spe- cific tasks. They can understand and generate contextually relevant text, which makes them widely used in AI assistants, chatbots, and other NLP applications [9]. The first GPT model was introduced in 2018 by OpenAI. Their product ’ChatGPT’ is considered a breakthrough in the field. ChatGPT takes the form a chatbot, in which the user is given a field to input text, and from which the service will answer in a human natural language. Other products such as Microsoft Bing Chat, Github Copilot, DeepSeek etc., while based on different implementations and architectures, use GPT technology as their backbone. We refer to the products of this family as ’GPT services’ in this study. Understanding Trust Formation in GPT Services: An Empirical Study 3 While popularity of GPT services grows, is still much to be explored about their capabilities and limitations [30]. Trustworthiness is one of the main chal- lenges in GPT acceptance [4]. \"Low trust in a highly capable technology would be a huge productivity loss, whereas high trust in a less performant technology can lead to over-reliance and misuse of a technology\" [8]. 2.2 Social Trust and Trust in Technology In the research literature on trust, the act of trust is often represented as a relationship between a subject (the trustor) and an object of trust (the trustee) [32][22]. It is characterized by the trustor’s willingness to be vulnerable, to rely upon trustee in some situation where risk is involved. Outcome of trust is defined as an actual engagement or interaction between trustor and trustee. The Integrative Model of Organization Trust, developed by Mayer, Davis, and Schoorman [16], provides a theoretical framework for understanding how trust between organizations and/or individuals is formed. It is composed of three factors of perceived trustworthiness, which contribute to interpersonal trust: – Perceived Ability: The skills, competencies, and characteristics that enable a trustee to have influence within some specific domain. – Perceived Benevolence: The extent to which a trustee is believed to want to do good to the trustor, aside from an egocentric profit motive. – Perceived Integrity: The perception that the trustee adheres to a set of prin- ciples that the trustor finds acceptable. Those factors are directly affecting trust and are moderated by the trustor’s propensity, that is, the general willingness of the trustor to trust others which can be explained by the personality, experience, etc, and by the perceived risk that comes from the nature of this interaction [16]. The authors of [27] argue that the dimensions of trustworthiness proposed by Mayer [16] are poorly suited for studying trust relationships between users and IT artifacts, since they are defined to fit the human character traits and human decision making. For example, to assess a ’perceived benevolence’ of an IT artifact one has to assume that this artifact is able to actively decide weather to act in the interest of the user (trustor) or not. As an alternative to social trust, Trust in specific technology is widely ad- dressed in the literature [8, 19, 10, 7, 27, 25]. McKnight et al. [18] provides a framework for understanding how trust in technology is formed and its effects on technology usage. The authors put forward performance, functionality and reliability as the factors of trust in specific technology. Institution-based trust, including situational normality and structural assurance, exerts a mediated pos- itive effect on post-adoption technology use according to [18]. According to [30], GPT services exhibit strong resemblance to human be- havior and ability to simulate human decision making and character traits. This makes us reconsider the question ’Do people rely on the same dimensions of trustworthiness when deciding whether or not to trust other people compared to deciding whether or not to trust an IT artifact?’ debated in [27]. 4 H. Li et I. Rychkova 2.3 Theoretical Models of Technology Acceptance In empirical research, theoretical models specify a set of constructs and the rela- tionships between them that explain a phenomenon of interest. The constructs can play a role of predictors, mediators or moderators for the examined phenom- ena. Predictors are independent variables that directly influence the dependent variable. Mediators are variables that explain the mechanism through which predictors influence the dependent variable. They act as intermediaries in the causal chain, helping to clarify how or why a certain effect occurs. Moderators are variables that affect the strength or direction of the relationship between pre- dictors and the dependent variable. They provide insights into when or under what conditions certain effects occur. Theoretical models such as the Technology Acceptance Model (TAM) [5] or its extension, the Unified Theory of Acceptance and Use of Technology (UTAUT) [29] are often applied within the context of information systems to understand predictors of human behavior toward potential acceptance or rejection of the technology. TAM defines two predictors of acceptance (or acceptance factors): Perceived usefulness is the degree to which a person believes that using a particular system would enhance his or her job performance. Perceived ease of use is the degree to which a person believes that using a particular system would be free of effort. The Unified Theory of Acceptance and Use of Technology (UTAUT) was created in 2004 [29]. Built upon the TAM, it reviewed multiple TAM models and identified the four factors of acceptance: performance expectancy, effort expectancy, social influence and facilitating conditions. Whereas the first two factors are drawn upon the original TAM, the last two elaborate on the role of social relationships and environment in acceptance. Social Influence is defined as the perceived importance for an individual to use new technology by other people. Facilitating conditions is defined as the belief of an individual that \"an organization and technical infrastructure exists to support the use of a system [technology]\" [29]. Those four factors are influenced by four moderators: age, gen- der, experience, and voluntariness of use, which result in possible gaps between the intentions to use a technology and the actual use of the technology. Acceptance predictors from TAM and UTAUT show the importance of com- bining technical factors and social factors, related to user’s personality and the context of use. While trust is placed among the second-order factors of accep- tance, the authors agree that without trust, users are unlikely to engage with a technology or adopt it for their needs [5]. 2.4 Trust and Acceptance of GPT services in the Literature Factors of rapid acceptance and adoption of GPT services and ChatGPT in par- ticular draw a lot of attention in the research community. The authors of [13] and [26] use extended TAM to examine factors of ChatGPT adoption. In [13], the study is conducted among 352 students from 12 higher education institutions. The results highlight the response quality and user-friendliness of ChatGPT as main factors of its adoption. In [26], the study is conducted among Chinese uni- versity students. The findings highlight the importance of trust in the adoption Understanding Trust Formation in GPT Services: An Empirical Study 5 process of ChatGPT: in particular, the study shows that perceived trust mod- erates the relationship between awareness about ChatGPT and perceived ease of use, usefulness - main concepts of TAM. The study reported in [3] focuses on user trust and its influence on the intention and actual use of ChatGPT (no reference to a particular theoretical model is provided). The survey reveals that trust has a significant direct effect on both the intention to use and actual use of ChatGPT. The authors of [1] and [14] explore factors influencing acceptance and use of ChatGPT using an extended UTAUT model. The work in [1] shows that performance expectancy, effort expectancy, hedonic motivation, facilitating conditions, and habit positively impact the behavioral intention to use Chat- GPT. Trust moderates the relationship between behavioral intention and actual use behavior. The findings presented in [14] reveal that relative risk perception and emotional factors play significant roles in predicting behavioral intentions toward ChatGPT. These studies indicate the significant role trust plays in acceptance of Chat- GPT. While using the theoretical acceptance models (i.e., TAM, UTAUT) as underlying theories, these works do not specifically address trust formation pro- cess. Moreover, in these studies, trust is often co-notated with technical aspects (e.g., privacy and security) of the considered technology, ignoring the social and emotional aspects. Earlier works examine trust in AI using social conceptualization of trust: The authors of [10] examine the nature of trust in AI and discuss a model inspired by interpersonal trust. The authors associate the AI trustworthiness with the AI model correctness and commitment to some contract (contractual trust). In [8], the authors propose a theoretical framework and discuss the determinants of human trust in AI. This study identifies tangibility, transparency, reliability and immediacy of AI technology with formation of cognitive trust (based on rational thinking) in users, whereas the AI’s anthropomorphism plays an important role in emotional trust (based on affection). This study goes one step further from the conventional research exploring ChatGPT acceptance factors and is designed to investigate the process of trust formation and the role of social predictors of trust in GPT services. We propose a theoretical model for trust in GPT services grounded upon Mayer et al. [16]. We extend the existing body of knowledge explicitly linking trust in ChatGPT to the established social factors of trust: Ability, Benevolence and Integrity. 3 Methodology 3.1 Theoretical Model and Hypotheses Development Table 1 summarizes the theoretical models of trust and technology acceptance discussed in the previous section. We draw upon [16] to define the trust formation process. Figure 1 presents the theoretical model for this study. Social factors of trustworthiness as predictors of Trust in GPT ser- vices. Perceived Ability, Benevolence and Integrity are the main factors of trust according to [16]. In our context, we specify these construct integrating the trust 6 H. Li et I. Rychkova Table 1. Summary on the theoretical models of trust and acceptance Relationship Subject Object Predictors/ Media- Moderators Outcome tors Social Trust Organization Organization Ability, benevolence, in- Propensity to Interaction, [16] / Individual / Individual tegrity trust, perceived collabora- risk tion Trust in Organization Technology Performance, functional- Propensity to Acceptance, Techn. [18] / Individual (system, ity, reliability trust, institution- use service) based trust Techn. accep- Organization Technology Performance expectancy, Age, experience, Acceptance, tance [5, 29] / Individual (system, effort expectancy, social voluntariness of use service) influence facilitating use conditions Fig. 1. Theoretical model of Trust in GPT services based on [16] indicators from our earlier works [25] and from the theories of technology ac- ceptance from Table 1. Perceived Ability construct is defined as competence (including technical functionality, performance, usability [31]) of a trustee (GPT services in our case) to have influence within some specific domain. It is closely re- lated to perceived usefulness in TAM [5] and performance expectancy in UTAUT [29]. Perceived Benevolence is defined as the extent to which GPT services are believed to act in the interest of trustor (user), for example, by protecting her personal data or ensuring exactitude, fairness and objectivity of provided an- swers. In digital world, benevolence is closely associated with perceived privacy and security of data [31]. Perceived Integrity is defined as the perception that GPT services adhere to a set of principles (ethical, legal or other) that the trustor finds acceptable, for example, by ensuring verifiable and tracable answers. This construct can be also associated with credibility and transparency in technology [20][12]. H1: Perceived Ability has a significant effect on Trust in GPT services. H2: Perceived Benevolence has a significant effect on Trust in GPT services. H3: Perceived integrity has a significant effect on Trust in GPT services. Understanding Trust Formation in GPT Services: An Empirical Study 7 Trust in GPT services and Use of GPT services. Trust is a core variable in our model. We define Trust in GPT services as willingness of user to rely on these services in a situation where a risk or a negative outcome is possible. Risk Taking in relationship [16] defines the actual Use of GPT services in our model. H4: Trust has a significant positive effect on Use of GPT services. The role of technical background in trust formation. The Integrative model of organizational trust defines Propensity to trust as an individual char- acteristic of a trustor which predicts interpersonal Trust and also moderates the impact of PA, PB, PI on Trust. In our model, we consider that the role of propensity to trust can be fulfilled by the trustor’s knowledge about technology: we suggest that the users exposed to technology in general (via their academic background and professional experience) are more likely to rely on a new tech- nology such as GPT services and trust it. Moreover, we suggest that a user with a strong Background in Technology will be more aware about technical function- alities and other characteristics of GPT services constituting Perceived Ability, Benevolence and Integrity. Therefore Background in Technology can moderate the effect of these factors of trust. H5: User’s Background in Technology has a significant effect on Trust in GPT services. H6-H8: User’s Background in Technology has a moderating effect on her Per- ceived Ability, Benevolence, and Integrity of GPT services. The role of specific knowledge in AI in trust formation. We consider that Specific Knowledge in AI can also improve trust in GPT services. H9: Specific Knowledge in AI has a significant effect on Trust in GPT services. 3.2 Measurement Model We use reflective latent constructs to model Perceived Ability, Benevolence and Integrity as well as Trust and Use (see Teble 2). A construct is modeled as latent if it cannot be measured directly. In our study, Perceived Ability is associated with perceived functionality, effi- ciency [17], usefulness [5], performance expectancy and effort expectancy [29]. It is measured as a latent variable, using 5 items PA1-PA5. Perceived Benevolence - ’willingness to do good’ according to [16] - is associated with confidentiality, assertion that GPT services will not share or disclose user personal data or chat details. It is also associated with objectivity (non bias) of responses [25]. Per- ceived Integrity is associated with transparency and traceability in our model. Each of these constructs is measured with 4 indicators PB1-PB4, PI1-PI4 (see Table 2). Dependent variables Trust and Use are also modeled as latent variables. We measure Trust by suggesting respondents a specific situation and asking wether they would trust GPT Service in such situation (T1-T4). The indicator T5 is used to measure how important for the respondents is the fact that the content is produced by a human (it is reverse coded). To measure Use, we define the items that are not Likert-coded. In U1, the respondents are invited to checkbox 8 H. Li et I. Rychkova GPT services they know/use in the list. In U2, U3, we measure frequency of use for leisure and work as ordinal variables. Each item is measured using a 7 point Likert scale. Background in Technology and Specific Knowledge in AI are calculated from the categorical variables in socio-demographic data (see Table 3). For example, Background in Technology is calculated as: Degree in Tech. x Education. 3.3 Data Collection and Sample Characteristics The questionnaire has been designed iteratively and tested on a small conve- nience sample for a feedback. Final version of the questionnaire has been dis- tributed among the students (bachelor and master) of MIAGE master program of Sorbonne University and on the professional LinkedIn network of the authors (non-probabilistic accidental sampling [15]). Data has been collected in 2 pe- riods: between February and April 2024 and between November and December 2024. Collected through Google Forms, the data was exported into CSV, cleaned and coded for further analysis. In total, 124 responses have been collected (N=124). Table 3 summarizes the socio-demographic data on the sample. 3.4 Data Analysis We conducted the data analysis using JASP - an open-source program for sta- tistical analysis supported by the University of Amsterdam [11]. We use Structural Equation Modeling (SEM) method to test the defined hypotheses [24]. Following the recommendations from [21], we choose to use Covariance-Based Structural Equation Modeling (CB-SEM) for the following reasons: the goal of our study is to test and confirm a well-established theory and not a not to explore a new theory; we consider that our model consists predominantly of reflective constructs (where indicators are manifestations of the latent variable). An independent sample t-test with mean comparison verification is conducted to analyze differences in Trust and Use of GPT services between two samples, corresponding to the two evaluation periods in our survey, separated by a nine- month interval. 4 Results In this section, first, we evaluate our measurement model and determine if the collected data represents reliably the theoretical constructs; than we discuss the hypotheses testing results. 4.1 Measurement Model Assessment We asses factor loadings, convergent and discriminant validity of the model. Factor loadings represent the correlation between latent variables (factors) and their measured items. The values of factor loadings are generally expected to be Understanding Trust Formation in GPT Services: An Empirical Study 9 Table 2. Measurement model Construct Item Measurement Perceived PA1 Cover letters written by GPT services are better than if I wrote Ability them myself PA2 The use of GPT services for candidate screening makes the re- cruitment process more efficient PA3 I think GPT services are more efficient than Tabnine (or other similar tool for coding assistance)to generate/review code and save me time PA4 I think GPT services are more convenient than Tabnine (or other similar tool for coding assistance) for coding PA5 Working with GPT services is easier than configuring a query on a searching engine like Google scholar Perceived PB1 I would not mind typing personal details like my location, di- Benevolence etary restriction, allergies in a GPT service PB2 I think GPT Services would give a restaurant recommendation that is less biased than a person’s PB3 GPT services for candidate screening make less biased decisions than a human actor PB4 I would use code, or details about my work/project when prompting GPT services for help Perceived PI1 GPT services are more transparent as a recruiting tool than Integrity human actors PI2 I think the sources GPT services return are traceable PI3 I think GPT algorithm and/or data training is transparent. PI4 The fact that Wikipedia provide the references to the sources is important to me (reverced coding) Trust T1 I would trust a GPT service to find a restaurant for a friend meeting T2 I would trust a GPT service (e.g., ChatGPT) to conduct a lit- erature research and text editing for an essay/article/thesis T3 I would trust a GPT service (e.g., copilot) to help me correcting a bug in the code / getting the code ready for production today T4 I would trust a GPT service (e.g., chatbot) to help me with my CV and/or cover letter. T5 The fact that Wikipedia articles are written by human contrib- utors is important to me (reversed coding) Use U1 What kind of GPT services do you use? U2 How often do you use ChatGPT for work? U3 How often do you use ChatGPT for fun/personal stuff? 10 H. Li et I. Rychkova Table 3. Socio-Demographic Data Variable n (N=124) % Variable n (N=124) % Age 18- 1 1% Education High school 14 11% 18-25 96 77% Bachelor 49 40% 26-35 14 11% Master 48 39% 36-50 8 6% PhD 10 8% 51+ 5 4% Other 3 2% Degree Yes 113 91% Knowledge None 6 5% in Tech. No 11 9% in AI Some 39 31% Fairly good 26 21% Expert 53 43% ≥ 0.7 - for strong loading (good representation of the factor) and 0.4 - 0.7 for acceptable loading. For our model, all latent variables show moderate to strong loading, with the lowest value for PI2 = 0.404. All factor loadings are statistically significant with p-value <0.001. We use Cronbach’s Alpha to asses model reliability (≥ 0.7 for good reliabil- ity). The overall reliability α = 0.828 shows that the full set of items is reliable even though some individual constructs are weak (e.g., Trust: α = 0.296). Discriminant validity measures pairwise how each construct is distinct from other constructs in the model. We use Heterotrait-Monotrait (HTMT) ratio to asses discriminant validity in our model. Most construct pairs have HTMT <0.85 (validity threshold), indicating that they are sufficiently distinct. However, Benev- olence and Integrity (HTMT = 0.872) indicates higher correlation between the two factors. We conclude, that convergent validity is supported in the model as the fac- tor loadings are high, indicating that indicators are strongly related to their respective latent constructs. Discriminant validity is partially supported. The high correlation between Benevolence, and Integrity suggests some overlap. 4.2 Path Estimates and Hypotheses Testing A significant path coefficient (β and p-value <0.05 in Table 4) is one of the primary measures to evaluate the causal relationship in our structural model. Figure 2 summarises the hypotheses testing results. The findings of this study provide empirical support for the hypotheses H1-H9 as follows: H1: Perceived Ability shows a small positive effect on Trust in GPT services (β = 0.009); however, this relationship is not statistically significant (p > 0.05). This suggests that, based on the current data, there is no strong evidence that users’ perception of the system’s capability directly influences their trust. H2: Perceived Benevolence has a significant positive effect on Trust in GPT services (β = 0.578, p<0.001). This finding suggests that potential subjectivity (bias) and confidentiality of user interactions with GPT services represent an important concern for the users, inhibiting trust. Understanding Trust Formation in GPT Services: An Empirical Study 11 Table 4. Regression coefficients Outcome | Predictor | Std.Estimate β | Std.Err | p(<0.05) Trust Ability 0.009 0.255 0.925 Benevolence 0.578 0.226 < .001 Integrity -0.140 0.215 0.444 Spec.Kn.AI 0.378 0.081 < .001 Bgr. In Tech. 0.078 0.088 0.349 INT_A_TBgr 0.393 0.252 < .001 INT_B_TBgr -0.210 0.227 0.032 INT_I_TBgr 0.105 0.207 0.566 Use Trust 0.875 0.067 < .001 Fig. 2. Structural model: Hypotheses testing H3: This study does not confirm a statistically significant effect of Perceived Integrity on trust in GPT services (β = −0.140, p > 0.05). The lack of statisti- cal significance implies that perceived integrity does not play a decisive role in shaping user trust, or that other factors may have a stronger influence on trust formation in this context. H4: The study confirms that Trust has significant positive effect on Use of GPT services (β = 0.875, p < 0.001). The high effect size suggests that trust is a primary determinant of adoption and continued use, indicating that users who perceive GPT services as reliable, capable, and well-intentioned are much more inclined to integrate them into their activities. H5: The study does not confirm a statistically significant direct effect of User’s background in technology on Trust (β = 0.078, p > 0.05). Nevertheless, we were able to confirm its moderating effect: H6: User’s background in technology significantly moderates the relationship between Perceived Ability and Trust (IN T : β = 0.393, p < 0.001). This A−T Bgr suggests that the higher the user’s level of education in technology, the stronger 12 H. Li et I. Rychkova the impact of Perceived Ability on Trust in GPT services. Specifically, users with a stronger technological background are more likely to develop trust in GPT services when they perceive the system as capable. H7: User’s background in technology significantly moderates the relationship between Perceived Benevolence and Trust (IN T : β = −0.210, p = B−T Bgr 0.032). Negative value of β suggests that the higher the user’s level of education in technology, the weaker the impact of Perceived Benevolence on Trust in GPT services. Specifically, users with a stronger technological background are less likely to rely on the perceived benevolence (fairness, objectivity, confidentiality etc) of the exchanges with GPT services when forming trust. H8: User’s background in technology does not significantly influence the re- lationship between Perceived Integrity and Trust in GPT services. The non- significant moderating effect (IN T : β = 0.105, p = 0.566) suggests that I−T Bgr factors such as honesty, transparency, and adherence to ethical principles im- pact trust in GPT services consistently across users, regardless of their technical background. H9: Specific Knowledge in AI has a significant positive effect on Trust in GPT services (β = 0.378, p < 0.001), indicating that users with greater AI- related knowledge are more likely to trust these services. This finding suggests that familiarity with AI concepts, mechanisms, and limitations enhances users’ confidence in GPT systems. Users who understand AI may better assess its ca- pabilities, interpret its outputs more accurately, and manage their expectations, leading to increased trust. 4.3 Further Observations We conducted the independent samples t-test in order to evaluate whether there is a significant difference between the means for Trust and Use of GPT services between two groups. The first group of respondents participated in the survey on February 2024 (Period 1) and the second group - on November 2024 (Period 2). The results are illustrated in Fig.3. We did not find significant evidence of increase of Trust in GPT services over time, with the t-statistics t = -1.817 and p = 0.072 (conventional significance threshold - p < 0.05). In contrast, we found significant increase of self-reported Use of GPT services between February 2024 and November 2024, with the t- statistics t = -3.876 and p < 0.001. This suggests that factors other than Trust (e.g., habit formation, external influences, peer pressure, availability of new ser- vices etc.) may be driving the growing use and adoption of GPT services. 5 Conclusion This study aimed to provide a deeper understanding of the social dimensions of trust in increasingly human-like AI technologies. We explored user trust in GPT services by examining social factors influencing its formation as defined by the Integrative Model of Organizational Trust [16]. The study empirically validated a proposed theoretical model through a survey of 124 participants. Data analysis was conducted using Structural Equation Modeling (SEM) in JASP. Understanding Trust Formation in GPT Services: An Empirical Study 13 Fig. 3. Independent samples t-test on Trust (T) and Use (U) Our findings show that perceived benevolence of GPT services has a signif- icant positive effect on trust in GPT services (H2), highlighting users’ concerns about potential bias and the confidentiality of their interactions. While perceived ability and perceived integrity showed small positive and negative effects respec- tively (H1, H3). However, based on our data, those effects were not statistically significant. The study confirmed that trust has a significant positive effect on the use of GPT services (H4). The role of user background in technology was also explored. While it did not have a direct statistically significant effect on trust (H5), it did exhibit significant moderating effects. In particular, we were able to confirm that a higher level of education in technology strengthens the positive impact of perceived ability on trust (H6). Similarly, a higher level of education in technology weakens the positive impact of perceived benevolence on trust (H7). The impact of perceived integrity on trust was not significantly moderated by the user’s technological background (H8). We also confirmed that specific knowledge in AI has a significant positive effect on trust in GPT services, suggesting that familiarity with AI enhances user confidence (H9). Finally, our study shown that, while the self-reported use of GPT services significantly increased between February and November 2024, there was no sig- nificant increase in trust during the same period. This implies that factors beyond trust might be driving the growing adoption of GPT services. This study contributes to the literature by reaffirming the significant role of trust in user engagement with GPT services and by explicitly linking trust in GPT services to the established factors of social trust. In our future research, we are going to further explore the factors driving adoption of GPT services and delve deeper into the interplay between social trust and technical understanding in the context of rapidly evolving AI technologies."
}